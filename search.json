[
  {
    "objectID": "howtos/windowsAndLinux.html",
    "href": "howtos/windowsAndLinux.html",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "",
    "text": "Windows 10 has a powerful new feature that allows a full Linux system to be installed and run from within Windows. This is incredibly useful for building/testing code in Linux, without having a dedicated Linux machine, but it poses strange new behaviors as two very different operating systems coexist in one place. Initially, this document mirrors the Windows Install tutorial, showing you how to install Ubuntu and setting up R, RStudio, and LaTex. Then, we cover some of the issues of running two systems together, starting with finding files, finding the Ubuntu subsystem, and file modifications.",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-ubuntu",
    "href": "howtos/windowsAndLinux.html#installing-ubuntu",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing Ubuntu",
    "text": "Installing Ubuntu\nThere are 2 parts to installing a Linux subsystem in Windows. I will write this using Ubuntu as the example, as it is my preferred Linux distro, but several others are provided by Windows.\nSources:\n\nOfficial Windows Instructions\nUbuntu Update Instructions\n\n\n1) Enable Linux Subsystem\nBy default, the Linux subsystem is an optional addition in Windows. This feature has to be enabled prior to installing Linux. There are two ways to do it.\n\nCMD Line\nThe simplest way to enable the Linux subsystem is through PowerShell.\n\nOpen PowerShell as Administrator\nRun the following (on one line):\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\nRestart the computer\n\nGUI\nIf you don’t wish to use PowerShell, you can work your way through the control panel and turn on the Linux subsystem.\n\nOpen the settings page through the search bar\nGo to Programs and Features\nFind Turn Windows Features on or off on the right side\nEnable the Windows Subsystem for Linux option\nRestart the computer\n\n\n\n\n2) Install Linux Subsystem\nOnce the Linux subsystem feature has been enabled, there are multiple methods to download and install the Linux distro you want. I highly recommend installing Ubuntu from the Microsoft store. There are several other flavors available as well, but Ubuntu is generally the easiest to learn and the most well supported.\n\nOpen the Microsoft Store\nSearch for Ubuntu\n\nYou’re looking for the highest number followed by LTS, currently 20.04 LTS (or 18.04 LTS is fine too). This is the current long-term-release, meaning it will be supported for the next 5 years.\n\nClick on the tile, then click Get, and this should start the installation.\nFollow the prompts to install Ubuntu.\n\nAfter installing Ubuntu, it is advisable to update it. This is something you should do on a regular basis.\n\nOpen a Bash terminal.\nType sudo apt update to update your local package database.\nType sudo apt upgrade to upgrade your installed packages.",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#using-the-linux-terminal-from-r-in-windows",
    "href": "howtos/windowsAndLinux.html#using-the-linux-terminal-from-r-in-windows",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Using the Linux Terminal from R in Windows",
    "text": "Using the Linux Terminal from R in Windows\nTo get all the functionality of a UNIX-style commandline from within R (e.g., for bash code chunks), you should set the terminal under R in Windows to be the Linux subsystem.",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#a-note-on-file-modification",
    "href": "howtos/windowsAndLinux.html#a-note-on-file-modification",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "A Note on File Modification",
    "text": "A Note on File Modification\nDO NOT MODIFY LINUX FILES FROM WINDOWS\nIt is highly recommended that you never modify Linux files from Windows because of metadata corruption issues. Any files created under the Linux subsystem, only modify them with Linux tools. In contrast, you can create files in the Windows system and modify them with both Windows or Linux tools. There could be file permission issues because Windows doesn’t have the same concept of file permissions as Linux. So, if you intend to work on files using both Linux and Windows, create the files in the C drive under Windows, and you should be safe to edit them with either OS.",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#finding-windows-from-linux",
    "href": "howtos/windowsAndLinux.html#finding-windows-from-linux",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Finding Windows from Linux",
    "text": "Finding Windows from Linux\nOnce you have some flavor of Linux installed, you need to be able to navigate from your Linux home directory to wherever Windows stores files. This is relatively simple, as the Windows Subsystem shows Windows to Linux as a mounted drive.\n\nSource\n\n\nOpen a Bash terminal.\nType cd / to get to the root directory.\nIn root, type cd /mnt. This gets you to the mount point for your drives.\nType ls to see what drives are available (you should see a c, and maybe d as well).\nType cd c to get into the Windows C drive. This is the root of the C directory for Windows.\nTo find your files, change directy into the users folder, then into your username.\n\ncd Users/&lt;your-user-name&gt;\nThis is your home directory in Windows. If you type ls here, you should see things like\n\nDocuments\nDownloads\nPictures\nVideos\netc…",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#finding-linux-from-windows",
    "href": "howtos/windowsAndLinux.html#finding-linux-from-windows",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Finding Linux from Windows",
    "text": "Finding Linux from Windows\nThis is slightly more tricky than getting from Linux to Windows. Windows stores the Linux files in a hidden subfolder so that you don’t mess with them from Windows. However, you can find them, and then the easiest way (note, do not read as safest or smartest) to find those files in the future is by creating a desktop shortcut.\n\nSource\n\n\nOpen File Explorer\nIn the address bar, type %userprofile%\\AppData\\Local\\Packages\n\n%userprofile% will expand to something like C:\\Users\\&lt;your-user-name&gt;\n\nLook for a folder related to the Linux distro that you installed\n\nThese names will change slightly over time, but look for something similar-ish.\nFor Ubuntu, look for something with CanonicalGroupLimited.UbuntuonWindows in it.\n\nCanonical is the creator/distributor of Ubuntu.\n\n\nClick LocalState\nClick rootfs\n\nThis is the root of your Linux distro.\n\nClick into home and then into your user name.\n\nThis is your home directory under Linux.\n\nDO NOT MODIFY THESE FILES FROM WINDOWS\n\nData corruption is a possibility.\n\n\nSo, the final path to find your home directory from windows will look like:\n%userprofile%\\AppData\\Local\\Packages\\&lt;Distro-Folder&gt;\\LocalStat\\rootfs\\home\\&lt;your-user-name&gt;\\",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-r-on-the-linux-subsystem",
    "href": "howtos/windowsAndLinux.html#installing-r-on-the-linux-subsystem",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing R on the Linux Subsystem",
    "text": "Installing R on the Linux Subsystem\nIMPORTANT: This section is only if you’d like to try using R under Linux. For class, using R under Windows should be fine.\nThe Linux Subsystem behaves exactly like a regular Linux installation, but for completeness, I will provide instructions here for people new to Linux. These instructions are written from the perspective of Ubuntu, but will be similar for other repos.\nR is not a part of the standard Ubuntu installation. So, we have to add the repository manually to our repository list. This is relatively straightforward, and R supports several versions of Ubuntu.\nSources:\n\nCRAN guide for Ubuntu\nDigital Ocean quick tutorial\n\n\nIn a bash window, type:\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\n\nThis adds the key to “sign”, or validate, the R repository\n\nThen, type:\nsudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/'\n\ncloud.r-project.org is the default mirror, however, it is prudent to connect to the mirror closest to you geographically. Berkeley has it’s own mirror, so the command with the Berkeley mirror would look like\n\nsudo add-apt-repository 'deb https://cran.r-project.org/bin/linux/ubuntu/bionic-cran40/'\nFinally, type sudo apt install r-base, and press y to confirm installation\nTo test that it worked, type R into the console, and an R session should begin\n\nType q() to quit the R session",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-rstudio-on-the-linux-subsystem",
    "href": "howtos/windowsAndLinux.html#installing-rstudio-on-the-linux-subsystem",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing Rstudio on the Linux Subsystem",
    "text": "Installing Rstudio on the Linux Subsystem\n\n\n\n\n\n\nWarning\n\n\n\nTHIS NO LONGER WORKS\n\n\nAs of Rstudio 1.5.x, it does not run on WSL. Link\nAlso possible issues, WSL has no GUI, and therefore can’t support anything that uses a GUI.\nThese instructions work, but Rstudio doesn’t run.\nSources:\n\nRstudio\nSource\n\n\nGo the the Rstudio website (link above) and download the appropriate Rstudio Desktop version.\n\nFor most people, this is the Ubuntu 18 (64-bit) installer.\nSave it somewhere that you can find it.\nYou should have a file similar to rstudio-&lt;version number&gt;-amd64.deb\n\nOpen a terminal window and navigate to wherever you saved the rstudio install file.\nType the command sudo dpkg -i ./rstudio-&lt;version number&gt;-amd64.deb\n\nThis tells the package installer (dpkg) to install (-i) the file specified (./thing.deb)\n\nType the command sudo apt-get install -f\n\nThis tells the package manager (apt-get) to fix (-f) any dependency issues that may have arisen when installing the package.\n\nType the command which rstudio to make sure the system can find it.\n\nOutput should be similar to /usr/bin/rstudio\n\nRun rstudio from linux by typing rstudio &\n\nThe & runs it in the background, allowing you to close the terminal window.",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-latex-on-the-linux-subsystem",
    "href": "howtos/windowsAndLinux.html#installing-latex-on-the-linux-subsystem",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing LaTeX on the Linux Subsystem",
    "text": "Installing LaTeX on the Linux Subsystem\n\n\n\n\n\n\nImportant\n\n\n\nThis section is only if you’d like to try using LaTeX under Linux. For class, using LaTeX (or R Markdown) under Windows should be fine.\n\n\nLaTeX is a text-markup language used when generating documents from .Rmd files.\nSource LaTeX\n\nType sudo apt-get install texlive-full, press y to confirm installation\n\nGenerally, if you want to create and edit R Markdown documents you will also need a text editor to go with your LaTeX installation, but we won’t go into that here.",
    "crumbs": [
      "How tos",
      "Windows 10 and the Ubuntu Subsystem"
    ]
  },
  {
    "objectID": "howtos/installGit.html",
    "href": "howtos/installGit.html",
    "title": "Installing Git",
    "section": "",
    "text": "Here are some instructions for installing Git on your computer. Git is the version control software we’ll use in the course.\nYou can install Git by downloading and installing the correct binary from here.\nFor macOS, we recommend using the Homebrew option.\nGit comes installed on the SCF, so if you login to an SCF machine and want to use Git there, you don’t need to install Git.",
    "crumbs": [
      "How tos",
      "Installing Git"
    ]
  },
  {
    "objectID": "howtos/accessPython.html",
    "href": "howtos/accessPython.html",
    "title": "Accessing Python",
    "section": "",
    "text": "We recommend using the Miniforge distribution as your Python 3.12 installation.\nOnce you’ve installed Python, please install the following packages:\nAssuming you installed Miniforge, you should be able to do this from the command line:\nWhile you’re welcome to work with Python in a Jupyter notebook for exploration (e.g., using the campus DataHub, you’ll need to submit Quarto (.qmd) documents with Python chunks for your problem sets. So you’ll need Python set up on your laptop or to use it by logging in to an SCF machine.",
    "crumbs": [
      "How tos",
      "Accessing Python"
    ]
  },
  {
    "objectID": "howtos/accessPython.html#python-from-the-command-line",
    "href": "howtos/accessPython.html#python-from-the-command-line",
    "title": "Accessing Python",
    "section": "Python from the command line",
    "text": "Python from the command line\nOnce you get your SCF account, you can access Python or IPython from the UNIX command line as soon as you login to an SCF server. Just SSH to an SCF Linux machine (e.g., gandalf.berkeley.edu or radagast.berkeley.edu) and run ‘python’ or ‘ipython’ from the command line.\nMore details on using SSH are here. Note that if you have the Ubuntu subsystem for Windows, you can use SSH directly from the Ubuntu terminal.",
    "crumbs": [
      "How tos",
      "Accessing Python"
    ]
  },
  {
    "objectID": "howtos/accessPython.html#python-via-jupyter-notebook",
    "href": "howtos/accessPython.html#python-via-jupyter-notebook",
    "title": "Accessing Python",
    "section": "Python via Jupyter notebook",
    "text": "Python via Jupyter notebook\nYou can use a Jupyter notebook to run Python code from the SCF JupyterHub or the Berkeley DataHub.\nIf you’re on the SCF JupyterHub, select Start My Server. Then, unless you are running long or parallelized code, just click Spawn (in other words, accept the default ‘standalone’ partition). On the next page select ‘New’ and ‘Python 3’.\nTo finish your session, click on File, Hub Control Panel and Stop My Server. Do not click Logout.",
    "crumbs": [
      "How tos",
      "Accessing Python"
    ]
  },
  {
    "objectID": "howtos/accessUnixCommandLine.html",
    "href": "howtos/accessUnixCommandLine.html",
    "title": "Accessing the Unix Command Line",
    "section": "",
    "text": "You have several options for UNIX command-line access. You’ll need to choose one of these and get it working.\n\nMac OS (on your personal machine):\nOpen a Terminal by going to Applications -&gt; Utilities -&gt; Terminal\n\n\nWindows (on your personal machine):\n\nYou may be able to use the Ubuntu bash shell available in Windows.\nYour PC must be running a 64-bit version of Windows 10 Anniversary Update or later (build 1607+).\nPlease see this link for more information:\n\nhttps://msdn.microsoft.com/en-us/commandline/wsl/install_guide\n\nFor more detailed instructions, see the Installing the Linux Subsystem on Windows tutorial.\n(Not recommended) There’s an older program called cygwin that provides a UNIX command-line interface.\n\nNote that when you install Git on Windows, you will get Git Bash. While you can use this to control Git, the functionality is limited so this will not be enough for general UNIX command-line access for the course.\n\n\nLinux (on your personal machine):\nIf you have access to a Linux machine, you very likely know how to access a terminal.\n\n\nAccess via DataHub (provided by UC Berkeley’s Data Science Education Program)\n\nGo to https://datahub.berkeley.edu\nClick on Log in to continue, sign in via CalNet, and authorize DataHub to have access to your account.\nIn the upper left, click on File, New and Terminal.\nTo end your session, click on File, Hub Control Panel and Stop My Server. Note that Logout will not end your running session, it will just log you out of it.\n\n\n\nAccess via the Statistical Computing Facility (SCF)\nWith an SCF account (available here), you can access a bash shell in the ways listed below.\nThose of you in the Statistics Department should be in the process of getting an SCF account. Everyone else will need an SCF account when we get to the unit on parallel computing, but you can request an account now if you prefer.\n\nYou can login to our various Linux servers and access a bash shell that way. Please see http://statistics.berkeley.edu/computing/access.\nYou can also access a bash shell via the SCF JupyterHub interface; please see the Accessing Python instructions but when you click on New, choose Terminal. This is very similar to the DataHub functionality discussed above.",
    "crumbs": [
      "How tos",
      "Accessing the Unix Command Line"
    ]
  },
  {
    "objectID": "rubric.html",
    "href": "rubric.html",
    "title": "",
    "section": "",
    "text": "This document provides guidance for submitting high-quality problem set (and project) solutions. This guidance is based on general good practices for scientific communication, reproducible research, and software development."
  },
  {
    "objectID": "rubric.html#general-presentation",
    "href": "rubric.html#general-presentation",
    "title": "",
    "section": "General presentation",
    "text": "General presentation\n\nSimply presenting code or derivations is not sufficient.\nBriefly describe the overall goal or strategy before providing code/derivations.\nAs needed describe what the pieces of your code/derivation are doing to make it easier for a reader to follow the steps.\nKeep your output focused on illustrating what you did, without distracting from the flow of your solution by showing voluminous output. The output should illustrate and demonstrate, not overwhelm or obscure. If you need to show longer output, you can add it at the end as supplemental material.\nOutput should be produced by your code (i.e., from the code chunks running when the document is rendered), not by copying and pasting results into the document."
  },
  {
    "objectID": "rubric.html#coding-practice",
    "href": "rubric.html#coding-practice",
    "title": "",
    "section": "Coding practice",
    "text": "Coding practice\n\nMinimize (or eliminate) use of global variables.\nBreak down work into core tasks and develop small, modular, self-contained functions (or class methods) to carry out those tasks.\nDon’t repeat code. As needed refactor code to create new functions (or class methods).\nFunctions and classes should be “weakly coupled”, interacting via their interfaces and not by having to know the internals of how they work.\nUse data structures appropriate for the computations that need to be done.\nDon’t hard code ‘magic’ numbers. Assign such numbers to variables with clear names, e.g., speed_of_light = 3e8.\nProvide reasonable default arguments to functions (or class methods) when possible.\nProvide tests (including unit tests) when requested (this is good general practice but we won’t require it in all cases).\nAvoid overly complicated syntax – try to use the clearest syntax you can to solve the problem.\nIn terms of speed, don’t worry about it too much so long as the code finishes real-world tasks in a reasonable amount of time. When optimizing, focus on the parts of the code that are the bottlenecks.\nUse functions already available in the language rather than recreating yourself."
  },
  {
    "objectID": "rubric.html#code-style",
    "href": "rubric.html#code-style",
    "title": "",
    "section": "Code style",
    "text": "Code style\n\nFollow a consistent style. While you don’t have to follow Python’s PEP8 style guide exactly, please look at it and follow it generally.\nUse informative variable and function names and have a consistent naming style.\nUse whitespace (spaces, newlines) and parentheses to make the structure of the code easy to understand and the individual syntax pieces clear.\nUse consistent indentation to make the structure of the code easy to understand.\nProvide comments that give the goal of a given piece of code and why it does things, but don’t use comments to restate what the code does when it should be obvious from reading the code.\n\nProvide summaries for blocks of code.\nFor particularly complicated syntax, say what a given piece of code does."
  },
  {
    "objectID": "ps/ps1.html",
    "href": "ps/ps1.html",
    "title": "Problem Set 1",
    "section": "",
    "text": "This covers material in Units 2 and 4 as well as practice with Quarto.\nIt’s due at 10 am (Pacific) on September 11, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease note my comments in the syllabus about when to ask for help and about working together. In particular, include a short initial section giving the names of any other students that you worked with on the problem set (or indicating you didn’t work with anyone if that was the case) and then indicate in the text or in code comments any specific ideas or code you borrowed from another student or any online reference (including ChatGPT or the like).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "ps/ps1.html#comments",
    "href": "ps/ps1.html#comments",
    "title": "Problem Set 1",
    "section": "",
    "text": "This covers material in Units 2 and 4 as well as practice with Quarto.\nIt’s due at 10 am (Pacific) on September 11, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease note my comments in the syllabus about when to ask for help and about working together. In particular, include a short initial section giving the names of any other students that you worked with on the problem set (or indicating you didn’t work with anyone if that was the case) and then indicate in the text or in code comments any specific ideas or code you borrowed from another student or any online reference (including ChatGPT or the like).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "ps/ps1.html#formatting-requirements",
    "href": "ps/ps1.html#formatting-requirements",
    "title": "Problem Set 1",
    "section": "Formatting requirements",
    "text": "Formatting requirements\n\nYour electronic solution should be in the form of an Quarto file named ps1.qmd, with Python code chunks. Please see Lab 1 and the dynamic documents tutorial for more information on how to do this. You can put longer functions in a .py file and show the source code in your qmd file using inspect.getsource().\nYour PDF submission to Gradescope should be the PDF produced from your qmd. Your GitHub submission should include the qmd file, any Python code files (modules) that you use in your qmd file, your environment file (see problem 4d) and the final PDF, all named according to the submission guidelines.\nYour solution should not just be code - you should have text describing how you approached the problem and what the various steps were. Your code should have comments indicating what each function or block of code does, and for any lines of code or code constructs that may be hard to understand, a comment indicating what that code does.\nYou do not need to (and should not) show exhaustive output, but in general you should show short examples of what your code does to demonstrate its functionality. Please see the grading rubric, and note that the output should be produced as a result of the code chunks being run during the rendering process, not by copy-pasting of output from running the code separately (and definitely not as screenshots).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "ps/ps1.html#problems",
    "href": "ps/ps1.html#problems",
    "title": "Problem Set 1",
    "section": "Problems",
    "text": "Problems\n\nPlease read these lecture notes about how computers work, used in a class on statistical computing at CMU. Briefly (a few sentences) describe the difference between disk and memory based on that reference and/or other resources you find.\nThis problem uses the ideas and tools in Unit 2, Sections 1-3 to explore approaches to reading and writing data from files and to consider file sizes in ASCII plain text vs. binary formats in light of the fact that numbers are (generally) stored as 8 bytes per number in binary formats.\n\nGenerate a numpy array (named x) of random numbers from a standard normal distribution with 20 columns and as many rows as needed so that the data take up about 16 MB (megabytes) in size. As part of your answer, show the arithmetic (formatted using LaTeX math syntax) you did to determine the number of rows.\nExplain the sizes of the two files created below. In discussing the CSV text file, how many characters do you expect to be in the file (i.e., you should be able to estimate this reasonably accurately from first principles without using wc or any explicit program that counts characters). Hint: what do we know about numbers drawn from a standard normal distribution?\n\nimport os\nimport pandas as pd\nx = x.round(decimals = 12)\n\npd.DataFrame(x).to_csv('x.csv', header = False, index = False)\nprint(f\"{str(os.path.getsize('x.csv')/1e6)} MB\")\n\npd.DataFrame(x).to_pickle('x.pkl', compression = None) \nprint(f\"{str(os.path.getsize('x.pkl')/1e6)} MB\")\n\n30.777135 MB\n16.000572 MB\n\n\nSuppose we had rounded each number to four decimal places. Would using CSV have saved disk space relative to the pickle file?\nNow consider saving out the numbers one number per row in a CSV file. Given we no longer have to save all the commas, why is the file size unchanged?\nRead the CSV file into Python using pandas.read_csv. Compare the speed of reading the CSV to reading the pickle file with pandas.read_pickle. Note that in some cases you might find that the first time you read a file is slower; if so this has to do with the operating system caching the file in memory (we’ll discuss this further in Unit 7 when we talk about databases).\nFinally, in the next parts of the question, we’ll consider reading the CSV file in chunks as discussed in Unit 2. First, time how long it takes to read the first 10,000 rows in a single chunk using nrows.\nNow experiment with the skiprows to see if you can read in a large chunk of data from the middle of the file as quickly as the same size chunk from the start of the file. What does this indicate regarding whether Pandas/Python has to read in all the data up to the point where the chunk in the middle starts or can skip over it in some fashion? Is there any savings relative to reading all the initial rows and the chunk in the middle all at once?\n\nPlease read the Code syntax and style section of Unit 4 on good programming/project practices and incorporate what you’ve learned from that reading into your solution for Problem 4. (You can skip the section on Assertions and Testing, as we’ll cover that in Lab.) In particular, lint your code (e.g., using ruff or another tool of your choice as discussed in the Unit 4. (This is most straightforward for code in a .py file. If your code is directly in the qmd file, you probably need to copy-paste it into another file to lint it, unfortunately. If anyone figures out a good way to directly lint the code chunks, please post on Ed!)\nAs your response to this question, very briefly (a few sentences) note what you did in your code for Problem 4 that reflects what you read. Please also note anything in what you read in Unit 4 that you disagree with, if you have a different stylistic perspective.\nWe’ll experiment with webscraping and manipulating HTML by getting publication information off of Google Scholar. Note that Google Scholar does not have an API, so we are forced to deconstruct the queries that are produced when we point and click on the website.\nYour functions may be short enough that it’s ok to put a function directly within a code chunk in your qmd file. Or you might choose to put one or more functions into a .py file and use inspect.getsource() to show us the code. For this problem in which the focus of the work is the function and how it works, it will generally be best to show the function code as part of the problem solution rather than in an appendix.\n\nGo to Google Scholar and enter the name (including first name to help with disambiguation) for a researcher whose work interests you. (If you want to do the one that will match the problem set solutions, you can use “Michael Jordan”, who is a well-known statistician and machine learning researcher here at Berkeley.) If you’ve entered the name of a researcher that Google Scholar recognizes as having a Google Scholar profile, you should see that the first item shown in the results page is a “User profile”. Now, based on the information returned, show the HTML element containing the Google Scholar ID and determine the Google Scholar ID for the researcher. Ideally (see the extra credit part (e)) we would automate that process and write a Python function that returns the ID, but see Question 5 for why that seemingly would violate Google Scholar’s terms of use.\nCreate a function that constructs the http GET request (and submits that request) to get the citations for the scholar, taking the ID as the input argument and returning the HTML as a Python object.\nIMPORTANT: While running the query in an automated fashion is seemingly allowed (see Problem 5), Google may return “429” errors because it detects automated usage. Here are some things to do in that case:\n\nYou can try to download the HTML file via the UNIX curl command, which you can run within Python as subprocess.run([\"curl\", \"-L\", request_string], capture_output=True). If necessary for part (c), you can use curl or wget from the command line to separately download the file and then read that file into Python.\nWhen developing your code, once you have the code in this part of the problem working to download the HTML, use the downloaded HTML to develop the remainder of your code for part (c) and don’t keep re-downloading the HTML as you work on the remainder of the code.\n\nFor now, you can assume the user will provide a valid ID and that Google Scholar returns a result for the specified person. We’ll deal with making the code more robust in PS2.\nNow write a function that processes the HTML to return a Pandas (or Polars) data frame with the citation information (article title, authors, journal information, year of publication, and number of citations as five columns of information) for the researcher. Try your function on a second researcher to provide more confidence that your function is working properly. (We’ll add unit tests in PS2). Given the comments in (b), ideally your function will work either if given (i) the name of a file that you’ve already downloaded or (ii) the HTML content produced by your function in (b).\nHint: a possibly useful argument for find_all is to request element(s) with certain attributes, e.g., html.find(\"p\", attrs = {'class': 'songtext'}) for finding a p element whose class is songtext.\nCreate a requirements file (based on either pip or Conda) that has the necessary information (in particular Python package versions) to reproduce the environment in which you ran your code. Include this file in your GitHub repository directory for this problem set.\n(Extra credit) If you’d like extra practice, write a Python function that will return the Google Scholar ID when the function is provided an html file as its argument. The file would be the file that is returned by searching for a researcher name as the input at scholar.google.com. As discussed in (a), your function should not query Google Scholar using the requests package, but rather should manipulate an HTML file that you download after manually querying Google Scholar yourself.\n(Extra credit) If you’d like extra practice, fix your function so that you get all of the results for a researcher and not just the first 20. E.g., for Michael Jordan there are several hundred.\n\nLook at the robots.txt for Google Scholar (scholar.google.com) and the references in Unit 2 on the ethics of webscraping. Does it seem like it’s ok to scrape data from Google Scholar? Hopefully this will make clear why our scraping in Problem 4 did not include programmatically obtaining the Google Scholar ID.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 1"
    ]
  },
  {
    "objectID": "office_hours.html",
    "href": "office_hours.html",
    "title": "Office hours",
    "section": "",
    "text": "Chris (Evans 495 or Zoom (email in advance))\n\nMondays, 4-5 pm\nTuesdays, 10:30-11:30 am\nThursdays, 12:30-1:30 pm\nBy appoinment\nFeel free to drop by if my door is open, though I won’t always be able to help at the time.\n\nJoão:\n\nWednesdays, 3-4 pm (Evans 444)\nFridays during unused section time up until 3:30 pm (Evans 340)",
    "crumbs": [
      "Office Hours"
    ]
  },
  {
    "objectID": "labs/lab1-submission.html",
    "href": "labs/lab1-submission.html",
    "title": "Lab 1: Submitting problem set solutions",
    "section": "",
    "text": "By now you should already have access to the following 5 basic tools:\n\nUnix shell\nGit\nQuarto\nPython\nA text editor of your choice\n\nToday we will use all these tools together to submit a solution for Problem Set 0 (not a real problem set) to make sure you know how to submit solutions to upcoming (real) problem sets.\nHere is a selection of some basic reference tutorials and documentation for unix, bash and unix commands, git & GitHub, quarto, python and VS Code\nSome books to learn more about Unix.",
    "crumbs": [
      "Labs",
      "Lab 1 (Quarto/Git/Submission)"
    ]
  },
  {
    "objectID": "labs/lab1-submission.html#submitting-problem-set-solutions-090624",
    "href": "labs/lab1-submission.html#submitting-problem-set-solutions-090624",
    "title": "Lab 1: Submitting problem set solutions",
    "section": "",
    "text": "By now you should already have access to the following 5 basic tools:\n\nUnix shell\nGit\nQuarto\nPython\nA text editor of your choice\n\nToday we will use all these tools together to submit a solution for Problem Set 0 (not a real problem set) to make sure you know how to submit solutions to upcoming (real) problem sets.\nHere is a selection of some basic reference tutorials and documentation for unix, bash and unix commands, git & GitHub, quarto, python and VS Code\nSome books to learn more about Unix.",
    "crumbs": [
      "Labs",
      "Lab 1 (Quarto/Git/Submission)"
    ]
  },
  {
    "objectID": "labs/lab1-submission.html#quick-intro-to-git-and-github",
    "href": "labs/lab1-submission.html#quick-intro-to-git-and-github",
    "title": "Lab 1: Submitting problem set solutions",
    "section": "Quick Intro to git and GitHub",
    "text": "Quick Intro to git and GitHub\n\nCreating a new repository\nMaking changes\n\n\nEditing and saving files\nStaging changes\nCommitting changes locally\nPushing changes to remote repository\n\n\nUndoing changes:\n\n\nLocal changes\nLocal staged changes\nLocal commited changes\nPushed changes\n\n\nMerging divergent versions\nWorking with branches\nGUI options (sourcetree)\nGetting help\n\nDiscussion: - Why is git so damn complicated? - What do you need to remember when working with collaborators on the same repository?",
    "crumbs": [
      "Labs",
      "Lab 1 (Quarto/Git/Submission)"
    ]
  },
  {
    "objectID": "labs/lab1-submission.html#lab-submission",
    "href": "labs/lab1-submission.html#lab-submission",
    "title": "Lab 1: Submitting problem set solutions",
    "section": "Lab Submission",
    "text": "Lab Submission\nRefer to this guide and please ask questions if something is not clear.",
    "crumbs": [
      "Labs",
      "Lab 1 (Quarto/Git/Submission)"
    ]
  },
  {
    "objectID": "labs/lab1-submission.html#hands-on-lab-instructions---steps",
    "href": "labs/lab1-submission.html#hands-on-lab-instructions---steps",
    "title": "Lab 1: Submitting problem set solutions",
    "section": "Hands-on Lab Instructions - Steps",
    "text": "Hands-on Lab Instructions - Steps\n\nClone your github repository to your development environment\nCreate a subdirectory in your github repository with the name ps0\nIn that subdirectory, create a quarto document (ps0.qmd) that has some simple code that creates a simple plot (you can follow this example/tutorial here)\nUse the quarto command line to render it into a pdf document (quarto render FILE —to pdf)\nCommit the changes to your repository (git add FILES; git commit -m MESSAGE; git push)\nAdd another section to your quarto document (use your imagination), then preview and commit the changes\nUse the quarto command line to render the updated document into a pdf document\nAdd the pdf document to the repository as well\nMake sure that you can log into gradescope and upload a pdf document\n[optional] Undo your last set of changes and regenerate the pdf file\n\nIf we finish early, We will also take today’s lab as an opportunity to get familiar with the basic use of all the 5 basic tools listed above.\nFor git and quarto, very basic knowledge should be sufficient for now, but for unix commands and python, the more you learn the more effective you will be at solving the problem sets (and at any computational task you take on after that). You will need to learn more advanced use of git and github towards the end of the semester when you start working with other team members on the same project.\n\nChunk options\nLike RMarkdown, quarto allows for several execution options to be set per document and per chunk. Spend some time getting familiar with the various options, and keep this link handy when you are working on the first few problem sets.\nDepending on what’s required in the problem sets, you may need to set eval to false (just print out code) or error to true (print errors and don’t halt rendering of the document). Some of the other options may be useful for controlling how the code gets printed.\n\n\nTroubleshooting\n\nQuarto succeeds in rendering html but fails at rendering pdf\nInstall tinytex via quarto install tinytex\n\n\nProblems running and rendering bash commands in quarto\nIf you are using the knitr engine, you should be able to tag your code chunks in quarto with {bash} and use verbatim bash commands. If you are using the Jupyter engine, the {python} tag should be used instead and every line containing a bash command should be prefixed with an exclamation mark (!).\n\n\nQuarto rendering (or python execution) works from the terminal but not from the IDE\nYou can go to the settings in your IDE and point it to the specific python installation that you find when you execute which python in the terminal.\n\n\nQuarto rendering (or python execution) works from the IDE but not from the terminal\nyou can fix the quarto configuration by setting the environment variable QUARTO_PYTHON to the correct python path or by running quarto check. Restarting the IDE may also help if you had just installed something in the other environment.\n\n\nPushing to git fails with message “Make sure you configure your ‘user.email’ and ‘user.name’ in git”\nFollow the suggested course of action in the error message to configure your email and name, then push again.",
    "crumbs": [
      "Labs",
      "Lab 1 (Quarto/Git/Submission)"
    ]
  },
  {
    "objectID": "labs/lab1-submission.html#acknowledgements",
    "href": "labs/lab1-submission.html#acknowledgements",
    "title": "Lab 1: Submitting problem set solutions",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis lab was originally authored by Ahmed Eldeeb and adapted for the Fall 2024 semester.",
    "crumbs": [
      "Labs",
      "Lab 1 (Quarto/Git/Submission)"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "License"
    ]
  },
  {
    "objectID": "units/unit1-intro.html",
    "href": "units/unit1-intro.html",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "",
    "text": "PDF",
    "crumbs": [
      "Units",
      "Unit 1 (UNIX intro)"
    ]
  },
  {
    "objectID": "units/unit1-intro.html#summary-of-some-useful-editors",
    "href": "units/unit1-intro.html#summary-of-some-useful-editors",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "Summary of some useful editors",
    "text": "Summary of some useful editors\n\nvarious editors available on all operating systems:\n\ntraditional editors born in UNIX: emacs, vim\nsome newer editors: Atom, Sublime Text (Sublime is proprietary/not free)\n\nWindows-specific: WinEdt\nMac-specific: Aquamacs Emacs, TextMate, TextEdit\nRStudio provides a built-in editor for R code and Quarto/R Markdown files. One can actually edit and run Python code chunks quite nicely in RStudio. (Note: RStudio as a whole is an IDE (integrated development environment. The editor is just the editing window where you edit code (and Markdown) files.)\nVSCode has a powerful code editor that is customized to work with various languages, and it has a Quarto extension.\n\nAs you get started it’s ok to use a very simple text editor such as Notepad in Windows, but you should take the time in the next few weeks to try out more powerful editors such as one of those listed above. It will be well worth your time over the course of your graduate work and then your career.\nBe careful in Windows - file suffixes are often hidden.",
    "crumbs": [
      "Units",
      "Unit 1 (UNIX intro)"
    ]
  },
  {
    "objectID": "units/unit1-intro.html#optional-basic-emacs",
    "href": "units/unit1-intro.html#optional-basic-emacs",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "(Optional) Basic emacs",
    "text": "(Optional) Basic emacs\nEmacs is one option as an editor. I use Emacs a fair amount, so I’m including some tips here, but other editors listed above are just as good.\n\nEmacs has special modes for different types of files: Python code files, R code files, C code files, Latex files – it’s worth your time to figure out how to set this up on your machine for the kinds of files you often work on\n\nIf working with Python and R, one can start up a Python or R interpreter in an additional Emacs buffer and send code to that interpreter and see the results of running the code.\nFor working with R, ESS (emacs speaks statistics) mode is helpful. This is built into Aquamacs Emacs.\n\nTo open emacs in the terminal window rather than as a new window, which is handy when it’s too slow (or impossible) to pass (i.e., tunnel) the graphical emacs window through ssh: emacs -nw file.txt\n\n\n(Optional) Emacs keystroke sequence shortcuts (aka, key bindings).\n\nNote Several of these (Ctrl-a, Ctrl-e, Ctrl-k, Ctrl-y) work in the command line, interactive Python and R sessions, and other places as well.\n\n\n\n\n\n\n\n\nSequence\nResult\n\n\n\n\nCtrl-x,Ctrl-c\nClose the file\n\n\nCtrl-x,Ctrl-s\nSave the file\n\n\nCtrl-x,Ctrl-w\nSave with a new name\n\n\nCtrl-s\nSearch\n\n\nESC\nGet out of command buffer at bottom of screen\n\n\nCtrl-a\nGo to beginning of line\n\n\nCtrl-e\nGo to end of line\n\n\nCtrl-k\nDelete the rest of the line from cursor forward\n\n\nCtrl-space, then move to end of block\nHighlight a block of text\n\n\nCtrl-w\nRemove the highlighted block, putting it in the kill buffer\n\n\nCtrl-y (after using Ctrl-k or Ctrl-w)\nPaste from kill buffer (‘y’ is for ‘yank’)",
    "crumbs": [
      "Units",
      "Unit 1 (UNIX intro)"
    ]
  },
  {
    "objectID": "units/unit1-intro.html#optional-basic-vim",
    "href": "units/unit1-intro.html#optional-basic-vim",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "(Optional) Basic vim",
    "text": "(Optional) Basic vim\nvim is another option as an editor. Like emacs, it’s been around for a long time, and some of the other options above are probably more user friendly. However, it can be helpful to know how to do some basic things in vim.\nFor example, if you run git commit without the -m flag to add a message, you’ll be put in a vim editor window by default (you can also modify what editor git uses).\nvim has two modes: normal mode, which allows you to carry out various operations (such as navigation, saving files, moving and deleting lines) and insert mode, which allows you to actually insert text.\nTo get into insert mode from normal mode, type “i”. To get back to normal mode, press Esc.\nWhen in normal mode, you can type :w to save, :x to save and exit, and :q to exit. To search a document for a string (e.g., “python docstring”, type /python docstring and return/enter. Type Esc to get out of the search.",
    "crumbs": [
      "Units",
      "Unit 1 (UNIX intro)"
    ]
  },
  {
    "objectID": "units/regex.html",
    "href": "units/regex.html",
    "title": "Assignment: regex problems",
    "section": "",
    "text": "Overview\nRead the regular expression section of the bash shell tutorial and provide regular expression syntax to match the strings in the following scenarios. Any reasonable syntax si fine, and even better, challenge yourself to figure out multiple ways to answer each question.\nThis is an assignment, graded credit/no credit, and it does not need to follow the requirements for problem set submissions. Any format is fine (including hand-written and scanned).\n\n\nProblems\n\nMatch the strings “dog”, “Dog”, “dOg”, “doG”, “DOg”, etc. (The word ‘dog’ in any combination of lower and upper case.)\nMatch the strings “cat”, “caat”, “caaat”, etc.\nMatch the strings “cat”, “at”, and “t”.\nMatch two words separated by any amount of whitespace."
  },
  {
    "objectID": "units/unit2-dataTech.html",
    "href": "units/unit2-dataTech.html",
    "title": "Data technologies, formats, and structures",
    "section": "",
    "text": "PDF\nReferences (see syllabus for links):\n(Optional) Videos:\nThere are four videos from 2020 in the bCourses Media Gallery that you can use for reference if you want to:\nNote that the videos were prepared for a version of the course that used R, so there are some differences from the content in the current version of the unit that reflect translating between R and Python. I’m not sure how helpful they’ll be, but they are available.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#text-and-binary-files",
    "href": "units/unit2-dataTech.html#text-and-binary-files",
    "title": "Data technologies, formats, and structures",
    "section": "Text and binary files",
    "text": "Text and binary files\nIn general, files can be divided into text files and binary files. In both cases, information is stored as a series of bits. Recall that a bit is a single value in base 2 (i.e., a 0 or a 1), while a byte is 8 bits.\nA text file is one in which the bits in the file encode individual characters. Note that the characters can include the digit characters 0-9, so one can include numbers in a text file by writing down the digits needed for the number of interest. Examples of text file formats include CSV, XML, HTML, and JSON.\nText files may be simple ASCII files (i.e., files encoded using ASCII) or files in other encodings such as UTF-8, both covered in Section 5. ASCII files have 8 bits (1 byte) per character and can represent 128 characters (the 52 lower and upper case letters in English, 10 digits, punctuation and a few other things – basically what you see on a standard US keyboard). UTF-8 files have between 1 and 4 bytes per character.\nSome text file formats, such as JSON or HTML, are not easily interpretable/manipulable on a line-by-line basis (unlike, e.g., CSV), so they are not as amenable to processing using shell commands.\nA binary file is one in which the bits in the file encode the information in a custom format and not simply individual characters. Binary formats are not (easily) human readable but can be more space-efficient and faster to work with (because it can allow random access into the data rather than requiring sequential reading). The meaning of the bytes in such files depends on the specific binary format being used and a program that uses the file needs to know how the format represents information. Examples of binary files include netCDF files, Python pickle files, R data (e.g., .Rda) files, , and compiled code files.\nNumbers in binary files are usually stored as 8 bytes per number. We’ll discuss this much more in Unit 8.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#common-file-types",
    "href": "units/unit2-dataTech.html#common-file-types",
    "title": "Data technologies, formats, and structures",
    "section": "Common file types",
    "text": "Common file types\nHere are some of the common file types, some of which are text formats and some of which are binary formats.\n\n‘Flat’ text files: data are often provided as simple text files. Often one has one record or observation per row and each column or field is a different variable or type of information about the record. Such files can either have a fixed number of characters in each field (fixed width format) or a special character (a delimiter) that separates the fields in each row. Common delimiters are tabs, commas, one or more spaces, and the pipe (|). Common file extensions are .txt and .csv. Metadata (information about the data) are often stored in a separate file. CSV files are quite common, but if you have files where the data contain commas, other delimiters might be preferable. Text can be put in quotes in CSV files, and this can allow use of commas within the data. This is difficult to deal with from the command line, but read_table() in Pandas handles this situation.\n\nOne occasionally tricky difficulty is as follows. If you have a text file created in Windows, the line endings are coded differently than in UNIX. Windows uses a newline (the ASCII character \\n) and a carriage return (the ASCII character \\r) whereas UNIX uses onlyl a newline in UNIX). There are UNIX utilities (fromdos in Ubuntu, including the SCF Linux machines and dos2unix in other Linux distributions) that can do the necessary conversion. If you see \\^M at the end of the lines in a file, that’s the tool you need. Alternatively, if you open a UNIX file in Windows, it may treat all the lines as a single line. You can fix this with todos or unix2dos.\n\nIn some contexts, such as textual data and bioinformatics data, the data may be in a text file with one piece of information per row, but without meaningful columns/fields.\nData may also be in text files in formats designed for data interchange between various languages, in particular XML or JSON. These formats are “self-describing”; namely the metadata is part of the file. The lxml and json packages are useful for reading and writing from these formats. More in Section 4.\nYou may be scraping information on the web, so dealing with text files in various formats, including HTML. The requests and BeautifulSoup packages are useful for reading HTML.\nIn scientific contexts, netCDF (.nc) (and the related HDF5) are popular format for gridded data that allows for highly-efficient storage and contains the metadata within the file. The basic structure of a netCDF file is that each variable is an array with multiple dimensions (e.g., latitude, longitude, and time), and one can also extract the values of and metadata about each dimension. The netCDF4 package in Python nicely handles working with netCDF files.\nData may already be in a database or in the data storage format of another statistical package (Stata, SAS, SPSS, etc.). The Pandas package in Python has capabilities for importing Stata (read_stata), SPSS (read_spss), and SAS (read_sas) files, among others.\nFor Excel, there are capabilities to read an Excel file (see the read_excel function in Pandas), but you can also just go into Excel and export as a CSV file or the like and then read that into Python. In general, it’s best not to pass around data files as Excel or other spreadsheet format files because (1) Excel is proprietary, so someone may not have Excel and the format is subject to change, (2) Excel imposes limits on the number of rows, (3) one can easily manipulate text files such as CSV using UNIX tools, but this is not possible with an Excel file, (4) Excel files often have more than one sheet, graphs, macros, etc., so they’re not a data storage format per se.\nPython can easily interact with databases (SQLite, PostgreSQL, MySQL, Oracle, etc.), querying the database using SQL and returning results to Python. More in the big data unit and in the large datasets tutorial mentioned above.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#csv-vs.-specialized-formats-such-as-parquet",
    "href": "units/unit2-dataTech.html#csv-vs.-specialized-formats-such-as-parquet",
    "title": "Data technologies, formats, and structures",
    "section": "CSV vs. specialized formats such as Parquet",
    "text": "CSV vs. specialized formats such as Parquet\nCSV is a common format (particularly in some disciplines/contexts) and has the advantages of being simple to understand, human readable, and readily manipulable by line-based processing tools such as shell commands. However, it has various disadvantages:\n\nstorage is by row, which will often mix values of different types;\nextra space is taken up by explicitly storing commas and newlines; and\none must search through the document to find a given row or value – e.g., to find the 10th row, we must search for the 9th newline and then read until the 10th newline.\n\nA popular file format that has some advantages over plain text formats such as CSV is Parquet. The storage is by column (actually in chunks of columns). This works well with how datasets are often structured in that a given field/variable will generally have values of all the same type and there may be many repeated values, so there are opportunities for efficient storage including compression. Storage by column also allows retrieval only of the columns that a user needs. As a result data stored in the Parquet format often takes up much less space than stored as CSV and can be queried much faster. Also note that data stored in Parquet will often be stored as multiple files.\nHere’s a brief exploration using a data file not in the class repository because of its size but available online here.\n\nimport time\n\n## Read from CSV.\nt0 = time.time()\ndata_from_csv = pd.read_csv(os.path.join('..', 'data', 'airline.csv'))\nprint(time.time() - t0)\n\n1.0188117027282715\n\n\n\n## Write out Parquet-formatted data.\ndata_from_csv.to_parquet(os.path.join('..', 'data', 'airline.parquet'))\n\n## Read from Parquet.\nt0 = time.time()\ndata_from_parquet = pd.read_parquet(os.path.join(\n                                    '..', 'data', 'airline.parquet'))\nprint(time.time() - t0)\n\n0.12586140632629395\n\n\nThe CSV file is 51 MB while the Parquet file is 8 MB.\n\nimport subprocess\nsubprocess.run([\"ls\", \"-l\", os.path.join(\"..\", \"data\", \"airline.csv\")])\nsubprocess.run([\"ls\", \"-l\", os.path.join(\"..\", \"data\", \"airline.parquet\")])\n\n-rw-r--r-- 1 paciorek scfstaff 51480244 Aug 27 11:35 ../data/airline.csv\n\n\nCompletedProcess(args=['ls', '-l', '../data/airline.csv'], returncode=0)\n\n\n-rw-r--r-- 1 paciorek scfstaff 8153160 Sep  3 09:12 ../data/airline.parquet\n\n\nCompletedProcess(args=['ls', '-l', '../data/airline.parquet'], returncode=0)",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#core-python-functions",
    "href": "units/unit2-dataTech.html#core-python-functions",
    "title": "Data technologies, formats, and structures",
    "section": "Core Python functions",
    "text": "Core Python functions\nThe read_table and read_csv functions in the Pandas package are commonly used for reading in data. They read in delimited files (CSV specifically in the latter case). The key arguments are the delimiter (the sep argument) and whether the file contains a header, a line with the variable names. We can use read_fwf() to read from a fixed width text file into a data frame.\nThe most difficult part of reading in such files can be dealing with how Pandas determines the types of the fields that are read in. While Pandas will try to determine the types automatically, it can be safer (and faster) to tell Pandas what the types are, using the dtype argument to read_table().\nLet’s work through a couple examples. Before we do that, let’s look at the arguments to read_table. Note that sep='' can use regular expressions (which would be helpful if you want to separate on any amount of white space, as one example).\n\ndat = pd.read_table(os.path.join('..', 'data', 'RTADataSub.csv'),\n                    sep = ',', header = None)\ndat.dtypes.head()   # 'object' is string or mixed type\ndat.loc[0,1]     \ntype(dat.loc[0,1]) # string!\n## Whoops, there is an 'x', presumably indicating missingness:\ndat.loc[:,1].unique()\n\n0    object\n1    object\n2    object\n3    object\n4    object\ndtype: object\n\n\n'2336'\n\n\nstr\n\n\narray(['2336', '2124', '1830', '1833', '1600', '1578', '1187', '1005',\n       '918', '865', '871', '860', '883', '897', '898', '893', '913',\n       '870', '962', '880', '875', '884', '894', '836', '848', '885',\n       '851', '900', '861', '866', '867', '829', '853', '920', '877',\n       '908', '855', '845', '859', '856', '825', '828', '854', '847',\n       '840', '873', '822', '818', '838', '815', '813', '816', '849',\n       '802', '805', '792', '823', '808', '798', '800', '842', '809',\n       '807', '826', '810', '801', '794', '771', '796', '790', '787',\n       '775', '751', '783', '811', '768', '779', '795', '770', '821',\n       '830', '767', '772', '791', '781', '773', '777', '814', '778',\n       '782', '837', '759', '846', '797', '835', '832', '793', '803',\n       '834', '785', '831', '820', '812', '824', '728', '760', '762',\n       '753', '758', '764', '741', '709', '735', '749', '752', '761',\n       '750', '776', '766', '789', '763', '864', '858', '869', '886',\n       '844', '863', '916', '890', '872', '907', '926', '935', '933',\n       '906', '905', '912', '972', '996', '1009', '961', '952', '981',\n       '917', '1011', '1071', '1920', '3245', '3805', '3926', '3284',\n       '2700', '2347', '2078', '2935', '3040', '1860', '1437', '1512',\n       '1720', '1493', '1026', '928', '874', '833', '850', nan, 'x'],\n      dtype=object)\n\n\n\n## Let's treat 'x' as a missing value indicator.\ndat2 = pd.read_table(os.path.join('..', 'data', 'RTADataSub.csv'),\n                     sep = ',', header = None, na_values = 'x')\ndat2.dtypes.head()\ndat2.loc[:,1].unique()\n\n0     object\n1    float64\n2    float64\n3    float64\n4    float64\ndtype: object\n\n\narray([2336., 2124., 1830., 1833., 1600., 1578., 1187., 1005.,  918.,\n        865.,  871.,  860.,  883.,  897.,  898.,  893.,  913.,  870.,\n        962.,  880.,  875.,  884.,  894.,  836.,  848.,  885.,  851.,\n        900.,  861.,  866.,  867.,  829.,  853.,  920.,  877.,  908.,\n        855.,  845.,  859.,  856.,  825.,  828.,  854.,  847.,  840.,\n        873.,  822.,  818.,  838.,  815.,  813.,  816.,  849.,  802.,\n        805.,  792.,  823.,  808.,  798.,  800.,  842.,  809.,  807.,\n        826.,  810.,  801.,  794.,  771.,  796.,  790.,  787.,  775.,\n        751.,  783.,  811.,  768.,  779.,  795.,  770.,  821.,  830.,\n        767.,  772.,  791.,  781.,  773.,  777.,  814.,  778.,  782.,\n        837.,  759.,  846.,  797.,  835.,  832.,  793.,  803.,  834.,\n        785.,  831.,  820.,  812.,  824.,  728.,  760.,  762.,  753.,\n        758.,  764.,  741.,  709.,  735.,  749.,  752.,  761.,  750.,\n        776.,  766.,  789.,  763.,  864.,  858.,  869.,  886.,  844.,\n        863.,  916.,  890.,  872.,  907.,  926.,  935.,  933.,  906.,\n        905.,  912.,  972.,  996., 1009.,  961.,  952.,  981.,  917.,\n       1011., 1071., 1920., 3245., 3805., 3926., 3284., 2700., 2347.,\n       2078., 2935., 3040., 1860., 1437., 1512., 1720., 1493., 1026.,\n        928.,  874.,  833.,  850.,   nan])\n\n\nUsing dtype is a good way to control how data are read in.\n\ndat = pd.read_table(os.path.join('..', 'data', 'hivSequ.csv'),\n                  sep = ',', header = 0,\n                  dtype = {\n                  'PatientID': int,\n                  'Resp': int,\n                  'PR Seq': str,\n                  'RT Seq': str,\n                  'VL-t0': float,\n                  'CD4-t0': int})\ndat.dtypes\ndat.loc[0,'PR Seq']\n\nPatientID      int64\nResp           int64\nPR Seq        object\nRT Seq        object\nVL-t0        float64\nCD4-t0         int64\ndtype: object\n\n\n'CCTCAAATCACTCTTTGGCAACGACCCCTCGTCCCAATAAGGATAGGGGGGCAACTAAAGGAAGCYCTATTAGATACAGGAGCAGATGATACAGTATTAGAAGACATGGAGTTGCCAGGAAGATGGAAACCAAAAATGATAGGGGGAATTGGAGGTTTTATCAAAGTAARACAGTATGATCAGRTACCCATAGAAATCTATGGACATAAAGCTGTAGGTACAGTATTAATAGGACCTACACCTGTCAACATAATTGGAAGAAATCTGTTGACTCAGCTTGGTTGCACTTTAAATTTY'\n\n\nNote that you can avoid reading in one or more columns by using the usecols argument. Also, specifying the dtype argument explicitly should make for faster file reading.\nIf possible, it’s a good idea to look through the input file in the shell or in an editor before reading into Python to catch such issues in advance. Using the UNIX command less on RTADataSub.csv would have revealed these various issues, but note that RTADataSub.csv is a 1000-line subset of a much larger file of data available from the kaggle.com website. So more sophisticated use of UNIX utilities (as we will see in Unit 3) is often useful before trying to read something into a program.\nIf the file is not nicely arranged by field (e.g., if it has ragged lines), we’ll need to do some more work. We can read each line as a separate string, after which we can process the lines using text manipulation. Here’s an example from some US meteorological data where I know from metadata (not provided here) that the 4-11th values are an identifier, the 17-20th are the year, the 22-23rd the month, etc.\n\nfile_path = os.path.join('..', 'data', 'precip.txt')\nwith open(file_path, 'r') as file:\n     lines = file.readlines()\n\nid = [line[3:11] for line in lines]\nyear = [int(line[17:21]) for line in lines]\nmonth = [int(line[21:23]) for line in lines]\nnvalues = [int(line[27:30]) for line in lines]\nyear[0:5]\n\n[2010, 2010, 2010, 2010, 2010]\n\n\nActually, that file, precip.txt, is in a fixed-width format (i.e., every element in a given column has the exact same number of characters),so reading in using pandas.read_fwf() would be a good strategy.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#connections-and-streaming",
    "href": "units/unit2-dataTech.html#connections-and-streaming",
    "title": "Data technologies, formats, and structures",
    "section": "Connections and streaming",
    "text": "Connections and streaming\nPython allows you to read in not just from a file but from a more general construct called a connection. This can include reading in text from the output of running a shell command and from unzipping a file on the fly.\nHere are some examples of connections:\n\nimport gzip\nwith gzip.open('dat.csv.gz', 'r') as file:\n     lines = file.readlines()\n\nimport zipfile\nwith zipfile.ZipFile('dat.zip', 'r') as archive:\n     with archive.open('data.txt', 'r') as file:\n          lines = file.readlines()\n\nimport subprocess\ncommand = \"ls -al\"\noutput = subprocess.check_output(command, shell = True)\n# `output` is a sequence of bytes.\nwith io.BytesIO(output) as stream:  # Create a file-like object.\n    content = stream.readlines()\n\ndf = pd.read_csv(\"https://download.bls.gov/pub/time.series/cu/cu.item\", sep=\"\\t\")\n\nIf a file is large, we may want to read it in in chunks (of lines), do some computations to reduce the size of things, and iterate. This is referred to as online processing, streaming, or chunking, and can be done using Pandas (among other tools).\n\nfile_path = os.path.join('..', 'data', 'RTADataSub.csv')\nchunksize = 50 # Obviously this would be much larger in any real application.\n\nwith pd.read_csv(file_path, chunksize = chunksize) as reader:\n     for chunk in reader:\n         # manipulate the lines and store the key stuff\n         print(f'Read {len(chunk)} rows.')\n\nMore details on sequential (on-line) processing of large files can be found in the tutorial on large datasets mentioned in the reference list above.\nOne cool trick that can come in handy is to ‘read’ from a string as if it were a text file. Here’s an example:\n\nfile_path = os.path.join('..', 'data', 'precip.txt')\nwith open(file_path, 'r') as file:\n     text = file.read()\n\nstringIOtext = io.StringIO(text)\ndf = pd.read_fwf(stringIOtext, header = None, widths = [3,8,4,2,4,2])\n\nWe can create connections for writing output too. Just make sure to open the connection first.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#file-paths",
    "href": "units/unit2-dataTech.html#file-paths",
    "title": "Data technologies, formats, and structures",
    "section": "File paths",
    "text": "File paths\nA few notes on file paths, related to ideas of reproducibility.\n\nIn general, you don’t want to hard-code absolute paths into your code files because those absolute paths won’t be available on the machines of anyone you share the code with. Instead, use paths relative to the directory the code file is in, or relative to a baseline directory for the project, e.g.:\n\n\ndat = pd.read_csv('../data/cpds.csv')\n\nUsing UNIX style directory separators will work in Windows, Mac or Linux, but using Windows style separators is not portable across operating systems.\n\n## good: will work on Windows\ndat = pd.read_csv('../data/cpds.csv')\n## bad: won't work on Mac or Linux\ndat = pd.read_csv('..\\data\\cpds.csv')  \n\nEven better, use os.path.join so that paths are constructed specifically for the operating system the user is using:\n\n\n## good: operating-system independent\ndat = pd.read_csv(os.path.join('..', 'data', 'cpds.csv'))",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#reading-data-quickly-arrow-and-polars",
    "href": "units/unit2-dataTech.html#reading-data-quickly-arrow-and-polars",
    "title": "Data technologies, formats, and structures",
    "section": "Reading data quickly: Arrow and Polars",
    "text": "Reading data quickly: Arrow and Polars\nApache Arrow provides efficient data structures for working with data in memory, usable in Python via the PyArrow package. Data are stored by column, with values in a column stored sequentially and in such a way that one can access a specific value without reading the other values in the column (O(1) lookup). Arrow is designed to read data from various file formats, including Parquet, native Arrow format, and text files. In general Arrow will only read data from disk as needed, avoiding keeping the entire dataset in memory.\nOther options for avoiding reading all your data into memory include the Dask package and using numpy.load with the mmap_mode argument.\npolars is designed to be a faster alternative to Pandas for working with data in-memory.\n\nimport polars\nimport time\nt0 = time.time()\ndat = pd.read_csv(os.path.join('..', 'data', 'airline.csv'))\nt1 = time.time()\ndat2 = polars.read_csv(os.path.join('..', 'data', 'airline.csv'), null_values = ['NA'])\nt2 = time.time()\nprint(f\"Timing for Pandas: {t1-t0}.\")\nprint(f\"Timing for Polars: {t2-t1}.\")\n\nTiming for Pandas: 1.005718469619751.\nTiming for Polars: 0.326127290725708.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#writing-output-to-files",
    "href": "units/unit2-dataTech.html#writing-output-to-files",
    "title": "Data technologies, formats, and structures",
    "section": "Writing output to files",
    "text": "Writing output to files\nFunctions for text output are generally analogous to those for input.\n\nfile_path = os.path.join('/tmp', 'tmp.txt')\nwith open(file_path, 'w') as file:\n     file.writelines(lines)\n\nWe can also use file.write() to write individual strings.\nIn Pandas, we can use DataFrame.to_csv and DataFrame.to_parquet.\nWe can use the json.dump function to output appropriate data objects (e.g., dictionaries or possibly lists) as JSON. One use of JSON as output from Python would be to ‘serialize’ the information in an Python object such that it could be read into another program.\nAnd of course you can always save to a Pickle data file (a binary file format) using pickle.dump() and pickle.load() from the pickle package. Happily this is platform-independent so can be used to transfer Python objects between different OS.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#formatting-output",
    "href": "units/unit2-dataTech.html#formatting-output",
    "title": "Data technologies, formats, and structures",
    "section": "Formatting output",
    "text": "Formatting output\nWe can use string formatting to control how output is printed to the screen.\nThe mini-language involved in the format specification can get fairly involved, but a few basic pieces of syntax can do most of what one generally needs to do.\nWe can format numbers to chosen number of digits and decimal places and handle alignment, using the format method of the string class.\nFor example:\n\n'{:&gt;10}'.format(3.5)    # right-aligned, using 10 characters\n'{:.10f}'.format(1/3)   # force 10 decimal places\n'{:15.10f}'.format(1/3) # force 15 characters, with 10 decimal places\nformat(1/3, '15.10f') # alternative using a function\n\n'       3.5'\n\n\n'0.3333333333'\n\n\n'   0.3333333333'\n\n\n'   0.3333333333'\n\n\nWe can also “interpolate” variables into strings.\n\n\"The number pi is {}.\".format(np.pi)\n\"The number pi is {:.5f}.\".format(np.pi)\n\"The number pi is {:.12f}.\".format(np.pi)\n\n'The number pi is 3.141592653589793.'\n\n\n'The number pi is 3.14159.'\n\n\n'The number pi is 3.141592653590.'\n\n\n\nval1 = 1.5\nval2 = 2.5\n# As of Python 3.6, put the variable names in directly.\nprint(f\"Let's add {val1} and {val2}.\")  \nnum1 = 1/3\nprint(\"Let's add the %s numbers %.5f and %15.7f.\"\n       %('floating point', num1 ,32+1/7))\n\nLet's add 1.5 and 2.5.\nLet's add the floating point numbers 0.33333 and      32.1428571.\n\n\nOr to insert into a file:\n\nfile_path = os.path.join('/tmp', 'tmp.txt')\nwith open(file_path, 'a') as file:\n     file.write(\"Let's add the %s numbers %.5f and %15.7f.\"\n                %('floating point', num1 ,32+1/7))\n\nround is another option, but it’s often better to directly control the printing format.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#reading-html",
    "href": "units/unit2-dataTech.html#reading-html",
    "title": "Data technologies, formats, and structures",
    "section": "Reading HTML",
    "text": "Reading HTML\nHTML (Hypertext Markup Language) is the standard markup language used for displaying content in a web browser. In simple webpages (ignoring the more complicated pages that involve Javascript), what you see in your browser is simply a rendering (by the browser) of a text file containing HTML.\nHowever, instead of rendering the HTML in a browser, we might want to use code to extract information from the HTML.\nLet’s see a brief example of reading in HTML tables.\nNote that before doing any coding, it can be helpful to look at the raw HTML source code for a given page. We can explore the underlying HTML source in advance of writing our code by looking at the page source directly in the browser (e.g., in Firefox under the 3-lines (hamburger) “open menu” symbol, see Web Developer (or More Tools) -&gt; Page Source and in Chrome View -&gt; Developer -&gt; View Source), or by downloading the webpage and looking at it in an editor, although in some cases (such as the nytimes.com case), what we might see is a lot of JavaScript.\nOne lesson here is not to write a lot of your own code to do something that someone else has probably already written a package for. We’ll use the BeautifulSoup4 package.\n\nimport io\nimport requests\nfrom bs4 import BeautifulSoup as bs\n\nURL = \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\"\nresponse = requests.get(URL)\nhtml = response.content\n\n# Create a BeautifulSoup object to parse the HTML\nsoup = bs(html, 'html.parser')\n\nhtml_tables = soup.find_all('table')\n\n## Pandas `read_html` doesn't want `str` input directly.\npd_tables = [pd.read_html(io.StringIO(str(tbl)))[0] for tbl in html_tables]\n\n[x.shape for x in pd_tables]\n\npd_tables[0]\n\n[(241, 7), (13, 2), (1, 2)]\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nLocation\nPopulation\n% of world\nDate\nSource (official or from the United Nations)\nNotes\n\n\n\n\n0\n–\nWorld\n8127721171\n100%\n2 Sep 2024\nUN projection[3]\nNaN\n\n\n1\n1/2 [b]\nChina\n1409670000\n17.3%\n31 Dec 2023\nOfficial estimate[5]\n[c]\n\n\n2\n1/2 [b]\nIndia\n1404910000\n17.3%\n1 Jul 2024\nOfficial projection[6]\n[d]\n\n\n3\n3\nUnited States\n335893238\n4.1%\n1 Jan 2024\nOfficial estimate[7]\n[e]\n\n\n4\n4\nIndonesia\n281603800\n3.5%\n1 Jul 2024\nNational annual projection[8]\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n236\n–\nNiue (New Zealand)\n1681\n0%\n11 Nov 2022\n2022 Census [234]\nNaN\n\n\n237\n–\nTokelau (New Zealand)\n1647\n0%\n1 Jan 2019\n2019 Census [235]\nNaN\n\n\n238\n195\nVatican City\n764\n0%\n26 Jun 2023\nOfficial figure[236]\n[af]\n\n\n239\n–\nCocos (Keeling) Islands (Australia)\n593\n0%\n30 Jun 2020\n2021 Census[237]\nNaN\n\n\n240\n–\nPitcairn Islands (UK)\n40\n0%\n1 Jul 2021\nOfficial estimate[238]\nNaN\n\n\n\n\n241 rows × 7 columns\n\n\n\nBeautiful Soup works by reading in the HTML as text and then parsing it to build up a tree containing the HTML elements. Then one can search by HTML tag or attribute for information you want using find_all.\nAs another example, it’s often useful to be able to extract the hyperlinks in an HTML document.\n\nURL = \"http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year\"\nresponse = requests.get(URL)\nsoup = bs(response.content, 'html.parser')\n\n## Approach 1: search for HTML 'a' tags.\na_elements = soup.find_all('a')\nlinks1 = [x.get('href') for x in a_elements]\n## Approach 2: search for 'a' elements with 'href' attribute\nhref_elements = soup.find_all('a', href = True)\nlinks2 = [x.get('href') for x in href_elements]\n## In either case, then use `get` to retrieve the `href` attribute value.\n\nlinks2[0:9]\n# help(bs.find_all)\n\n['?C=N;O=D',\n '?C=M;O=A',\n '?C=S;O=A',\n '?C=D;O=A',\n '/pub/data/ghcn/daily/',\n '1750.csv.gz',\n '1763.csv.gz',\n '1764.csv.gz',\n '1765.csv.gz']\n\n\nThe kwargs keyword arguments to find and find_all allow one to search for elements with particular characteristics, such as having a particular attribute (seen above) or having an attribute have a particular value (e.g., picking out an element with a particular id).\nHere’s another example of extracting specific components of information from a webpage (results not shown, since headlines will vary from day to day). We’ll use get_text to retrieve the element’s value.\n\nURL = \"https://dailycal.org\"\nresponse_news = requests.get(URL)\nsoup_news = bs(response_news.content, 'html.parser')\nh2_elements = soup_news.find_all(\"h2\")\nheadlines2 = [x.get_text() for x in h2_elements]\nh3_elements = soup_news.find_all(\"h3\")\nheadlines3 = [x.get_text() for x in h3_elements]\n\nMore generally, we may want to read an HTML document, parse it into its components (i.e., the HTML elements), and navigate through the tree structure of the HTML.\nWe can use CSS selectors with the select method for more powerful extraction capabilities. Going back to the climate data, let’s extract all the th elements nested within tr elements:\n\nsoup.select(\"tr th\")\n\n[&lt;th&gt;&lt;a href=\"?C=N;O=D\"&gt;Name&lt;/a&gt;&lt;/th&gt;,\n &lt;th&gt;&lt;a href=\"?C=M;O=A\"&gt;Last modified&lt;/a&gt;&lt;/th&gt;,\n &lt;th&gt;&lt;a href=\"?C=S;O=A\"&gt;Size&lt;/a&gt;&lt;/th&gt;,\n &lt;th&gt;&lt;a href=\"?C=D;O=A\"&gt;Description&lt;/a&gt;&lt;/th&gt;,\n &lt;th colspan=\"4\"&gt;&lt;hr/&gt;&lt;/th&gt;,\n &lt;th colspan=\"4\"&gt;&lt;hr/&gt;&lt;/th&gt;]\n\n\nOr we could extract the a elements whose parents are th elements:\n\nsoup.select(\"th &gt; a\")\n\n[&lt;a href=\"?C=N;O=D\"&gt;Name&lt;/a&gt;,\n &lt;a href=\"?C=M;O=A\"&gt;Last modified&lt;/a&gt;,\n &lt;a href=\"?C=S;O=A\"&gt;Size&lt;/a&gt;,\n &lt;a href=\"?C=D;O=A\"&gt;Description&lt;/a&gt;]\n\n\nNext let’s use the XPath language to specify elements rather than CSS selectors. XPath can also be used for navigating through XML documents.\n\nimport lxml.html\n\n# Convert the BeautifulSoup object to a lxml object\nlxml_doc = lxml.html.fromstring(str(soup))\n\n# Use XPath to select elements\na_elements = lxml_doc.xpath('//a[@href]')\nlinks = [x.get('href') for x in a_elements]\nlinks[0:9]\n\n['?C=N;O=D',\n '?C=M;O=A',\n '?C=S;O=A',\n '?C=D;O=A',\n '/pub/data/ghcn/daily/',\n '1750.csv.gz',\n '1763.csv.gz',\n '1764.csv.gz',\n '1765.csv.gz']",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#xml-json-and-yaml",
    "href": "units/unit2-dataTech.html#xml-json-and-yaml",
    "title": "Data technologies, formats, and structures",
    "section": "XML, JSON, and YAML",
    "text": "XML, JSON, and YAML\nXML, JSON, and YAML are three common file formats for storing data. All of them allow for key-value pairs and arrays/lists of unnamed elements and for hierarchical structure.\nTo read them into Python (or other languages), we want to use a package that understands the file format and can read the data into appropriate Python data structures. Usually one ends up with a set of nested (because of the hierarchical structure) lists and dictionaries.\n\nXML\nXML is a markup language used to store data in self-describing (no metadata needed) format, often with a hierarchical structure. It consists of sets of elements (also known as nodes because they generally occur in a hierarchical structure and therefore have parents, children, etc.) with tags that identify/name the elements, with some similarity to HTML. Some examples of the use of XML include serving as the underlying format for Microsoft Office and Google Docs documents and for the KML language used for spatial information in Google Earth.\nHere’s a brief example. The book with id attribute bk101 is an element; the author of the book is also an element that is a child element of the book. The id attribute allows us to uniquely identify the element.\n    &lt;?xml version=\"1.0\"?&gt;\n    &lt;catalog&gt;\n       &lt;book id=\"bk101\"&gt;\n          &lt;author&gt;Gambardella, Matthew&lt;/author&gt;\n          &lt;title&gt;XML Developer's Guide&lt;/title&gt;\n          &lt;genre&gt;Computer&lt;/genre&gt;\n          &lt;price&gt;44.95&lt;/price&gt;\n          &lt;publish_date&gt;2000-10-01&lt;/publish_date&gt;\n          &lt;description&gt;An in-depth look at creating applications with XML.&lt;/description&gt;\n       &lt;/book&gt;\n       &lt;book id=\"bk102\"&gt;\n          &lt;author&gt;Ralls, Kim&lt;/author&gt;\n          &lt;title&gt;Midnight Rain&lt;/title&gt;\n          &lt;genre&gt;Fantasy&lt;/genre&gt;\n          &lt;price&gt;5.95&lt;/price&gt;\n          &lt;publish_date&gt;2000-12-16&lt;/publish_date&gt;\n         &lt;description&gt;A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.&lt;/description&gt;\n       &lt;/book&gt;\n    &lt;/catalog&gt;\nWe can read XML documents into Python using various packages, including lxml and then manipulate the resulting structured data object. Here’s an example of working with lending data from the Kiva lending non-profit. You can see the XML format in a browser at http://api.kivaws.org/v1/loans/newest.xml.\nXML documents have a tree structure with information at nodes. As above with HTML, one can use the XPath language for navigating the tree and finding and extracting information from the node(s) of interest.\nHere is some example code for extracting loan info from the Kiva data. We’ll first show the ‘brute force’ approach of working with the data as a list and then the better approach of using XPath.\n\nimport xmltodict\n\nURL = \"https://api.kivaws.org/v1/loans/newest.xml\"\nresponse = requests.get(URL)\ndata = xmltodict.parse(response.content)\ndata.keys()\ndata['response'].keys()\ndata['response']['loans'].keys()\nlen(data['response']['loans']['loan'])\ndata['response']['loans']['loan'][2]\ndata['response']['loans']['loan'][2]['activity']\n\ndict_keys(['response'])\n\n\ndict_keys(['paging', 'loans'])\n\n\ndict_keys(['@type', 'loan'])\n\n\n20\n\n\n{'id': '2841918',\n 'name': 'Azanlessossi Kossi',\n 'description': {'languages': {'@type': 'list', 'language': ['fr', 'en']}},\n 'status': 'fundraising',\n 'funded_amount': '0',\n 'basket_amount': '0',\n 'image': {'id': '5653560', 'template_id': '1'},\n 'activity': 'Cereals',\n 'sector': 'Food',\n 'use': 'to buy 20 sacks of corn.',\n 'location': {'country_code': 'TG',\n  'country': 'Togo',\n  'town': 'Kégué',\n  'geo': {'level': 'town', 'pairs': '6.202204 1.239713', 'type': 'point'}},\n 'partner_id': '296',\n 'posted_date': '2024-09-03T15:43:12Z',\n 'planned_expiration_date': '2024-10-08T15:43:12Z',\n 'loan_amount': '850',\n 'borrower_count': '1',\n 'lender_count': '0',\n 'bonus_credit_eligibility': '1',\n 'tags': {'@type': 'list',\n  'tag': [{'name': '#Vegan', 'id': '10'}, {'name': '#Parent', 'id': '16'}]}}\n\n\n'Cereals'\n\n\n\nfrom lxml import etree\ndoc = etree.fromstring(response.content)\n\nloans = doc.xpath(\"//loan\")\n[loan.xpath(\"activity/text()\") for loan in loans]\n\n## suppose we only want the country locations of the loans (using XPath)\n[loan.xpath(\"location/country/text()\") for loan in loans]\n## or extract the geographic coordinates\n[loan.xpath(\"location/geo/pairs/text()\") for loan in loans]\n\n[['Beauty Salon'],\n ['Fish Selling'],\n ['Cereals'],\n ['Construction'],\n ['Food'],\n ['Farming'],\n ['Farming'],\n ['Farming'],\n ['Farming'],\n ['Fishing'],\n ['Retail'],\n ['Farming'],\n ['Food Market'],\n ['Agriculture'],\n ['Food Market'],\n ['Personal Housing Expenses'],\n ['Farming'],\n ['Clothing Sales'],\n ['Farming'],\n ['Farming']]\n\n\n[['Brazil'],\n ['Togo'],\n ['Togo'],\n ['El Salvador'],\n ['Liberia'],\n ['Nicaragua'],\n ['El Salvador'],\n ['Nicaragua'],\n ['Nicaragua'],\n ['Nicaragua'],\n ['Guatemala'],\n ['El Salvador'],\n ['Liberia'],\n ['Guatemala'],\n ['Sierra Leone'],\n ['Nicaragua'],\n ['Kenya'],\n ['Sierra Leone'],\n ['Kenya'],\n ['Kenya']]\n\n\n[['-23.55052 -46.633309'],\n ['6.202204 1.239713'],\n ['6.202204 1.239713'],\n ['13.332346 -87.850064'],\n ['6.272723 -10.746428'],\n ['13.088391 -85.9994'],\n ['13.332346 -87.850064'],\n ['13.166667 -86.333333'],\n ['12.435556 -86.879444'],\n ['12.435556 -86.879444'],\n ['14.683333 -91.016667'],\n ['13.332346 -87.850064'],\n ['6.272723 -10.746428'],\n ['15.405833 -91.146111'],\n ['8.634544 -12.264173'],\n ['12.435556 -86.879444'],\n ['-0.785561 35.33914'],\n ['8.634544 -12.264173'],\n ['0.3 34.933333'],\n ['-0.583333 35.183333']]\n\n\n\n\nJSON\nJSON files are structured as “attribute-value” pairs (aka “key-value” pairs), often with a hierarchical structure. Here’s a brief example:\n    {\n      \"firstName\": \"John\",\n      \"lastName\": \"Smith\",\n      \"isAlive\": true,\n      \"age\": 25,\n      \"address\": {\n        \"streetAddress\": \"21 2nd Street\",\n        \"city\": \"New York\",\n        \"state\": \"NY\",\n        \"postalCode\": \"10021-3100\"\n      },\n      \"phoneNumbers\": [\n        {\n          \"type\": \"home\",\n          \"number\": \"212 555-1234\"\n        },\n        {\n          \"type\": \"office\",\n          \"number\": \"646 555-4567\"\n        }\n      ],\n      \"children\": [],\n      \"spouse\": null\n    }\nA set of key-value pairs is a named array and is placed inside braces (squiggly brackets). Note the nestedness of arrays within arrays (e.g., address within the overarching person array and the use of square brackets for unnamed arrays (i.e., vectors of information), as well as the use of different types: character strings, numbers, null, and (not shown) boolean/logical values. JSON and XML can be used in similar ways, but JSON is less verbose than XML.\nWe can read JSON into Python using the json package. Let’s play again with the Kiva data. The same data that we had worked with in XML format is also available in JSON format: https://api.kivaws.org/v1/loans/newest.json.\n\nURL = \"https://api.kivaws.org/v1/loans/newest.json\"\nresponse = requests.get(URL)\n\nimport json\ndata = json.loads(response.text)\ntype(data)\ndata.keys()\n\ntype(data['loans'])\ndata['loans'][0].keys()\n\ndata['loans'][0]['location']['country']\n[loan['location']['country']  for loan in data['loans']]\n\ndict\n\n\ndict_keys(['paging', 'loans'])\n\n\nlist\n\n\ndict_keys(['id', 'name', 'description', 'status', 'funded_amount', 'basket_amount', 'image', 'activity', 'sector', 'use', 'location', 'partner_id', 'posted_date', 'planned_expiration_date', 'loan_amount', 'borrower_count', 'lender_count', 'bonus_credit_eligibility', 'tags'])\n\n\n'Brazil'\n\n\n['Brazil',\n 'Togo',\n 'Togo',\n 'El Salvador',\n 'Liberia',\n 'Nicaragua',\n 'El Salvador',\n 'Nicaragua',\n 'Nicaragua',\n 'Nicaragua',\n 'Guatemala',\n 'El Salvador',\n 'Liberia',\n 'Guatemala',\n 'Sierra Leone',\n 'Nicaragua',\n 'Kenya',\n 'Sierra Leone',\n 'Kenya',\n 'Kenya']\n\n\nOne disadvantage of JSON is that it is not set up to deal with missing values, infinity, etc.\n\n\nYAML\nYAML is a similar format commonly used for configuration files that control how code/software/tools behave.\nHere’s an example of the YAML file specifying a GitHub Actions workflow.\nNote the use of indentation (similar to Python) for nesting/hierarchy and the lack of quotation with the strings. This makes it lightweight and readable. Also note the use of arrays/lists and sets of key-value pairs.\n\nimport yaml\n\nwith open(\"book.yml\") as stream:\n   config = yaml.safe_load(stream)  ## `safe_load` avoids running embedded code.\n\nprint(config)\n## How many steps in the `deploy-book` job?\nlen(config['jobs']['deploy-book']['steps'])\n\n{'name': 'deploy-book', True: {'push': {'branches': ['main']}}, 'jobs': {'deploy-book': {'runs-on': 'ubuntu-latest', 'steps': [{'uses': 'actions/checkout@v2'}, {'name': 'Set up Python 3.9', 'uses': 'actions/setup-python@v1', 'with': {'python-version': 3.9}}, {'name': 'Install dependencies', 'run': 'pip install -r book-requirements.txt\\n'}, {'name': 'Build the book', 'run': 'jupyter-book build .\\n'}, {'name': 'GitHub Pages action', 'uses': 'peaceiris/actions-gh-pages@v3', 'with': {'github_token': '${{ secrets.GITHUB_TOKEN }}', 'publish_dir': './_build/html'}}]}}}\n\n\n5\n\n\nNote that (unfortunately) on is treated as a boolean, as discussed in this GitHub issue for the PyYAML package.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#webscraping-and-web-apis",
    "href": "units/unit2-dataTech.html#webscraping-and-web-apis",
    "title": "Data technologies, formats, and structures",
    "section": "Webscraping and web APIs",
    "text": "Webscraping and web APIs\nHere we’ll see some examples of making requests over the Web to get data. We’ll use APIs to systematically query a website for information. Ideally, but not always, the API will be documented. In many cases that simply amounts to making an HTTP GET request, which is done by constructing a URL.\nThe requests package is useful for a wide variety of such functionality. Note that much of the functionality I describe below is also possible within the shell using either wget or curl.\n\nWebscraping ethics and best practices\nWebscraping is the process of extracting data from the web, either directly from a website or using a web API (application programming interface).\n\nShould you webscrape? In general, if we can avoid webscraping (particularly if there is not an API) and instead directly download a data file from a website, that is greatly preferred.\nMay you webscrape? Before you set up any automated downloading of materials/data from the web you should make sure that what you are about to do is consistent with the rules provided by the website.\n\nSome places to look for information on what the website allows are:\n\nlegal pages such as Terms of Service or Terms and Conditions on the website.\ncheck the robots.txt file (e.g., https://scholar.google.com/robots.txt) to see what a web crawler is allowed to do, and whether the site requires a particular delay between requests to the sites\npotentially contact the site owner if you plan to scrape a large amount of data\n\nHere are some links with useful information:\n\nBlog post on webscraping ethics\nSome information on how to understand a robots.txt file\n\nTips for when you make automated requests:\n\nWhen debugging code that processes the result of such a request, just run the request once, save (i.e., cache) the result, and then work on the processing code applied to the result. Don’t make the same request over and over again.\nIn many cases you will want to include a time delay between your automated requests to a site, including if you are not actually crawling a site but just want to automate a small number of queries.\n\n\n\nWhat is HTTP?\nHTTP (hypertext transfer protocol) is a system for communicating information from a server (i.e., the website of interest) to a client (e.g., your laptop). The client sends a request and the server sends a response.\nWhen you go to a website in a browser, your browser makes an HTTP GET request to the website. Similarly, when we did some downloading of html from webpages above, we used an HTTP GET request.\nAnytime the URL you enter includes parameter information after a question mark (www.somewebsite.com?param1=arg1&param2=arg2), you are using an API.\nThe response to an HTTP request will include a status code, which can be interpreted based on this information.\nThe response will generally contain content in the form of text (e.g., HTML, XML, JSON) or raw bytes.\n\n\nAPIs: REST- and SOAP-based web services\nIdeally a web service documents their API (Applications Programming Interface) that serves data or allows other interactions. REST and SOAP are popular API standards/styles. Both REST and SOAP use HTTP requests; we’ll focus on REST as it is more common and simpler. When using REST, we access resources, which might be a Facebook account or a database of stock quotes. The API will (hopefully) document what information it expects from the user and will return the result in a standard format (often a particular file format rather than producing a webpage).\nOften the format of the request is a URL (aka an endpoint) plus a query string, passed as a GET request. Let’s search for plumbers near Berkeley, and we’ll see the GET request, in the form:\nhttps://www.yelp.com/search?find_desc=plumbers&find_loc=Berkeley+CA&ns=1\n\nthe query string begins with ?\nthere are one or more Parameter=Argument pairs\npairs are separated by &\n+ is used in place of each space\n\nLet’s see an example of accessing economic data from the World Bank, using the documentation for their API. Following the API call structure, we can download (for example), data on various countries. The documentation indicates that our REST-based query can use either a URL structure or an argument-based structure.\n\n## Queries based on the documentation\napi_url = \"https://api.worldbank.org/V2/incomeLevel/LIC/country\"\napi_args = \"https://api.worldbank.org/V2/country?incomeLevel=LIC\"\n\n## Generalizing a bit\nurl = \"https://api.worldbank.org/V2/country?incomeLevel=MIC&format=json\"\nresponse = requests.get(url)\n\ndata = json.loads(response.content)\n\n## Be careful of data truncation/pagination\nif False:\n    url = \"https://api.worldbank.org/V2/country?incomeLevel=MIC&format=json&per_page=1000\"\n    response = requests.get(url)\n    data = json.loads(response.content)\n\n## Programmatic control\nbaseURL = \"https://api.worldbank.org/V2/country\"\ngroup = 'MIC'\nformat = 'json'\nargs = {'incomeLevel': group, 'format': format, 'per_page': 1000}\nurl = baseURL + '?' + '&'.join(['='.join(\n                               [key, str(args[key])]) for key in args])\nresponse = requests.get(url)\ndata = json.loads(response.content)\n   \ntype(data)\nlen(data[1])\ntype(data[1][5])\ndata[1][5]\n\nlist\n\n\n105\n\n\ndict\n\n\n{'id': 'BEN',\n 'iso2Code': 'BJ',\n 'name': 'Benin',\n 'region': {'id': 'SSF', 'iso2code': 'ZG', 'value': 'Sub-Saharan Africa '},\n 'adminregion': {'id': 'SSA',\n  'iso2code': 'ZF',\n  'value': 'Sub-Saharan Africa (excluding high income)'},\n 'incomeLevel': {'id': 'LMC',\n  'iso2code': 'XN',\n  'value': 'Lower middle income'},\n 'lendingType': {'id': 'IDX', 'iso2code': 'XI', 'value': 'IDA'},\n 'capitalCity': 'Porto-Novo',\n 'longitude': '2.6323',\n 'latitude': '6.4779'}\n\n\nAPIs can change and disappear. A few years ago, the example above involved the World Bank’s Climate Data API, which I can no longer find!\nAs another example, here we can see the US Treasury Department API, which allows us to construct queries for federal financial data.\nThe Nolan and Temple Lang book provides a number of examples of different ways of authenticating with web services that control access to the service.\nFinally, some web services allow us to pass information to the service in addition to just getting data or information. E.g., you can programmatically interact with your Facebook, Dropbox, and Google Drive accounts using REST based on HTTP POST, PUT, and DELETE requests. Authentication is of course important in these contexts and some times you would first authenticate with your login and password and receive a “token”. This token would then be used in subsequent interactions in the same session.\nI created your github.berkeley.edu accounts from Python by interacting with the GitHub API using requests.\n\n\nHTTP requests by deconstructing an (undocumented) API\nIn some cases an API may not be documented or we might be lazy and not use the documentation. Instead we might deconstruct the queries a browser makes and then mimic that behavior, in some cases having to parse HTML output to get at data. Note that if the webpage changes even a little bit, our carefully constructed query syntax may fail.\nLet’s look at some UN data (agricultural crop data). By going to\nhttps://data.un.org/Explorer.aspx?d=FAO, and clicking on “Crops”, we’ll see a bunch of agricultural products with “View data” links. Click on “apricots” as an example and you’ll see a “Download” button that allows you to download a CSV of the data. Let’s select a range of years and then try to download “by hand”. Sometimes we can right-click on the link that will download the data and directly see the URL that is being accessed and then one can deconstruct it so that you can create URLs programmatically to download the data you want.\nIn this case, we can’t see the full URL that is being used because there’s some Javascript involved. Therefore, rather than looking at the URL associated with a link we need to view the actual HTTP request sent by our browser to the server. We can do this using features of the browser (e.g., in Firefox see Web Developer -&gt; Network and in Chrome View -&gt; Developer -&gt; Developer tools and choose the Network tab) (or right-click on the webpage and select Inspect and then Network). Based on this we can see that an HTTP GET request is being used with a URL such as:\nhttp://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526;year:2012,2013,2014,2015,2016,2017&DataMartId=FAO&Format=csv&c=2,4,5,6,7&s=countryName:asc,elementCode:asc,year:desc.\nWe’e now able to easily download the data using that URL, which we can fairly easily construct using string processing in bash, Python, or R, such as this (here I just paste it together directly, but using more structured syntax such as I used for the World Bank example would be better):\nHere what is returned is a zip file, which is represented in Python as a sequence of “raw” bytes, so the example code also has some syntax for handling the unzipping and extraction of the CSV file with the data.\n\nimport zipfile\n\n## example URL:\n## https://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526;\n##year:2012,2013,2014,2015,2016,2017&DataMartId=FAO&Format=csv&c=2,4,5,6,7&\n##s=countryName:asc,elementCode:asc,year:desc\nitemCode = 526\nbaseURL = \"https://data.un.org/Handlers/DownloadHandler.ashx\"\nyrs = ','.join([str(yr) for yr in range(2012,2018)])\nfilter = f\"?DataFilter=itemCode:{itemCode};year:{yrs}\"\nargs1 = \"&DataMartId=FAO&Format=csv&c=2,3,4,5,6,7&\"\nargs2 = \"s=countryName:asc,elementCode:asc,year:desc\"\nurl = baseURL + filter + args1 + args2\n## If the website provided a CSV, this would be easier, but it zips the file.\nresponse = requests.get(url)\n\nwith io.BytesIO(response.content) as stream:  # create a file-like object\n     with zipfile.ZipFile(stream, 'r') as archive:   # treat the object as a zip file\n          with archive.open(archive.filelist[0].filename, 'r') as file:  # get a pointer to the embedded file\n              dat = pd.read_csv(file)\n\ndat.head()\n\n\n\n\n\n\n\n\nCountry or Area\nElement Code\nElement\nYear\nUnit\nValue\nValue Footnotes\n\n\n\n\n0\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2017.0\nIndex\n202.19\nNaN\n\n\n1\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2016.0\nIndex\n27.45\nNaN\n\n\n2\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2015.0\nIndex\n134.50\nNaN\n\n\n3\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2014.0\nIndex\n138.05\nNaN\n\n\n4\nAfghanistan\n432\nGross Production Index Number (2014-2016 = 100)\n2013.0\nIndex\n138.05\nNaN\n\n\n\n\n\n\n\nSo, what have we achieved?\n\nWe have a reproducible workflow we can share with others (perhaps ourself in the future).\nWe can automate the process of downloading many such files.\n\n\n\nMore details on HTTP requests\nA more sophisticated way to do the download is to pass the request in a structured way with named input parameters. This request is easier to construct programmatically.\n\ndata = {\"DataFilter\": f\"itemCode:{itemCode};year:{yrs}\",\n       \"DataMartID\": \"FAO\", \n       \"Format\": \"csv\", \n       \"c\": \"2,3,4,5,6,7\",\n       \"s\": \"countryName:asc,elementCode:asc,year:desc\"\n       }    \n\nresponse = requests.get(baseURL, params = data)\n\nwith io.BytesIO(response.content) as stream:  \n     with zipfile.ZipFile(stream, 'r') as archive:\n          with archive.open(archive.filelist[0].filename, 'r') as file:  \n              dat = pd.read_csv(file)\n\nIn some cases we may need to send a lot of information as part of the URL in a GET request. If it gets to be too long (e.g,, more than 2048 characters) many web servers will reject the request. Instead we may need to use an HTTP POST request (POST requests are often used for submitting web forms). A typical request would have syntax like this search (using requests):\n\nurl = 'http://www.wormbase.org/db/searches/advanced/dumper'\n\ndata = {      \"specipes\":\"briggsae\",\n              \"list\": \"\",\n              \"flank3\": \"0\",\n              \"flank5\": \"0\",\n              \"feature\": \"Gene Models\",\n              \"dump\": \"Plain TEXT\",\n              \"orientation\": \"Relative to feature\",\n              \"relative\": \"Chromsome\",\n              \"DNA\":\"flanking sequences only\",\n              \".cgifields\" :  \"feature, orientation, DNA, dump, relative\"\n}                                  \n\nresponse = requests.post(url, data = data)\nif response.status_code == 200:\n    print(\"POST request successful\")\nelse:\n    print(f\"POST request failed with status code: {response.status_code}\")\n\nUnfortunately that specific search doesn’t work because the server URL and/or API seem to have changed. But it gives you an idea of what the format would look like.\nrequests can handle other kinds of HTTP requests such as PUT and DELETE. Finally, some websites use cookies to keep track of users, and you may need to download a cookie in the first interaction with the HTTP server and then send that cookie with later interactions. More details are available in the Nolan and Temple Lang book.\n\n\nPackaged access to an API\nFor popular websites/data sources, a developer may have packaged up the API calls in a user-friendly fashion as functions for use from Python, R, or other software. For example there are Python (twitter) and R (twitteR) packages for interfacing with Twitter via its API. (Actually, with the change to X, I don’t know if this API is still available.)\nHere’s some example code for Python. This looks up the US senators’ Twitter names and then downloads a portion of each of their timelines, i.e., the time series of their tweets. Note that Twitter has limits on how much one can download at once.\n\nimport json\nimport twitter\n\n# You will need to set the following variables with your\n# personal information.  To do this you will need to create\n# a personal account on Twitter (if you don't already have\n# one).  Once you've created an account, create a new\n# application here:\n#    https://dev.twitter.com/apps\n#\n# You can manage your applications here:\n#    https://apps.twitter.com/\n#\n# Select your application and then under the section labeled\n# \"Key and Access Tokens\", you will find the information needed\n# below.  Keep this information private.\nCONSUMER_KEY       = \"\"\nCONSUMER_SECRET    = \"\"\nOAUTH_TOKEN        = \"\"\nOAUTH_TOKEN_SECRET = \"\"\n\nauth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n                           CONSUMER_KEY, CONSUMER_SECRET)\napi = twitter.Twitter(auth=auth)\n\n# get the list of senators\nsenators = api.lists.members(owner_screen_name=\"gov\",\n                             slug=\"us-senate\", count=100)\n\n# get all the senators' timelines\nnames = [d[\"screen_name\"] for d in senators[\"users\"]]\ntimelines = [api.statuses.user_timeline(screen_name=name, count = 500) \n             for name in names]\n\n# save information out to JSON\nwith open(\"senators-list.json\", \"w\") as f:\n    json.dump(senators, f, indent=4, sort_keys=True)\nwith open(\"timelines.json\", \"w\") as f:\n    json.dump(timelines, f, indent=4, sort_keys=True)\n\n\n\nAccessing dynamic pages\nMany websites dynamically change in reaction to the user behavior. In these cases you need a tool that can mimic the behavior of a human interacting with a site. Some options are:\n\nselenium is a popular tool for doing this, and there is a Python package of the same name.\nUsing scrapy plus splash is another approach.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#standard-data-structures-in-python-and-r",
    "href": "units/unit2-dataTech.html#standard-data-structures-in-python-and-r",
    "title": "Data technologies, formats, and structures",
    "section": "Standard data structures in Python and R",
    "text": "Standard data structures in Python and R\n\nIn Python and R, one often ends up working with dataframes, lists, and arrays/vectors/matrices/tensors.\nIn Python we commonly work with data structures that are part of additional packages, in particular numpy arrays and pandas dataframes.\nDictionaries in Python allow for easy use of key-value pairs where one can access values based on their key/label. In R one can do something similar with named vectors or named lists or (more efficiently) by using environments.\nIn R, if we are not working with rectangular datasets or standard numerical objects, we often end up using lists or enhanced versions of lists, sometimes with deeply nested structures.\n\nIn Unit 7, we’ll talk about distributed data structures that allow one to easily work with data distributed across multiple computers.",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "units/unit2-dataTech.html#other-kinds-of-data-structures",
    "href": "units/unit2-dataTech.html#other-kinds-of-data-structures",
    "title": "Data technologies, formats, and structures",
    "section": "Other kinds of data structures",
    "text": "Other kinds of data structures\nYou may have heard of various other kinds of data structures, such as linked lists, trees, graphs, queues, and stacks. One of the key aspects that differentiate such data structures is how one navigates through the elements.\nSets are collections of elements that don’t have any duplicates (like a mathematical set).\nWith a linked list, with each element (or node) has a value and a pointer (reference) to the location of the next element. (With a doubly-linked list, there is also a pointer back to the previous element.) One big advantage of this is that one can insert an element by simply modifying the pointers involved at the site of the insertion, without copying any of the other elements in the list. A big disadvantage is that to get to an element you have to navigate through the list.\n\n\n\nLinked list (courtesy of computersciencewiki.org)\n\n\nBoth trees and graphs are collections of nodes (vertices) and links (edges). A tree involves a set of nodes and links to child nodes (also possibly containing information linking the child nodes to their parent nodes). With a graph, the links might not be directional, and there can be cycles.\n\n\n\nTree (courtesy of computersciencewiki.org)\n\n\n\n\n\nGraph (courtesy of computersciencewiki.org)\n\n\nA stack is a collection of elements that behave like a stack of lunch trays. You can only access the top element directly(“last in, first out”), so the operations are that you can push a new element onto the stack or pop the top element off the stack. In fact, nested function calls behave as stacks, and the memory used in the process of evaluating the function calls is called the ‘stack’.\nA queue is like the line at a grocery store, behaving as “first in, first out”.\nOne can use such data structures either directly or via add-on packages in Python and R, though I don’t think they’re all that commonly used in R. This is probably because statistical/data science/machine learning workflows often involve either ‘rectangular’ data (i.e., dataframe-style data) and/or mathematical computations with arrays. That said, trees and graphs are widely used.\nSome related concepts that we’ll discuss further in Unit 5 include:\n\ntypes: this refers to how a given piece of information is stored and what operations can be done with the information.\n\n‘primitive’ types are the most basic types that often relate directly to how data are stored in memory or on disk (e.g., booleans, integers, numeric (real-valued), character, pointer (address, reference).\n\npointers: references to other locations (addresses) in memory. One often uses pointers to avoid unnecessary copying of data.\nhashes: hashing involves fast lookup of the value associated with a key (a label), using a hash function, which allows one to convert the key to an address. This avoids having to find the value associated with a specific key by looking through all the keys until the key of interest is found (an O(n) operation).",
    "crumbs": [
      "Units",
      "Unit 2 (Data technologies)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 243: Introduction to Statistical Computing",
    "section": "",
    "text": "Ed\n\n  Gradescope\n\n  Tutorials\n\n  Lecture recordings\n\n  bCourses\n\n  PollEV\n\n\nNo matching items",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "Statistics 243: Introduction to Statistical Computing",
    "section": "Course materials",
    "text": "Course materials\nSee the links above for the key resources for the course.\nMost course content (in particular unit notes, labs, and problem sets) are available through this website via the links in the left sidebar.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#questions-about-taking-the-class",
    "href": "index.html#questions-about-taking-the-class",
    "title": "Statistics 243: Introduction to Statistical Computing",
    "section": "Questions about taking the class",
    "text": "Questions about taking the class\nIf you would like to audit the class, enroll as a UC Berkeley undergraduate, or enroll as a concurrent enrollment student (i.e., for visiting students), or for some other reason are not enrolled, please fill out this survey as soon as possible. All those enrolled or wishing to take the class should have filled it out by Friday August 30 at noon.\nUndergraduates can only take the course with my permission and once all graduate students have an opportunity to register. I do not know if there will be space, but I recommend you attend the course as if you were enrolled until this is clear.\nConcurrent enrollment students (e.g., exchange students from other universities/countries) can take the course with my permission and once all UC Berkeley students have had an opportunity to register. I do not know if there will be space, but I recommend you attend the course as if you were enrolled until this is clear.\nPlease see the syllabus for the math and statistics background I expect, as well as the need to be familiar with Python or to get up to speed in Python during the first few weeks of the semester.\nThe first three weeks involve a lot of moving pieces, in part related to trying to get everyone up to speed with the bash shell and Python. Please use the schedule to keep on top of what you need to do.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Scheduling information",
    "section": "",
    "text": "Thu Aug 29:\n           \n           Event 1 (Optional) Library Introduction to LaTeX with Overleaf session\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Fri Aug 30:\n           \n           Lab 0 (Optional) help session for software installation/setup and UNIX-style command line basics\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 1 Class Survey\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 2 Office hour time survey\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 3 Login to github.berkeley.edu with your CalNet credentials\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Wed Sep 4:\n           \n           Activity 4 Read first three sections of Unit 2 (sections before 'Webscraping')\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 5 (Optional) work through the UNIX basics tutorial and answer (for yourself) the questions at the end\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Fri Sep 6:\n           \n           Lab 1 Using Git and Quarto and problem set submission\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Mon Sep 9:\n           \n           Assignment 1 Bash shell tutorial and exercises (see details below)\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Wed Sep 11:\n           \n           Problem_Set 1 Problem Set 1\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Event 2 (Optional) Library Introduction to LaTeX with Overleaf session\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Fri Sep 13:\n           \n           Assignment 2 Regular expression tutorial and exercises (see Details below)\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Fri Sep 20:\n           \n           Problem_Set 2 Problem Set 2\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n\nNo matching items",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#schedule",
    "href": "schedule.html#schedule",
    "title": "Scheduling information",
    "section": "",
    "text": "Thu Aug 29:\n           \n           Event 1 (Optional) Library Introduction to LaTeX with Overleaf session\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Fri Aug 30:\n           \n           Lab 0 (Optional) help session for software installation/setup and UNIX-style command line basics\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 1 Class Survey\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 2 Office hour time survey\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 3 Login to github.berkeley.edu with your CalNet credentials\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Wed Sep 4:\n           \n           Activity 4 Read first three sections of Unit 2 (sections before 'Webscraping')\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Activity 5 (Optional) work through the UNIX basics tutorial and answer (for yourself) the questions at the end\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Fri Sep 6:\n           \n           Lab 1 Using Git and Quarto and problem set submission\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Mon Sep 9:\n           \n           Assignment 1 Bash shell tutorial and exercises (see details below)\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Wed Sep 11:\n           \n           Problem_Set 1 Problem Set 1\n           \n                \n           \n                \n           \n        \n\n           \n           \n           Event 2 (Optional) Library Introduction to LaTeX with Overleaf session\n           \n                \n           \n                \n           \n        \n   \n\n        \n\n           \n           \n           Fri Sep 13:\n           \n           Assignment 2 Regular expression tutorial and exercises (see Details below)\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n   \n\n   \n   \n   \n   \n   \n   \n\n        \n\n           \n           \n           Fri Sep 20:\n           \n           Problem_Set 2 Problem Set 2\n           \n                \n           \n                \n           \n        \n   \n   \n   \n  \n\n\nNo matching items",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#quizzes",
    "href": "schedule.html#quizzes",
    "title": "Scheduling information",
    "section": "Quizzes",
    "text": "Quizzes\nQuizzes are in-person only.\n\nQuiz 1: Wednesday October 23 in class.\n\nReview session Friday October 18 in section.\n\nQuiz 2: Monday November 25 in class.\n\nReview session Friday November 22 in section.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#project",
    "href": "schedule.html#project",
    "title": "Scheduling information",
    "section": "Project",
    "text": "Project\nDue date: TBD during exam week.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "schedule.html#notes-on-assignments-and-activities",
    "href": "schedule.html#notes-on-assignments-and-activities",
    "title": "Scheduling information",
    "section": "Notes on assignments and activities",
    "text": "Notes on assignments and activities\n\nOptional library LaTeX session: I highly recommend (in particular if you are a Statistics graduate student) that you know how to create equations in LaTeX. Even if you develop your documents using Quarto, Jupyter notebooks, R Markdown, etc. rather than LaTeX-based documents, LaTeX math syntax is the common tool for writing math syntax that will render beautifully.\nOptional Lab 0 software/command line help session: (August 30 in lab room) Help session for installing software, accessing a UNIX-style command line, and basic command line usage (e.g., the UNIX basics tutorial). You can show up at any time (unlike all remaining labs). You should have software installed, be able to accesss the command line, and have started to become familiar with basic command line usage before class on Friday September 6.\nLab 1: (September 6) First section/lab on using Git, setting up your GitHub repository for problem sets, and using Quarto to generate dynamic documents. Please come only to the section you are registered for given space limits in the room, unless you have talked with Chris and have his permission.\nBash shell tutorial and exercises: (by September 9) Read through this tutorial on using the bash shell. You can skip the pages on Regular Expressions and Managing Processes. Work through the first 10 problems in the exercises and submit your answers via Gradescope. This is not a formal problem set, so you don’t need to worry about formatting nor about explaining/commenting your answers, nor do you need to put your answers in your GitHub class repository. In fact it’s even fine with me if you hand-write the answers and scan them to an electronic document. I just want to make sure you’ve worked through the tutorial. I’ll be doing demonstrations on using the bash shell in class starting on September 6, so that will be helpful as you work through the tutorial.\nRegular expression reading and exercises: (by September 13), read the regular expression material in the tutorial on using the bash shell. Then answer the regular expressions (regex) practice problems and submit your answers on Gradescope. This is not one of the graded problem sets but rather an assignment that will simply be noted as being completed or not.",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "units/unit5-programming.html",
    "href": "units/unit5-programming.html",
    "title": "Programming concepts",
    "section": "",
    "text": "PDF",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#string-processing-and-regular-expressions-in-python",
    "href": "units/unit5-programming.html#string-processing-and-regular-expressions-in-python",
    "title": "Programming concepts",
    "section": "String processing and regular expressions in Python",
    "text": "String processing and regular expressions in Python\nHere we’ll see functionality for working with strings in Python, focusing on regular expressions with the re package. This will augment our consideration of regular expressions in the shell, in particular by seeing how we can replace patterns in addition to finding them.\nThe re package provides Perl-style regular expressions, but it doesn’t seem to support named character classes such as [:digit:]. Instead use classes such as \\d and [0-9].\n\nFinding patterns\nIn Python, you can apply a matching function and then query the result to get information about what was matched and where in the string.\n\ntext = \"Here's my number: 919-543-3300.\"\nm = re.search(\"\\\\d+\", text)\nm\n\n&lt;re.Match object; span=(18, 21), match='919'&gt;\n\nm.group()\n\n'919'\n\nm.start()\n\n18\n\nm.end()\n\n21\n\nm.span()\n\n(18, 21)\n\n\nNotice that that showed us only the first match.\nThe discussion of special characters explains why we need to provide \\\\d rather than \\d.\nWe can instead use findall to get all the matches.\n\nre.findall(\"\\\\d+\", text)\n\n['919', '543', '3300']\n\n\nThis is equivalent to:\n\npattern = re.compile(\"\\\\d+\")\nre.findall(pattern, text)\n\n['919', '543', '3300']\n\n\nThe compile can be omitted and will be done implicitly, but is a good idea to do explicitly if you have a complex regex pattern that you will use repeatedly (e.g., on every line in a file). It is also a reminder that regular expressions is a separate language, which can be compiled into a program. The compilation results in an object that relies on finite state machines to match the pattern.\nTo ignore case, do the following:\n\ntext = \"That cat in the Hat\"\nre.findall(\"hat\", text, re.IGNORECASE)\n\n['hat', 'Hat']\n\n\nThere are several other regex flags (also called compilation flags) that can control the behavior of the matching engine in interesting ways (check out re.VERBOSE and re.MULTILINE for instance).\nWe can of course use list comprehension to work with multiple strings. But we need to be careful to check whether a match was found.\n\ndef return_group(pattern, txt):\n    m = re.search(pattern, txt)\n    if m:\n       return m.group()\n    else:\n       return None\n\ntext = [\"Here's my number: 919-543-3300.\", \"hi John, good to meet you\",\n        \"They bought 731 bananas\", \"Please call 1.919.554.3800\"]\n[return_group(\"\\\\d+\", str) for str in text]\n\n['919', None, '731', '1']\n\n\nRecall that we can search for location-specific matches in relation to the start and end of a string.\n\ntext = \"hats are all that are important to a hatter.\"\nre.findall(\"^hat\\\\w+\", text)\n\n['hats']\n\n\nRecall that we can search based on repetitions (as already demonstrated with the \\w+ just above).\n\ntext = \"Here's my number: 919-543-3300. They bought 731 hats. Please call 1.919.554.3800.\"\nre.findall(\"\\\\d{3}[-.]\\\\d{3}[-.]\\\\d{4}\", text)\n\n['919-543-3300', '919.554.3800']\n\n\nAs another example, the phone number detection problem could have been done a bit more compactly (as well as more generally to allow for an initial “1-” or “1.”) as:\n\ntext = \"Here's my number: 919-543-3300. They bought 731 bananas. Please call 1.919.554.3800.\"\nre.findall(\"((1[-.])?(\\\\d{3}[-.]){1,2}\\\\d{4})\", text)\n\n[('919-543-3300', '', '543-'), ('1.919.554.3800', '1.', '554.')]\n\n\nQuestion: the above regex would actually match something that is not a valid phone number. What can go wrong?\nWhen you are searching for all occurrences of a pattern in a large text object, it may be beneficial to use finditer:\n\nit = re.finditer(\"(http|ftp):\\\\/\\\\/\", text)  # http or ftp followed by ://\n\nfor match in it:\n    match.span()\n\nThis method behaves lazily and returns an iterator that gives us one match at a time, and only scans for the next match when we ask for it. This is similar to the behavior we saw with pandas.read_csv(chunksize = n)\n\n\nManipulating and replacing patterns\nWe can replace matching substrings with re.sub.\n\ntext = \"Here's my number: 919-543-3300.\"\nre.sub(\"\\\\d\", \"Z\", text   )\n\n\"Here's my number: ZZZ-ZZZ-ZZZZ.\"\n\n\nNext let’s consider grouping using (). We’ll see that the grouping operator also controls what is returned as the matched patterns.\nHere’s a basic example of using grouping via parentheses with the OR operator.\n\ntext = \"At the site http://www.ibm.com. Some other text. ftp://ibm.com\"\nre.search(\"(http|ftp):\\\\/\\\\/\", text).group()\n\n'http://'\n\n\nHowever, if we want to find all the matches and try to use findall, we see that, when grouping operators are present, it returns only the “captured” groups, as discussed a bit in help(re.findall), so we’d need to add an additional grouping operator to capture the full pattern when using findall:\n\nre.findall(\"(http|ftp):\\\\/\\\\/\", text)  \n\n['http', 'ftp']\n\nre.findall(\"((http|ftp):\\\\/\\\\/)\", text) \n\n[('http://', 'http'), ('ftp://', 'ftp')]\n\n\nGroups are also used when we need to reference back to a detected pattern when doing a replacement. This is why they are sometimes referred to as “capturing groups”. For example, here we’ll find any numbers and add underscores before and after them:\n\ntext = \"Here's my number: 919-543-3300. They bought 731 bananas. Please call 919.554.3800.\"\nre.sub(\"([0-9]+)\", \"_\\\\1_\", text)\n\n\"Here's my number: _919_-_543_-_3300_. They bought _731_ bananas. Please call _919_._554_._3800_.\"\n\n\nHere we’ll remove commas not used as field separators.\n\ntext = '\"H4NY07011\",\"ACKERMAN, GARY L.\",\"H\",\"$13,242\",,,'\nre.sub(\"([^\\\",]),\", \"\\\\1\", text)\n\n'\"H4NY07011\",\"ACKERMAN GARY L.\",\"H\",\"$13242\",,,'\n\n\nHow does that work? Consider that [^\\\",] matches a character that is not a quote and not a comma. The regex is such a character followed by a comma, with the matched character saved in \\\\1 because of the grouping operator.\n\nChallenge: Instead of removing the commas to remove the ambiguity, how would you convert the comma delimiters to pipe (|) delimiters (since the pipe is rarely used in text)?\n\nExtending the use of \\\\1, we can refer to multiple captured groups:\n\ntext = \"Here's my number: 919-543-3300. They bought 731 bananas. Please call 919.554.3800.\"\nre.sub(\"([0-9]{3})[-\\.]([0-9]{3})[-\\.]([0-9]{4})\", \"area code \\\\1, number \\\\2-\\\\3\", text)\n\n\"Here's my number: area code 919, number 543-3300. They bought 731 bananas. Please call area code 919, number 554-3800.\"\n\n\nGroups can also be given names, instead of having to refer to them by their numbers, but we will not demonstrate that here.\n\nChallenge: Suppose a text string has dates in the form “Aug-3”, “May-9”, etc. and I want them in the form “3 Aug”, “9 May”, etc. How would I do this regex?\n\n\n\nGreedy matching\nFinally, let’s consider where a match ends when there is ambiguity.\nAs a simple example consider that if we try this search, we match as many digits as possible, rather than returning the first “9” as satisfying the request for “one or more” digits.\n\ntext = \"See the 998 balloons.\"\nre.findall(\"\\\\d+\", text)\n\n['998']\n\n\nThat behavior is called greedy matching, and it’s the default. That example also shows why it is the default. What would happen if it were not the default?\nHowever, sometimes greedy matching doesn’t get us what we want.\nConsider this attempt to remove multiple html tags from a string.\n\ntext = \"Do an internship &lt;b&gt; in place &lt;/b&gt; of &lt;b&gt; one &lt;/b&gt; course.\"\nre.sub(\"&lt;.*&gt;\", \"\", text)\n\n'Do an internship  course.'\n\n\nNotice what happens because of greedy matching.\nOne way to avoid greedy matching is to use a ? after the repetition specifier.\n\nre.sub(\"&lt;.*?&gt;\", \"\", text)\n\n'Do an internship  in place  of  one  course.'\n\n\nHowever, that syntax is a bit frustrating because ? is also used to indicate 0 or 1 repetitions, making the regex a bit hard to read/understand.\n\nChallenge: Suppose I want to strip out HTML tags but without using the ? to avoid greedy matching. How can I be more careful in constructing my regex?",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#special-characters-in-python",
    "href": "units/unit5-programming.html#special-characters-in-python",
    "title": "Programming concepts",
    "section": "Special characters in Python",
    "text": "Special characters in Python\nRecall that when characters are used for special purposes, we need to ‘escape’ them if we want them interpreted as the actual (literal) character. In what follows, I show this in Python, but similar manipulations are sometimes needed in the shell and in R.\nThis can get particularly confusing in Python as the backslash is also used to input special characters such as newline (\\n) or tab (\\t). Apparently there was some change in handling escape sequences as of Python 3.12. We now need to do this for the regex \\d:\n\nre.search(\"\\\\d+\", \"a93b\")\n\n&lt;re.Match object; span=(1, 3), match='93'&gt;\n\n\nIn Python 3.11, it was fine to use \\d, but now we need \\\\d, because Python now tries to interpret \\d as a special character (like \\n, but \\d doesn’t exist) and doesn’t pass it directly along as regex syntax.\nHere are some examples of using special characters.\n\ntmp = \"Harry said, \\\"Hi\\\"\"\nprint(tmp)   # This prints out without a newline -- this is hard to show in rendered doc.\n\nHarry said, \"Hi\"\n\ntmp = \"Harry said, \\\"Hi\\\".\\n\"\nprint(tmp)   # This prints out with the newline -- hard to show in rendered doc.\n\nHarry said, \"Hi\".\n\ntmp = [\"azar\", \"foo\", \"hello\\tthere\\n\"]\nprint(tmp[2])\n\nhello   there\n\nre.search(\"[\\tZ]\", tmp[2])   # Search for a tab or a 'Z'.\n\n&lt;re.Match object; span=(5, 6), match='\\t'&gt;\n\n\nHere are some examples of using various special characters in regex syntax. To use a special character as a regular character, we need to escape it (which in Python 3.12 involves two backslashes, as discussed above):\n\n## Search for characters that are not 'z'\n## (using ^ as regular expression syntax)\nre.search(\"[^z]\", \"zero\")\n\n&lt;re.Match object; span=(1, 2), match='e'&gt;\n\n## Show results for various input strings:\nfor st in [\"a^2\", \"93\", \"zzz\", \"zit\", \"azar\"]:\n    print(st + \":\\t\", re.search(\"[^z]\", st))\n\na^2:     &lt;re.Match object; span=(0, 1), match='a'&gt;\n93:  &lt;re.Match object; span=(0, 1), match='9'&gt;\nzzz:     None\nzit:     &lt;re.Match object; span=(1, 2), match='i'&gt;\nazar:    &lt;re.Match object; span=(0, 1), match='a'&gt;\n\n    \n\n## Search for either a '^' (as a regular character) or a 'z':\nfor st in [\"a^2\", \"93\", \"zzz\", \"zit\", \"azar\"]:\n    print(st + \":\\t\", re.search(\"[\\\\^z]\", st))\n\na^2:     &lt;re.Match object; span=(1, 2), match='^'&gt;\n93:  None\nzzz:     &lt;re.Match object; span=(0, 1), match='z'&gt;\nzit:     &lt;re.Match object; span=(0, 1), match='z'&gt;\nazar:    &lt;re.Match object; span=(1, 2), match='z'&gt;\n\n    \n\n## Search for exactly three characters\n## (using . as regular expression syntax)\nfor st in [\"abc\", \"1234\", \"def\"]:\n    print(st + \":\\t\", re.search(\"^.{3}$\", st))\n\nabc:     &lt;re.Match object; span=(0, 3), match='abc'&gt;\n1234:    None\ndef:     &lt;re.Match object; span=(0, 3), match='def'&gt;\n\n    \n\n## Search for a period (as a regular character)\nfor st in [\"3.9\", \"27\", \"4.2\"]:\n    print(st + \":\\t\", re.search(\"\\\\.\", st)) \n\n3.9:     &lt;re.Match object; span=(1, 2), match='.'&gt;\n27:  None\n4.2:     &lt;re.Match object; span=(1, 2), match='.'&gt;\n\n\n\nChallenge Explain why we use a single backslash to get a newline and double backslash to write out a Windows path in the examples here:\n\n## Suppose we want to use a \\ in our string:\nprint(\"hello\\nagain\")\n\nhello\nagain\n\nprint(\"hello\\\\nagain\")\n\nhello\\nagain\n\nprint(\"My Windows path is: C:\\\\Users\\\\nadal.\")\n\nMy Windows path is: C:\\Users\\nadal.\n\n\nAnother way to achieve this effect if your string does not contain any special characters is to prefix your string literal with an r for “raw”:\n\nprint(r\"My Windows path is: C:\\Users\\nadal.\")\n\nMy Windows path is: C:\\Users\\nadal.\n\n\n\nAdvanced note: Searching for an actual backslash gets even more complicated (lookup “backslash plague” or “baskslash hell”), because we need to pass two backslashes as the regular expression, so that a literal backslash is searched for. However, to pass two backslashes, we need to escape each of them with a backslash so Python doesn’t treat each backslash as part of a special character. So that’s four backslashes to search for a single backslash! Yikes. One rule of thumb is just to keep entering backslashes until things work!\n\n## Use and search for an actual backslash\ntmp = \"something \\\\ other\\n\"\nprint(tmp) \n\nsomething \\ other\n\nre.search(\"\\\\\\\\\", tmp)\n\n&lt;re.Match object; span=(10, 11), match='\\\\'&gt;\n\ntry:\n    re.search(\"\\\\\", tmp)\nexcept Exception as error:\n    print(error)\n\nbad escape (end of pattern) at position 0\n\n\nAgain here you can use “raw” strings, at the price of losing the ability to use any special characters:\n\n## Search for an actual backslash\nre.search(r\"\\\\\", tmp)\n\n&lt;re.Match object; span=(10, 11), match='\\\\'&gt;\n\n\nThis tells python to treat this sting literal without any escaping, but that does not apply to the regex engine (or else we would have used a single backslash). So yes. This can be quite confusing.\n\nWarning Be careful when cutting and pasting from documents that are not text files as you may paste in something that looks like a single or double quote, but which Python cannot interpret as a quote because it’s some other ASCII (or Unicode) quote character. If you paste in a ” from PDF, it will not be interpreted as a standard R double quote mark.\n\nSimilar things come up in the shell and in R, but in the shell you often don’t need as many backslashes. E.g. you could do this to look for a literal backslash character.\n\necho \"hello\" &gt; file.txt\necho \"with a \\ there\" &gt;&gt; file.txt\ngrep '\\\\' file.txt\n\nwith a \\ there",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#interacting-with-the-operating-system",
    "href": "units/unit5-programming.html#interacting-with-the-operating-system",
    "title": "Programming concepts",
    "section": "Interacting with the operating system",
    "text": "Interacting with the operating system\nScripting languages allow one to interact with the operating system in various ways. Most allow you to call out to the shell to run arbitrary shell code and save results within your session.\nI’ll assume everyone knows about the following functions/functionality for interacting with the filesystem and file in Python: os.getcwd, os.chdir, import, pickle.dump, pickle.load\nAlso in IPython there is additional functionality/syntax.\nHere are a variety of tools for interacting with the operating system:\n\nTo run UNIX commands from within Python, use subprocess.run(), as follows, noting that we can save the result of a system call to an R object:\n\nimport subprocess, io\nsubprocess.run([\"ls\", \"-al\"])   ## results apparently not shown when compiled...\n\nCompletedProcess(args=['ls', '-al'], returncode=0)\n\nfiles = subprocess.run([\"ls\", \"-al\"], capture_output = True)\nfiles.stdout\n\nb'total 2254\\ndrwxr-sr-x 11 paciorek scfstaff     75 Sep 15 14:29 .\\ndrwxr-sr-x 13 paciorek scfstaff     35 Sep 15 14:28 ..\\n-rw-r--r--  1 paciorek scfstaff   2279 Sep  5 14:27 badCode.R\\n-rw-r--r--  1 paciorek scfstaff    803 Aug 27 11:03 book.yml\\n-rw-r--r--  1 paciorek scfstaff 117142 Aug 27 12:35 chatgpt-regex-numbers.png\\n-rw-r--r--  1 paciorek scfstaff    236 Sep  5 14:14 dummy.py\\n-rw-r--r--  1 paciorek scfstaff    102 Sep  5 14:08 dummy.py~\\n-rw-r--r--  1 paciorek scfstaff 175396 Aug 27 12:35 exampleGraphic.png\\n-rw-r--r--  1 paciorek scfstaff     21 Sep 15 12:11 file.txt\\n-rw-r--r--  1 paciorek scfstaff  14610 Aug 27 12:35 gauss-seidel.png\\n-rw-r--r--  1 paciorek scfstaff   4038 Sep  5 14:27 goodCode.R\\n-rw-r--r--  1 paciorek scfstaff  20260 Aug 27 12:35 graph.png\\n-rw-r--r--  1 paciorek scfstaff   2464 Aug 27 12:35 linked-list.png\\n-rw-r--r--  1 paciorek scfstaff     98 Sep 15 10:47 local.py\\n-rw-r--r--  1 paciorek scfstaff     79 Sep 15 12:21 mymod.py\\ndrwxr-sr-x  4 paciorek scfstaff      7 Sep 13 13:47 mypkg\\n-rw-r--r--  1 paciorek scfstaff    272 Sep  5 14:12 mytestfile.py~\\n-rw-r--r--  1 paciorek scfstaff  27757 Aug 27 12:35 nelder-mead.png\\n-rw-r--r--  1 paciorek scfstaff  63998 Aug 27 12:35 normalized_example.png\\ndrwxr-sr-x  2 paciorek scfstaff     10 Sep 15 12:28 __pycache__\\ndrwxr-sr-x  3 paciorek scfstaff      6 Sep  5 14:16 .pytest_cache\\n-rw-r--r--  1 paciorek scfstaff   1112 Sep  9 08:38 regex.qmd\\n-rw-r--r--  1 paciorek scfstaff   1285 Sep  9 08:19 regex.qmd~\\n-rw-------  1 paciorek scfstaff     72 Sep 13 17:40 .Rhistory\\ndrwxr-sr-x  3 paciorek scfstaff      5 Aug 28 09:41 .ruff_cache\\n-rw-r--r--  1 paciorek scfstaff    573 Sep 12 10:25 run_no_break.py\\n-rw-r--r--  1 paciorek scfstaff    591 Sep 12 10:25 run_with_break.py\\n-rw-r--r--  1 paciorek scfstaff  15667 Aug 27 12:35 steep-descent.png\\n-rw-r--r--  1 paciorek scfstaff    641 Aug 28 12:02 test2.aux\\ndrwxr-sr-x  3 paciorek scfstaff      3 Sep 15 11:40 test2_files\\n-rw-r--r--  1 paciorek scfstaff  45702 Aug 28 12:02 test2.log\\n-rw-r--r--  1 paciorek scfstaff    267 Aug 28 11:56 test2.qmd~\\n-rw-r--r--  1 paciorek scfstaff   8383 Aug 28 12:01 test2.tex\\n-rw-r--r--  1 paciorek scfstaff    267 Aug 28 11:58 test3.qmd~\\n-rw-r--r--  1 paciorek scfstaff    100 Sep 15 11:41 test4.qmd~\\n-rw-r--r--  1 paciorek scfstaff    672 Sep 15 12:28 test5.qmd~\\n-rw-r--r--  1 paciorek scfstaff    139 Sep 15 13:27 test6.qmd~\\n-rw-r--r--  1 paciorek scfstaff   2602 Aug 28 12:19 test.aux\\n-rw-r--r--  1 paciorek scfstaff    981 Sep 15 11:34 _test-bash-jpeg.qmd\\n-rw-r--r--  1 paciorek scfstaff    947 Sep 15 11:04 test-bash-jpeg.qmd~\\n-rw-r--r--  1 paciorek scfstaff    417 Sep  5 14:19 test_dummy.py\\n-rw-r--r--  1 paciorek scfstaff    398 Sep  5 14:16 test_dummy.py~\\ndrwxr-sr-x  3 paciorek scfstaff      3 Sep 15 13:34 test_files\\n-rw-r--r--  1 paciorek scfstaff  42470 Aug 28 12:19 test.log\\n-rw-r--r--  1 paciorek scfstaff  52194 Aug 28 12:19 test.pdf\\n-rw-r--r--  1 paciorek scfstaff     71 Sep  5 14:54 test.py\\n-rw-r--r--  1 paciorek scfstaff    228 Sep 15 11:38 test.qmd~\\n-rw-r--r--  1 paciorek scfstaff     71 Sep  5 14:51 test-save.py\\n-rw-r--r--  1 paciorek scfstaff     66 Sep 13 13:49 test_scope.py\\n-rw-r--r--  1 paciorek scfstaff  24439 Aug 28 12:19 test.tex\\n-rw-r--r--  1 paciorek scfstaff      0 Aug 28 12:19 test.toc\\n-rw-r--r--  1 paciorek scfstaff     10 Sep  3 09:12 tmp2.txt\\ndrwxr-sr-x  3 paciorek scfstaff      3 Sep 13 17:18 tmp_files\\n-rw-r--r--  1 paciorek scfstaff    623 Sep 13 12:05 tmp.qmd~\\n-rw-r--r--  1 paciorek scfstaff     39 Sep  9 10:39 tmp.txt\\n-rw-r--r--  1 paciorek scfstaff   9357 Aug 27 12:35 tree.png\\n-rw-r--r--  1 paciorek scfstaff  11178 Aug 28 09:04 unit1-intro.qmd\\n-rw-r--r--  1 paciorek scfstaff  58593 Sep  3 09:10 unit2-dataTech.qmd\\n-rw-r--r--  1 paciorek scfstaff  56222 Aug 27 10:40 unit2-dataTech.qmd~\\n-rw-r--r--  1 paciorek scfstaff    279 Sep  9 08:44 unit2-test.py\\n-rw-r--r--  1 paciorek scfstaff    226 Aug 31 12:19 unit2-test.py~\\n-rw-r--r--  1 paciorek scfstaff  18614 Sep 13 09:39 unit3-bash.qmd\\n-rw-r--r--  1 paciorek scfstaff  18297 Aug 28 09:29 unit3-bash.qmd~\\n-rw-r--r--  1 paciorek scfstaff  48441 Sep 12 10:58 unit4-goodPractices.qmd\\n-rw-r--r--  1 paciorek scfstaff  41222 Aug 28 09:29 unit4-goodPractices.qmd~\\ndrwxr-sr-x  4 paciorek scfstaff      4 Sep 15 13:33 unit5-programming_cache\\ndrwxr-sr-x  6 paciorek scfstaff      6 Sep 15 14:29 unit5-programming_files\\n-rw-r--r--  1 paciorek scfstaff 130213 Sep 10 17:33 _unit5_programming.qmd\\n-rw-r--r--  1 paciorek scfstaff 136632 Sep 15 14:26 unit5-programming.qmd\\n-rw-r--r--  1 paciorek scfstaff 127531 Sep 10 17:05 unit5-programming.qmd~\\n-rw-r--r--  1 paciorek scfstaff 138228 Sep 15 14:29 unit5-programming.rmarkdown\\n-rw-r--r--  1 paciorek scfstaff 259695 Sep 15 14:29 unit5-programming.tex\\n-rw-r--r--  1 paciorek scfstaff    142 Sep 13 13:50 vec_orig.py\\n-rw-r--r--  1 paciorek scfstaff    142 Sep 15 14:29 vec.py\\n-rw-r--r--  1 paciorek scfstaff    492 Sep 15 12:28 vec.pyc\\n'\n\nwith io.BytesIO(files.stdout) as stream:  # create a file-like object\n     content = stream.readlines()\ncontent[2:4]\n\n[b'drwxr-sr-x 13 paciorek scfstaff     35 Sep 15 14:28 ..\\n', b'-rw-r--r--  1 paciorek scfstaff   2279 Sep  5 14:27 badCode.R\\n']\n\n\nThere are also a bunch of functions that will do specific queries of the filesystem, including\n\nos.path.exists(\"unit2-dataTech.qmd\")\n\nTrue\n\nos.listdir(\"../data\")\n\n['IPs.RData', 'stackoverflow-2016.db', 'RTADataSub.csv', 'test.db', 'airline.csv', 'stackoverflow-2021.db', 'coop.txt.gz', 'co2_annmean_mlo.csv', 'precip.txt', 'airline.parquet', 'precipData.txt', 'hivSequ.csv', 'cpds.csv']\n\n\nThere are some tools for dealing with differences between operating systems. os.path.join is a nice example:\n\nos.listdir(os.path.join(\"..\", \"data\"))\n\n['IPs.RData', 'stackoverflow-2016.db', 'RTADataSub.csv', 'test.db', 'airline.csv', 'stackoverflow-2021.db', 'coop.txt.gz', 'co2_annmean_mlo.csv', 'precip.txt', 'airline.parquet', 'precipData.txt', 'hivSequ.csv', 'cpds.csv']\n\n\nIt’s best if you can to write your code, as shown here with os.path.join, in a way that is agnostic to the underlying operating system (i.e., that works regardless of the operating system).\nTo get some info on the system you’re running on:\n\nimport platform\nplatform.system()\n\n'Linux'\n\nos.uname()\n\nposix.uname_result(sysname='Linux', nodename='smeagol', release='5.15.0-107-generic', version='#117-Ubuntu SMP Fri Apr 26 12:26:49 UTC 2024', machine='x86_64')\n\nplatform.python_version()\n\n'3.11.0'\n\n\nTo retrieve environment variables:\n\nos.environ['PATH']\n\n'/system/linux/mambaforge-3.11/bin:/system/linux/mambaforge-3.11/condabin:/system/linux/mambaforge-3.11/bin:/system/linux/mambaforge-3.11/condabin:/system/linux/mambaforge-3.11/bin:/system/linux/mambaforge-3.11/bin:/system/linux/miniforge-3.12/condabin:/accounts/vis/paciorek/bin:/system/linux/bin:/usr/local/bin:/usr/bin:/usr/sbin:/accounts/vis/paciorek/.local/bin'\n\n\nYou can have an Python script act as a shell script (like running a bash shell script) as follows.\n\nWrite your Python code in a text file, say example.py\nAs the first line of the file, include #!/usr/bin/python (like #!/bin/bash in a bash shell file, as seen in Unit 2) or for more portability across machines, include #!/usr/bin/env python.\nMake the Python code file executable with chmod: chmod ugo+x example.py.\nRun the script from the command line: ./example.py\n\nIf you want to pass arguments into your script, you can do so with the argparse package.\n\nimport argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('-y', '--year', default=2002,\n                    help='year to download')\nparser.add_argument('-m', '--month', default=None,\n                    help='month to download')\nargs = parse.parse_args()\nargs.year\nyear = int(args.year)\n\nNow we can run it as follows in the shell:\n\n./example.py 2004 January\n\nUse Ctrl-C to interrupt execution. This will generally back out gracefully, returning you to a state as if the command had not been started. Note that if Python is exceeding the amount of memory available, there can be a long delay. This can be frustrating, particularly since a primary reason you would want to interrupt is when Python runs out of memory.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#interacting-with-external-code",
    "href": "units/unit5-programming.html#interacting-with-external-code",
    "title": "Programming concepts",
    "section": "Interacting with external code",
    "text": "Interacting with external code\nScripting languages such as R, Python, and Julia allow you to call out to “external code”, which often means C or C++ (but also Fortran, Java and other languages).\nCalling out to external code is particularly important in languages like R and Python that are often much slower than compiled code and less important in a fast language like Julia (which uses Just-In-Time compilation – more on that later).\nIn fact, the predecessor language to R, which was called ‘S’ was developed specifically (at AT&T’s Bell Labs in the 1970s and 1980s) as an interactive wrapper around Fortran, the numerical programming language most commonly used at the time (and still widely relied on today in various legacy codes).\nIn Python, one can directly call out to C or C++ code or one can use Cython to interact with C. With Cython, one can:\n\nHave Cython automatically translate Python code to C, if you provide type definitions for your variables.\nDefine C functions that can be called from your Python code.\n\nIn R, one can call directly out to C or C++ code using .Call or one can use the Rcpp package. Rcpp is specifically designed to be able to write C++ code that feels somewhat like writing R code and where it is very easy to pass data between R and C++.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#modules",
    "href": "units/unit5-programming.html#modules",
    "title": "Programming concepts",
    "section": "Modules",
    "text": "Modules\nA module is a collection of related code in a file with the extension .py. The code can include functions, classes, and variables, as well as runnable code. To access the objects in the module, you need to import the module.\nHere we’ll create mymod.py from the shell, but of course usually one would create it in an editor.\n\ncat &lt;&lt; EOF &gt; mymod.py\nx = 7\nrange = 3\ndef myfun(x):\n    print(\"The arg is: \", str(x), \".\", sep = '')\nEOF\n\n\nimport mymod\nprint(mymod.x)\n\n7\n\nmymod.myfun(7)\n\nThe arg is: 7.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#the-import-statement",
    "href": "units/unit5-programming.html#the-import-statement",
    "title": "Programming concepts",
    "section": "The import statement",
    "text": "The import statement\nThe import statement allows one to get access to code in a module. Importantly it associates the names of the objects in the module with a name accessible in the scope in which it was imported (i.e., the current context). The mapping of names (references) to objects is called a namespace. We discuss scopes and namespaces in more detail later.\n\ndel mymod\ntry:           # Check if `mymod` is in scope.\n    mymod.x\nexcept Exception as error:\n    print(error)\n\nname 'mymod' is not defined\n\ny = 3\n\nimport mymod\nmymod         # This is essentially a dictionary in the current (global) scope.\n\n&lt;module 'mymod' from '/accounts/vis/paciorek/teaching/243fall24/fall-2024/units/mymod.py'&gt;\n\nx             # This is not a name in the current (global) scope.\n\nNameError: name 'x' is not defined\n\nrange         # This is a builtin, not from the module.\n\n&lt;class 'range'&gt;\n\nmymod.x\n\n7\n\ndir(mymod)\n\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'myfun', 'range', 'x']\n\nmymod.x\n\n7\n\nmymod.range\n\n3\n\n\nSo y and mymod are in the global namespace and range and x are in the module namespace of mymod. You can access the built-in range function from the global namespace but it turns out it’s actually in the built-ins scope (more later).\nNote the usefulness of distinguishing the objects in a module from those in the global namespace. We’ll discuss this more in a bit.\nThat said, we can make an object defined in a module directly accessible in the current scope (adding it to the global namespace in this example) at which point it is distinct from the object in the module:\n\nfrom mymod import x\nx   # now part of global namespace\n\n7\n\ndir()\n\n['__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'content', 'files', 'io', 'it', 'm', 'math', 'mymod', 'os', 'pattern', 'platform', 'r', 're', 'return_group', 'st', 'stream', 'subprocess', 'sys', 'text', 'time', 'tmp', 'x', 'y']\n\nmymod.x = 5\nx = 3\nmymod.x\n\n5\n\nx\n\n3\n\n\nBut in general we wouldn’t want to use from to import objects in that fashion because we could introduce name conflicts and we reduce modularity.\nThat said, it can be tedious to always have to type the module name (and in many cases there are multiple submodule names you’d also need to type).\n\nimport mymod as m\nm.x\n\n5",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#packages",
    "href": "units/unit5-programming.html#packages",
    "title": "Programming concepts",
    "section": "Packages",
    "text": "Packages\nA package is a directory containing one or more modules and with a file named __init__.py that is called when a package is imported and serves to initialize the package.\nLet’s create a basic package.\n\nmkdir mypkg\n\ncat &lt;&lt; EOF &gt; mypkg/__init__.py\n## Make objects from mymod.py available as mypkg.foo rather than mypkg.mymod.foo.\n## The \".\" means find \"mymod\" here in this directory.\nfrom .mymod import *\n\nprint(\"Welcome to my package.\")\nEOF\n\ncat &lt;&lt; EOF &gt; mypkg/mymod.py\nx = 7\n\ndef myfun(val):\n    print(f\"Converting {val} to integer: {int(val)}.\")\nEOF\n\nNote that if there were other modules, we could have imported from those as well.\nNow we can use the objects from the module without having to know that it was in a particular module (because of how __init__.py was set up).\n\nimport mypkg\n\nWelcome to my package.\n\nmypkg.x\n\n7\n\nmypkg.myfun(7.3)\n\nConverting 7.3 to integer: 7.\n\n\nNote, one can set __all__ in an __init__.py to define what is imported, which makes clear what is publicly available and hides what is considered internal.\n\nInternal/private objects\nWe could add another module that the main module uses but that is not intended for direct use by the user.\n\ncat &lt;&lt; EOF &gt; mypkg/auxil.py\n\ndef helper(val):\n    return val + 10\nEOF\n\n\ncat &lt;&lt; EOF &gt;&gt; mypkg/mymod.py\n\nfrom .auxil import helper\n\ndef myfun10(val):\n    print(f\"Converting {val} to integer plus 10: {int(helper(val))}.\")\nEOF\n\n\nimport mypkg as mypkg_updated\nmypkg_updated.myfun10(7.3)\n\nConverting 7.3 to integer plus 10: 17.\n\n\n\n\nSubpackages\nPackages can also have modules in nested directories, achieving additional modularity via subpackages. A package can automatically import the subpackages via the main __init__.py or require the user to import them manually, e.g., import mypkg.mysubpkg.\n\nmkdir mypkg/mysubpkg\n\ncat &lt;&lt; EOF &gt; mypkg/mysubpkg/__init__.py\nfrom .values import *\nprint(\"Welcome to my package's subpackage.\")\nEOF\n\ncat &lt;&lt; EOF &gt; mypkg/mysubpkg/values.py\nx = 999\nb = 7\nd = 9\nEOF\n\n\nimport mypkg.mysubpkg     ## Note that __init__.py is invoked\n\nWelcome to my package's subpackage.\n\nmypkg.mysubpkg.b\n\n7\n\n\nNote that a given __init__.py is invoked when importing anything nested within the directory containing the __init__.py.\nIf we wanted to automatically import the subpackage we would add import mypkg.mysubpkg to mypkg/__init__.py.\nOne would generally not import the items from mysubpkg directly into the mypkg namespace but there may be cases one would do something like this. For example np.linspace is actually found in numpy/core/function_base.py, but we don’t need to refer to numpy.core.linspace because of how numpy structures the import statements in __init__.py.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#installing-packages",
    "href": "units/unit5-programming.html#installing-packages",
    "title": "Programming concepts",
    "section": "Installing packages",
    "text": "Installing packages\nIf a package is on PyPI or available through Conda but not on your system, you can install it easily (usually). You don’t need root permission on a machine to install a package, though you may need to use pip install --user or set up a new Conda environment.\nPackages often depend on other packages. In general, if one package depends on another, pip or conda will generally install the dependency automatically.\nOne advantage of Conda is that it can also install non-Python packages on which a Python package depends, whereas with pip you sometimes need to install a system package to satisfy a dependency.\nIt’s not uncommon to run into a case where conda has trouble installing a package because of version inconsistencies amongst the dependencies. mamba is a drop-in replacement for conda and often does a better job of this “dependency resolution”. We use mamba by default on the SCF. In recent versions of Conda, you can also use the Mamba’s dependency resolver when running conda commands by running conda config --set solver libmamba, which puts solver: libmamba in your .condarc file. It’s also generally recommended to use the conda-forge channel (i.e., location) when installing packages with Conda (this is done automatically when using mamba). conda-forge provides a wide variety of up-to-date packages, maintained by the community.\n\nMaking your package installable\nIt’s pretty easy to configure your package so that it can be built and installed via pip. See the structure of this example repository. In fact, one can install the package with only either setup.py or pyproj.toml, but the other files listed here are recommended:\n\npyproj.toml (or pyproject.toml): this is a configuration file used by packaging tools. In the mytoy example it specifies to use setuptools to build and install the package.\nsetup.py: this is run when the package is built and installed when using setuptools. In the example, it simply runs setuptools.setup(). With recent versions of setuptools, you don’t actually need this so long as you have the pyproj.toml file.\nsetup.cfg: provides metadata about the package when using setuptools.\nenvironment.yml: provides information about the full environment in which your package should be used (including examples, documentation, etc.). For projects using setuptools, a minimal list of dependencies needed for installation and use of the package can instead be included in the install_requires option of setup.cfg.\nLICENSE: specifies the license for your package giving the terms under which others can use it.\n\nThe postBuild file is a completely optional file only needed if you want to use the package with a MyBinder environment.\nAt the numpy GitHub repository, by looking in pyproject.toml, you can see that numpy is build and installed using a system called Meson, while at the Jupyter GitHub repository you can see that the jupyter package is built and installed using setuptools.\nBuilding a package usually refers to compiling source code but for a Python package that just has Python code, nothing needs to be compiled. Installing a package means putting the built package into a location on your computer where packages are installed.\nYou can also make your package public on PyPI or through Conda, but that is not something we’ll cover here.\n\n\nReproducibility and package management\nFor reproducibility, it’s important to know the versions of the packages you use (and the version of Python). pip and conda make it easy to do this. You can create a requirements file that captures the packages you are currently using (and, critically, their versions) and then install exactly that set of packages (and versions) based on that requirements file.\n\npip freeze &gt; requirements.txt\npip install -r requirements.txt\n\nconda env export &gt; environment.yml\nconda env create -f environment.yml\n\nConda is a general package manager. You can use it to manage Python packages but lots of other software as well, including R and Julia.\nConda environments provide an additional layer of modularity/reproducibility, allowing you to set up a fully reproducible environment for your computation. Here (by explicitly giving python=3.12) the Python 3.12 executable and all packages you install in the environment are fully independent of whatever Python executables are installed on the system.\n\nconda create -n myenv python=3.12\nsource activate myenv\nconda install numpy\n\n\nWarning If you use conda activate rather than source activate, Conda will prompt you to run conda init, which will make changes to your ~/.bashrc that, for one, activate the Conda base environment automatically when a shell is started. This may be fine, but it’s helpful to be aware.\n\n\n\nPackage locations\nPackages in Python (and in R, Julia, etc.) may be installed in various places on the filesystem, and it sometimes it is helpful (e.g., if you end up with multiple versions of a package installed on your system) to be able to figure out where on the filesystem the package is being loaded from.\nWe can use the __file__ and __version__ objects in a package to see where on the filesystem a package is installed and what version it is:\n\nimport numpy as np\nnp.__file__\n\n'/system/linux/mambaforge-3.11/lib/python3.11/site-packages/numpy/__init__.py'\n\nnp.__version__\n\n'1.23.5'\n\n\n(pip list or conda list will also show version numbers, for all packages).\nsys.path shows where Python looks for packages on your system.\n\n\nSource vs. binary packages\nThe difference between a source package and a binary package is that the source package has the raw Python (and C/C++ and Fortran, in some cases) code as text files, while the binary package has all the non-Python code in a binary/non-text format, with the C/C++ and Fortran code already having been compiled.\nIf you install a package from source, C/C++/Fortran code will be compiled on your system (if the package has such code). That should mean the compiled code will work on your system, but requires you to have a compiler available and things properly configured. A binary package doesn’t need to be compiled on your system, but in some cases the code may not run on your system because it was compiled in such a way that is not compatible with your system.\nPython wheels are a binary package format for Python packages. Wheels for some packages will vary by platform (i.e., operating system) so that the package will install correctly on the system where it is being installed.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#data-structures",
    "href": "units/unit5-programming.html#data-structures",
    "title": "Programming concepts",
    "section": "Data structures",
    "text": "Data structures\nPlease see the data structures section of Unit 2 for some general discussion of data structures.\nWe’ll also see more complicated data structures when we consider objects in the section on object-oriented programming.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#types-and-classes",
    "href": "units/unit5-programming.html#types-and-classes",
    "title": "Programming concepts",
    "section": "Types and classes",
    "text": "Types and classes\n\nOverview and static vs. dynamic typing\nThe term ‘type’ refers to how a given piece of information is stored and what operations can be done with the information.\n‘Primitive’ types are the most basic types that often relate directly to how data are stored in memory or on disk (e.g., boolean, integer, numeric (real-valued, aka double or floating point), character, pointer (aka address, reference).\nIn compiled languages like C and C++, one has to define the type of each variable. Such languages are statically typed. Interpreted (or scripting) languages such as Python and R have dynamic types. One can associate different types of information with a given variable name at different times and without declaring the type of the variable:\n\nx = 'hello'\nprint(x)\n\nhello\n\nx = 7\nx*3\n\n21\n\n\nIn contrast in a language like C, one has to declare a variable based on its type before using it:\n\ndouble y;\ndouble x = 3.1;\ny = x * 7.1;\n\nDynamic typing can be quite helpful from the perspective of quick implementation and avoiding tedious type definitions and problems from minor inconsistencies between types (e.g., multiplying an integer by a real-valued number). But static typing has some critical advantages from the perspective of software development, including:\n\nprotecting against errors from mismatched values and unexpected user inputs, and\ngenerally much faster execution because the type of a variable does not need to be checked when the code is run.\n\nMore complex types in Python (and in R) often use references (pointers, aka addresses) to the actual locations of the data. We’ll see this in detail when we discuss Memory.\n\n\nTypes in Python\nYou should be familiar with the important built-in data types in Python, most importantly lists, tuples, and dictionaries, as well as basic scalar types such as integers, floats, and strings.\nLet’s look at the type of various built-in data structures in Python and in numpy, which provides important types for numerical computing.\n\nx = 3\ntype(x)\n\n&lt;class 'int'&gt;\n\nx = 3.0\ntype(x)\n\n&lt;class 'float'&gt;\n\nx = 'abc'\ntype(x)\n\n&lt;class 'str'&gt;\n\nx = False\ntype(x)\n\n&lt;class 'bool'&gt;\n\nx = [3, 3.0, 'abc']\ntype(x)\n\n&lt;class 'list'&gt;\n\nimport numpy as np\n\nx = np.array([3, 5, 7])  ## array of integers\ntype(x)\n\n&lt;class 'numpy.ndarray'&gt;\n\ntype(x[0])\n\n&lt;class 'numpy.int64'&gt;\n\nx = np.random.normal(size = 3) # array of floats (aka 'doubles')\ntype(x[0])\n\n&lt;class 'numpy.float64'&gt;\n\nx = np.random.normal(size = (3,4)) # multi-dimensional array\ntype(x)\n\n&lt;class 'numpy.ndarray'&gt;\n\n\nSometimes numpy may modify a type to make things easier for you, which often works well, but you may want to control it yourself to be sure:\n\nx = np.array([3, 5, 7.3])\nx\n\narray([3. , 5. , 7.3])\n\ntype(x[0])\n\n&lt;class 'numpy.float64'&gt;\n\nx = np.array([3.0, 5.0, 7.0]) # Force use of floats (either `3.0` or `3.`).\ntype(x[0])\n\n&lt;class 'numpy.float64'&gt;\n\nx = np.array([3, 5, 7], dtype = 'float64')\ntype(x[0])\n\n&lt;class 'numpy.float64'&gt;\n\n\nThis can come up when working on a GPU, where the default is usually 32-bit (4-byte) numbers instead of 64-bit (8-byte) numbers. ### Composite objects\nMany objects can be composite (e.g., a list of dictionaries or a dictionary of lists, tuples, and strings).\n\nmydict = {'a': 3, 'b': 7}\nmylist = [3, 5, 7]\n\nmylist[1] = mydict\nmylist\n\n[3, {'a': 3, 'b': 7}, 7]\n\nmydict['a'] = mylist\n\n\n\nMutable objects\nMost objects in Python can be modified in place (i.e., modifying only some of the object), but tuples, strings, and sets are immutable:\n\nx = (3,5,7)\ntry:\n    x[1] = 4\nexcept Exception as error:\n    print(error)\n\n'tuple' object does not support item assignment\n\ns = 'abc'\ns[1]\n\n'b'\n\ntry:\n    s[1] = 'y'\nexcept Exception as error:\n    print(error)\n\n'str' object does not support item assignment\n\n\n\n\nConverting between types\nThis also goes by the term coercion and casting. Casting often needs to be done explicitly in compiled languages and somewhat less so in interpreted languages like Python.\nWe can cast (coerce) between different basic types:\n\ny = str(x[0])\ny\n\n'3'\n\ny = int(x[0])\ntype(y)\n\n&lt;class 'int'&gt;\n\n\nSome common conversions are converting numbers that are being interpreted as strings into actual numbers and converting between booleans and numeric values.\nIn some cases Python will automatically do conversions behind the scenes in a smart way (or occasionally not so smart way). Consider these attempts/examples of implicit coercion:\n\nx = np.array([False, True, True])\nx.sum()         # What do you think is going to happen?\n\n2\n\nx = np.random.normal(size = 5)\ntry:\n    x[3] = 'hat'    # What do you think is going to happen?\nexcept Exception as error:\n    print(error)\n\ncould not convert string to float: 'hat'\n\n  \nmyArray = [1, 3, 5, 9, 4, 7]\n# myArray[2.0]    # What do you think is going to happen?\n# myArray[2.73]   # What do you think is going to happen?\n\nR is less strict and will do conversions in some cases that Python won’t:\n\nx &lt;- rnorm(5)\nx[2.0]\n\n[1] 0.02045718\n\nx[2.73]\n\n[1] 0.02045718\n\n\nWhat are the advantages and disadvantages of the different behaviors of Python and R?\n\n\nDataframes\nHopefully you’re also familiar with the Pandas dataframe type.\nPandas picked up the idea of dataframes from R and functionality is similar in many ways to what you can do with R’s dplyr package.\ndplyr and pandas provide a lot of functionality for the “split-apply-combine” framework of working with “rectangular” data.\nOften analyses are done in a stratified fashion - the same operation or analysis is done on subsets of the data set. The subsets might be different time points, different locations, different hospitals, different people, etc.\nThe split-apply-combine framework is intended to operate in this kind of context: - first one splits the dataset by one or more variables, - then one does something to each subset, and - then one combines the results.\nsplit-apply-combine is also closely related to the famous Map-Reduce framework underlying big data tools such as Hadoop and Spark.\nIt’s also very similar to standard SQL queries involving filtering, grouping, and aggregation.\n\n\nPython object protocols\nThere are a number of broad categories of kinds of objects: mapping, number, sequence, iterator. These are called object protocols.\nAll objects that fall in a given category share key characteristics. For example sequence objects have a notion of “next”, while iterator objects have a notion of “stopping”.\nIf you implement your own class that falls into one of these categories, it should follow the relevant protocol by providing the required methods. For example a container class that supports iteration should provide the __iter__ and __next__ methods.\nHere we see that tuples are iterable containers:\n\nmytuple = (\"apple\", \"banana\", \"cherry\")\n\nfor item in mytuple:\n    print(item)\n\napple\nbanana\ncherry\n\nmyit = iter(mytuple)\n\nprint(next(myit))\n\napple\n\nprint(next(myit))\n\nbanana\n\nmyit.__next__()\n\n'cherry'\n\nx = zip(['clinton', 'bush', 'obama', 'trump'], ['Dem', 'Rep', 'Dem', 'Rep'])\nnext(x)\n\n('clinton', 'Dem')\n\nnext(x)\n\n('bush', 'Rep')\n\n\nWe can also go from an iterable object to a standard list:\n\nr = range(5)\nr\n\nrange(0, 5)\n\nlist(r)\n\n[0, 1, 2, 3, 4]",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#principles",
    "href": "units/unit5-programming.html#principles",
    "title": "Programming concepts",
    "section": "Principles",
    "text": "Principles\nSome of the standard concepts in object-oriented programming include encapsulation, inheritance, polymorphism, and abstraction.\nEncapsulation involves preventing direct access to internal data in an object from outside the object. Instead the class is designed so that access (reading or writing) happens through the interface set up by the programmer (e.g., ‘getter’ and ‘setter’ methods). However, Python actually doesn’t really enforce the notion of internal or private information.\nInheritance allows one class to be based on another class, adding more specialized features. For example in the statsmodels package, the OLS class inherits from the WLS class.\nPolymorphism allows for different behavior of an object or function depending on the context. A polymorphic function behaves differently depending on the input types. For example, think of a print function or an addition operator behaving differently depending on the type of the input argument(s). A polymorphic object is one that can belong to different classes (e.g., based on inheritance), and a given method name can be used with any of the classes. An example would be having a base or super class called ‘algorithm’ and various specific machine learning algorithms inheriting from that class. All of the classes might have a ‘predict’ method.\nAbstraction involves hiding the details of how something is done (e.g., via the method of a class), giving the user an interface to provide inputs and get outputs. By making the actual computation a black box, the programmer can modify the internals without changing how a user uses the system.\nClasses generally have constructors that initialize objects of the class and destructors that remove objects.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#classes-in-python",
    "href": "units/unit5-programming.html#classes-in-python",
    "title": "Programming concepts",
    "section": "Classes in Python",
    "text": "Classes in Python\nPython provides a pretty standard approach to writing object-oriented code focused on classes.\nOur example is to create a class for working with random time series. Each object of the class has specific parameter values that control the stochastic behavior of the time series. With a given object we can simulate one or more time series (realizations).\nHere’s the initial definition of the class with methods and fields.\n\nimport numpy as np\n\nclass tsSimClass:\n    '''\n    Class definition for time series simulators\n    '''\n    def __init__(self, times, mean = 0, corParam = 1):\n        ## add assertions that corParam is numeric, length 1 and times is np array\n        self._times = times\n        self.n = len(times)\n        self.mean = mean\n        self.corParam = corParam\n        self._currentU = False\n        self._calcMats()\n    def __str__(self):    # 'print' method\n        return f\"An object of class `tsSimClass` with {self.n} time points.\"\n    def __len__(self):\n        return self.n\n    def setTimes(self, newTimes):\n        self._times = newTimes\n        self._calcMats()\n    def getTimes(self):\n        return self._times\n    def simulate(self):\n        if not self._currentU:    \n            self._calcMats()\n        ## analogous to mu+sigma*z for generating N(mu, sigma^2)\n        return self.mean + np.dot(self.U.T, np.random.normal(size = self.n))\n    def _calcMats(self):\n        ## calculates correlation matrix and Cholesky factor\n        lagMat = np.abs(self._times[:, np.newaxis] - self._times)\n        corMat = np.exp(-lagMat ** 2 / self.corParam ** 2)\n        self.U = np.linalg.cholesky(corMat)\n        print(\"Done updating correlation matrix and Cholesky factor.\")\n        self._currentU = True\n\nNow let’s see how we would use the class.\n\nmyts = tsSimClass(np.arange(1, 101), 2, 1)\n\nDone updating correlation matrix and Cholesky factor.\n\nprint(myts)\n\nAn object of class `tsSimClass` with 100 time points.\n\nnp.random.seed(1)\n## here's a simulated time series\ny1 = myts.simulate()\n\nimport matplotlib.pyplot as plt\nplt.plot(myts.getTimes(), y1, '-')\nplt.xlabel('time')\nplt.ylabel('process values')\n## simulate a second series\ny2 = myts.simulate()\nplt.plot(myts.getTimes(), y2, '--')\nplt.show()\n\n\n\n\n\n\n\n\nWe could set up a different object that has different parameter values. That new simulated time series is less wiggly because the corParam value is larger than before.\n\nmyts2 = tsSimClass(np.arange(1, 101), 2, 4)\n\nDone updating correlation matrix and Cholesky factor.\n\nnp.random.seed(1)\n## here's a simulated time series with a different value of\n## the correlation parameter (corParam)\ny3 = myts2.simulate()\n\nplt.plot(myts2.getTimes(), y3, '-', color = 'red')\nplt.xlabel('time')\nplt.ylabel('process values')\nplt.show()\n\n\n\n\n\n\n\n\n\nCopies and references\nNext let’s think about when copies are made. In the next example mytsRef is a copy of myts in the sense that both names point to the same underlying object. But no data were copied when the assignment to mytsRef was done.\n\nmytsRef = myts\n## 'mytsRef' and 'myts' are names for the same underlying object\nimport copy\nmytsFullCopy = copy.deepcopy(myts)\n\n## Now let's change the values of a field\nmyts.setTimes(np.arange(1,1001,10))\n\nDone updating correlation matrix and Cholesky factor.\n\nmyts.getTimes()[0:4] \n\narray([ 1, 11, 21, 31])\n\nmytsRef.getTimes()[0:4] # the same as `myts`\n\narray([ 1, 11, 21, 31])\n\nmytsFullCopy.getTimes()[0:4] # different from `myts`\n\narray([1, 2, 3, 4])\n\n\nIn contrast mytsFullCopy is a reference to a different object, and all the data from myts had to be copied over to mytsFullCopy. This takes additional memory (and time), but is also safer, as it avoids the possibility that the user might modify myts and not realize that they were also affecting mytsRef. We’ll discuss this more when we discuss copying in the section on memory use.\n\n\nEncapsulation\nThose of you familiar with OOP will probably be familiar with the idea of public and private fields and methods.\nWhy have private fields (i.e., encapsulation)? The use of private fields shields them from modification by users. Python doesn’t really provide this functionality but by convention, attributes whose name starts with _ are considered private. In this case, we don’t want users to modify the times field. Why is this important? In this example, the correlation matrix and the Cholesky factor U are both functions of the vector of times. So we don’t want to allow a user to directly modify times. If they did, it would leave the fields of the object in inconsistent states. Instead we want them to use setTimes, which correctly keeps all the fields in the object internally consistent (by calling _calcMats). It also allows us to improve efficiency by controlling when computationally expensive operations are carried out.\nIn a module, objects that start with _ are a weak form of private attributes. Users can access them, but from foo import * does not import them.\n\n\nChallenge\n\nChallenge\nHow would you get Python to quit immediately, without asking for any more information, when you simply type q (no parentheses!) instead of quit()? There are actually a couple ways to do this. (Hint: you can do this by understanding what happens when you type q and how to exploit the characteristics of Python classes.)\n\n\n\nInheritance\nInheritance can be a powerful way to reduce code duplication and keep your code organized in a logical (nested) fashion. Special cases can be simple extensions of more general classes.\n\nclass Bear:\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n      def __str__(self):\n          return f\"A bear named '{self.name}' of age {self.age}.\"\n      def color(self):\n          return \"unknown\"\n\nclass GrizzlyBear(Bear):\n      def __init__(self, name, age, num_people_killed = 0):\n          super().__init__(name, age)\n          self.num_people_killed = num_people_killed\n      def color(self):\n          return \"brown\"\n\nyog = Bear(\"Yogi the Bear\", 23)\nprint(yog)\n\nA bear named 'Yogi the Bear' of age 23.\n\nyog.color()\n\n'unknown'\n\nnum399 = GrizzlyBear(\"Jackson Hole Grizzly 399\", 35)\nprint(num399)\n\nA bear named 'Jackson Hole Grizzly 399' of age 35.\n\nnum399.color()\n\n'brown'\n\nnum399.num_people_killed\n\n0\n\n\nHere the GrizzlyBear class has additional fields/methods beyond those inherited from the base class (the Bear class), i.e., num_people_killed (since grizzly bears are much more dangerous than some other kinds of bears), and perhaps additional or modified methods. Python uses the methods specific to the GrizzlyBear class if present before falling back to methods of the Bear class if not present in the GrizzlyBear class.\nThe above is an example of polymorphism. Instances of the GrizzlyBear class are polymorphic because they can have behavior from both the GrizzlyBear and Bear classes. The color method is polymorphic in that it can be used for both classes but is defined to behave differently depending on the class.\nMore relevant examples of inheritance in Python and R include how regression models are handled. E.g., in Python’s statsmodels, the OLS class inherits from the WLS class.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#attributes",
    "href": "units/unit5-programming.html#attributes",
    "title": "Programming concepts",
    "section": "Attributes",
    "text": "Attributes\nBoth fields and methods are attributes.\nWe saw the notion of attributes when looking at HTML and XML, where the information was stored as key-value pairs that in many cases had additional information in the form of attributes.\n\nClass attributes vs. instance attributes\nHere count is a class attribute while name and age are instance attributes.\n\nclass Bear:\n      count = 0\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n          Bear.count += 1\n\nyog = Bear(\"Yogi the Bear\", 23)\nyog.count\n\n1\n\nsmoke = Bear(\"Smoky the Bear\", 77)\nsmoke.count\n\n2\n\n\nThe class attribute allows us to manipulate information relating to all instances of the class, as seen here where we keep track of the number of bears that have been created.\n\n\nAdding attributes\nIt turns out we can add instance attributes on the fly in some cases, which is a bit disconcerting in some ways.\n\nyog.bizarre = 7\nyog.bizarre\n\n7\n\ndef foo(x):\n    print(x)\n\nfoo.bizarre = 3\nfoo.bizarre\n\n3",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#generic-function-oop",
    "href": "units/unit5-programming.html#generic-function-oop",
    "title": "Programming concepts",
    "section": "Generic function OOP",
    "text": "Generic function OOP\nLet’s consider the len function in Python. It seems to work magically on various kinds of objects.\n\nx = [3, 5, 7]\nlen(x)\n\n3\n\nx = np.random.normal(size = 5)\nlen(x)\n\n5\n\nx = {'a': 2, 'b': 3}\nlen(x)\n\n2\n\n\nSuppose you were writing the len function. What would you have to do to make it work as it did above? What would happen if a user wants to use len with a class that they define?\nInstead, Python implements the len function by calling the __len__ method of the class that the argument belongs to.\n\nx = {'a': 2, 'b': 3}\nlen(x)\n\n2\n\nx.__len__()\n\n2\n\n\n__len__ is a dunder method (a “Double-UNDERscore” method), which we’ll discuss more in a bit.\nSomething similar occurs with operators:\n\nx = 3\nx + 5\n\n8\n\nx = 'abc'\nx + 'xyz'\n\n'abcxyz'\n\nx.__add__('xyz')\n\n'abcxyz'\n\n\nThis use of generic functions is convenient in that it allows us to work with a variety of kinds of objects using familiar functions.\nThe use of such generic functions and operators is similar in spirit to function or method overloading in C++ and Java. It is also how the (very) old S3 system in R works. And it’s a key part of the (fairly) new Julia language.\n\nWhy use generic functions?\nThe Python developers could have written len as a regular function with a bunch of if statements so that it can handle different kinds of input objects.\nThis has some disadvantages:\n\nWe need to write the code that does the checking.\nFurthermore, all the code for the different cases all lives inside one potentially very long function, unless we create class-specific helper functions.\nMost importantly, len will only work for existing classes. And users can’t easily extend it for new classes that they create because they don’t control the len (built-in) function. So a user could not add the additional conditions/classes in a big if-else statement. The generic function approach makes the system extensible – we can build our own new functionality on top of what is already in Python.\n\n\nThe print function\nLike len, print is a generic function, with various class-specific methods.\nWe can write a print method for our own class by defining the __str__ method as well as a __repr__ method giving what to display when the name of an object is typed.\n\nclass Bear:\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n\nyog = Bear(\"Yogi the Bear\", 23)\nprint(yog)\n\n&lt;__main__.Bear object at 0x7f9bc9b85c10&gt;\n\nclass Bear:\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n      def __str__(self):\n          return f\"A bear named {self.name} of age {self.age}.\"\n      def __repr__(self):\n          return f\"Bear(name={self.name}, age={self.age})\"\n\nyog = Bear(\"Yogi the Bear\", 23)\nprint(yog)   # Invokes __str__\n\nA bear named Yogi the Bear of age 23.\n\nyog          # Invokes __repr__\n\nBear(name=Yogi the Bear, age=23)\n\n\n\n\n\nMultiple dispatch OOP\nThe dispatch system involved in len and + involves only the first argument to the function (or operator). In contrast, Julia emphasizes the importance of multiple dispatch as particularly important for mathematical computation. With multiple dispatch, the specific method can be chosen based on more than one argument.\nIn R, the old (but still used in some contexts) S4 system in R and the new R7 system both provide for multiple dispatch.\nAs a very simple example unrelated to any specific language, multiple dispatch would allow one to do the following with the addition operator:\n3 + 7    # 10\n3 + 'a'  # '3a'\n'hi' +  ' there'  # 'hi there'\nThe idea of having the behavior of an operator or function adapt to the type of the input(s) is one aspect of polymorphism.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#the-python-object-model-and-dunder-methods.",
    "href": "units/unit5-programming.html#the-python-object-model-and-dunder-methods.",
    "title": "Programming concepts",
    "section": "The Python object model and dunder methods.",
    "text": "The Python object model and dunder methods.\nNow that we’ve seen the basics of classes, as well as generic function OOP, we’re in a good position to understand the Python object model.\nObjects are dictionaries that provide a mapping from attribute names to their values, either fields or methods.\ndunder methods are special methods that Python will invoke when various functions are called on instances of the class or other standard operations are invoked. They allow classes to interact with Python’s built-ins.\nHere are some important dunder methods:\n\n__init__ is the constructor (initialization) function that is called when the class name is invoked (e.g., Bear(...))\n__len__ is called by len()\n__str__ is called by print()\n__repr__ is called when an object’s name is invoked\n__call__ is called if the instance is invoked as a function call (e.g., yog() in the Bear case)\n__add__ is called by the + operator.\n\nLet’s see an example of defining a dunder method for the Bear class.\n\nclass Bear:\n      def __init__(self, name, age):\n          self.name = name\n          self.age = age\n      def __str__(self):\n          return f\"A bear named {self.name} of age {self.age}.\"\n      def __add__(self, value):\n          self.age += value\n\nyog = Bear(\"Yogi the Bear\", 23)\nyog + 12\nprint(yog)\n\nA bear named Yogi the Bear of age 35.\n\n\nMost of the things we work with in Python are objects. Functions are also objects, as are classes.\n\ntype(len)\n\n&lt;class 'builtin_function_or_method'&gt;\n\ndef foo(x):\n    print(x)\n\ntype(foo)\n\n&lt;class 'function'&gt;\n\ntype(Bear)\n\n&lt;class 'type'&gt;",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#functional-programming-in-python",
    "href": "units/unit5-programming.html#functional-programming-in-python",
    "title": "Programming concepts",
    "section": "Functional programming (in Python)",
    "text": "Functional programming (in Python)\n\nOverview of functional programming\nFunctional programming is an approach to programming that emphasizes the use of modular, self-contained functions. Such functions should operate only on arguments provided to them (avoiding global variables), and produce no side effects, although in some cases there are good reasons for making an exception. Another aspect of functional programming is that functions are considered ‘first-class’ citizens in that they can be passed as arguments to another function, returned as the result of a function, and assigned to variables. In other words, a function can be treated as any other variable.\nIn many cases (including Python and R), anonymous functions (also called ‘lambda functions’) can be created on-the-fly for use in various circumstances.\nOne can do functional programming in Python by focusing on writing modular, self-contained functions rather than classes. And functions are first-class citizens. However, there are aspects of Python that do not align with the principles mentioned above.\n\nPython’s pass-by-reference behavior causes functions to potentially have the important side effects of modifying arguments that are mutable (e.g., lists and numpy arrays but not tuples) if the programmer is not careful about not modifying arguments within functions.\nSome operations are carried out by statements (e.g., import, def) rather than functions.\n\nIn contrast, R functions have pass-by-value behavior, which is more consistent with a pure functional programming approach.\n\n\nThe principle of no side effects\nBefore we discuss Python further, let’s consider how R behaves in more detail as R conforms more strictly to a functional programming perspective.\nMost functions available in R (and ideally functions that you write as well) operate by taking in arguments and producing output that is then (presumably) used subsequently. The functions generally don’t have any effect on the state of your R environment/session other than the output they produce.\nAn important reason for this (plus for not using global variables) is that it means that it is easy for people using the language to understand what code does. Every function can be treated a black box – you don’t need to understand what happens in the function or worry that the function might do something unexpected (such as changing the value of one of your variables). The result of running code is simply the result of a composition of functions, as in mathematical function composition.\nOne aspect of this is that R uses a pass-by-value approach to function arguments. In R (but not Python), when you pass an object in as an argument and then modify it in the function, you are modifying a local copy of the variable that exists in the context (the frame) of the function and is deleted when the function call finishes:\n\nx &lt;- 1:3\nmyfun &lt;- function(x) {\n      x[2] &lt;- 7\n      print(x)\n      return(x)\n}\n\nnew_x &lt;- myfun(x)\n\n[1] 1 7 3\n\nx   # unmodified\n\n[1] 1 2 3\n\n\nIn contrast, Python uses a pass-by-reference approach, seen here:\n\nx = np.array([1,2,3])\ndef myfun(x):\n  x[1] = 7\n  return x\n\nnew_x = myfun(x)\nx   # modified!\n\narray([1, 7, 3])\n\n\nAnd actually, given the pass-by-reference behavior, we would probably use a version of myfun that looks like this:\n\nx = np.array([1,2,3])\ndef myfun(x):\n  x[1] = 7\n  return None\n\nmyfun(x)\nx   # modified!\n\narray([1, 7, 3])\n\n\nNote how easy it would be for a Python programmer to violate the ‘no side effects’ principle. In fact to avoid it, we need to do some additional work in terms of making a copy of x to a new location in memory before modifying it in the function.\n\nx = np.array([1,2,3])\ndef myfun(x):\n  y = x.copy()\n  y[1] = 7\n  return y\n\nnew_x = myfun(x)\nx   # no side effects!\n\narray([1, 2, 3])\n\n\nMore on pass-by-value vs. pass-by-reference later.\nEven in R, there are some (necessary) exceptions to the idea of no side effects, such as par() and plot().\n\n\nFunctions are first-class objects\nEverything in Python is an object, including functions and classes. We can assign functions to variables in the same way we assign numeric and other values.\nWhen we make an assignment we associate a name (a ‘reference’) with an object in memory. Python can find the object by using the name to look up the object in the namespace.\n\nx = 3\ntype(x)\n\n&lt;class 'int'&gt;\n\ntry:\n    x(3)  # x is not a function (yet)\nexcept Exception as error:\n    print(error)\n\n'int' object is not callable\n\ndef x(val):\n    return pow(val, 2)\n\nx(3)\n\n9\n\ntype(x)\n\n&lt;class 'function'&gt;\n\n\nWe can call a function based on the text name of the function.\n\nfunction = getattr(np, \"mean\")\nfunction(np.array([1,2,3]))\n\n2.0\n\n\nWe can also pass a function into another function as the actual function object. This is an important aspect of functional programming. We can do it with our own function or (as we’ll see shortly) with various built-in functions, such as map.\n\ndef apply_fun(fun, a):\n    return fun(a)\n\napply_fun(round, 3.5)\n\n4\n\n\nA function that takes a function as an argument, returns a function as a result, or both is known as a higher-order function.\n\n\nWhich operations are function calls?\nPython provides various statements that are not formal function calls but allow one to modify the current Python session:\n\nimport: import modules or packages\ndef: define functions or classes\nreturn: return results from a function\ndel: remove an object\n\nOperators are examples of generic function OOP, where the appropriate method of the class of the first object that is part of the operation is called.\n\nx = np.array([0,1,2])\nx - 1\n\narray([-1,  0,  1])\n\nx.__sub__(1)\n\narray([-1,  0,  1])\n\nx\n\narray([0, 1, 2])\n\n\nNote that the use of the operator does not modify the object.\n(Note that you can use return(x) and del(x) but behind the scenes the Python interpreter is intepreting those as return x and del x.)\n\n\nMap operations\nA map operation takes a function and runs the function on each element of some collection of items, analogous to a mathematical map. This kind of operation is very commonly used in programming, particularly functional programming, and often makes for clean, concise, and readable code.\nPython provides a variety of map-type functions: map (a built-in) and pandas.apply. These are examples of higher-order functions – functions that take a function as an argument. Another map-type operation is list comprehension, shown here:\n\nx = [1,2,3]\ny = [pow(val, 2) for val in x]\ny\n\n[1, 4, 9]\n\n\nIn Python, map is run on the elements of an iterable object. Such objects include lists as well as the result of range() and other functions that produce iterables.\n\nx = [1.0, -2.7, 3.5, -5.1]\nlist(map(abs, x))\n\n[1.0, 2.7, 3.5, 5.1]\n\nlist(map(pow, x, [2,2,2,2]))\n\n[1.0, 7.290000000000001, 12.25, 26.009999999999998]\n\n\nOr we can use lambda functions to define a function on the fly:\n\nx = [1.0, -2.7, 3.5, -5.1]\nresult = list(map(lambda vals: vals * 2, x))\n\nIf you need to pass another argument to the function you can use a lambda function as above or functools.partial:\n\nfrom functools import partial\n\n# Create a new round function with 'ndigits' argument pre-set\nround3 = partial(round, ndigits = 3)\n\n# Apply the function to a list of numbers\nlist(map(round3, [32.134234, 7.1, 343.7775]))\n\n[32.134, 7.1, 343.777]\n\n\nLet’s compare using a map-style operation (with Pandas) to using a for loop to run a stratified analysis for a generic example (this code won’t run because the variables don’t exist):\n\n# stratification \nsubsets = df.groupby('grouping_variable')\n\n# map using pandas.apply: one line, easy to understand\nresults = subsets.apply(analysis_function)\n\n# for loop: needs storage set up and multiple lines\nresults &lt;- []\nfor _,subset in subsets:   # iterate over the key-value pairs (the subsets)\n  results.append(analysis_function(subset))\n\nMap operations are also at the heart of the famous MapReduce framework, used in Hadoop and Spark for big data processing.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#function-evaluation-frames-and-the-call-stack",
    "href": "units/unit5-programming.html#function-evaluation-frames-and-the-call-stack",
    "title": "Programming concepts",
    "section": "Function evaluation, frames, and the call stack",
    "text": "Function evaluation, frames, and the call stack\n\nOverview\nWhen we run code, we end up calling functions inside of other function calls. This leads to a nested series of function calls. The series of calls is the call stack. The stack operates like a stack of cafeteria trays - when a function is called, it is added to the stack (pushed) and when it finishes, it is removed (popped).\nUnderstanding the series of calls is important when reading error messages and debugging. In Python, when an error occurs, the call stack is shown, which has the advantage of giving the complete history of what led to the error and the disadvantage of producing often very verbose output that can be hard to understand. (In contrast, in R, only the function in which the error occurs is shown, but you can see the full call stack by invoking traceback().)\nWhat happens when an Python function is evaluated?\n\nThe user-provided function arguments are evaluated in the calling scope and the results are matched to the argument names in the function definition.\nA new frame containing a new namespace is created to store information related to the function call and placed on the stack. Assignment to the argument names is done in the namespace, including any default arguments.\nThe function is evaluated in the (new) local scope. Any look-up of variables not found in the local scope (using the namespace that was created) is done using the lexical scoping rules to look in the series of enclosing scopes (if any exist), then in the global/module scope, and then in the built-ins scope.\nWhen the function finishes, the return value is passed back to the calling scope and the frame is taken off the stack. The namespace is removed, unless the namespace is the enclosing scope for an existing namespace.\n\nI’m not expecting you to fully understand that previous paragraph and all the terms in it yet. We’ll see all the details as we proceed through this Unit.\n\n\nFrames and the call stack\nPython keeps track of the call stack. Each function call is associated with a frame that has a namespace that contains the local variables for that function call.\nThere are a bunch of functions that let us query what frames are on the stack and access objects in particular frames of interest. This gives us the ability to work with objects in the frame from which a function was called.\nWe can use functions from the traceback package to query the call stack.\n\nimport traceback\n\ndef function_a():\n    function_b()\n\ndef function_b():\n    function_c()\n\ndef function_c():\n    traceback.print_stack()\n\nfunction_a()\n\n  File \"&lt;string&gt;\", line 2, in &lt;module&gt;\n  File \"&lt;string&gt;\", line 3, in function_a\n  File \"&lt;string&gt;\", line 3, in function_b\n  File \"&lt;string&gt;\", line 3, in function_c",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#function-inputs-and-outputs",
    "href": "units/unit5-programming.html#function-inputs-and-outputs",
    "title": "Programming concepts",
    "section": "Function inputs and outputs",
    "text": "Function inputs and outputs\n\nArguments\nYou can see the arguments (and any default values) for a function using the help system.\nLet’s create an example function:\n\ndef add(x, y, z=1, absol=False):\n    if absol:\n        return(abs(x+y+z))\n    else:\n        return(x+y+z)\n\nWhen using a function, there are some rules that must be followed.\nArguments without defaults are required.\nArguments can be specified by position (based on the order of the inputs) or by name (keyword), using name=value, with positional arguments appearing first.\n\nadd(3, 5)\n\n9\n\nadd(3, 5, 7)\n\n15\n\nadd(3, 5, absol=True, z=-5)\n\n3\n\nadd(z=-5, x=3, y=5)\n\n3\n\ntry:\n    add(3)\nexcept Exception as error:\n    print(error)\n\nadd() missing 1 required positional argument: 'y'\n\n\nHere’s another error related to positional vs. keyword arguments.\n\nadd(z=-5, 3, 5)  ## Can't trap `SyntaxError` with `try`\n# SyntaxError: positional argument follows keyword argument  \n\nFunctions may have unspecified arguments, which are designated using *args. (‘args’ is a convention - you can call it something else). Unspecified arguments occurring at the beginning of the argument list are generally a collection of like objects that will be manipulated (consider print).\nHere’s an example where we see that we can manipulate args, which is a tuple, as desired.\n\ndef sum_args(*args):\n    print(args[2])\n    total = sum(args)\n    return total\n\nresult = sum_args(1, 2, 3, 4, 5)\n\n3\n\nprint(result)  # Output: 15\n\n15\n\n\nThis syntax also comes in handy for some existing functions, such as os.path.join, which can take either an arbitrary number of inputs or a list.\n\nos.path.join('a','b','c')\n\n'a/b/c'\n\nx = ['a','b','c']\nos.path.join(*x)\n\n'a/b/c'\n\n\n\n\nFunction outputs\nreturn x will specify x as the output of the function. return can occur anywhere in the function, and allows the function to exit as soon as it is done.\nWe can return multiple outputs using return - the return value will then be a tuple.\n\ndef f(x):\n    if x &lt; 0:\n        return -x**2\n    else:\n        res = x^2\n        return x, res\n\n\nf(-3)\n\n-9\n\nf(3)\n\n(3, 1)\n\nout1,out2 = f(3)\n\nIf you want a function to be invoked for its side effects, you can omit return or explicitly have return None or simply return.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#pass-by-value-vs.-pass-by-reference",
    "href": "units/unit5-programming.html#pass-by-value-vs.-pass-by-reference",
    "title": "Programming concepts",
    "section": "Pass by value vs. pass by reference",
    "text": "Pass by value vs. pass by reference\nWhen talking about programming languages, one often distinguishes pass-by-value and pass-by-reference.\nPass-by-value means that when a function is called with one or more arguments, a copy is made of each argument and the function operates on those copies. In pass-by-value, changes to an argument made within a function do not affect the value of the argument in the calling environment.\nPass-by-reference means that the arguments are not copied, but rather that information is passed allowing the function to find and modify the original value of the objects passed into the function. In pass-by-reference changes inside a function can affect the object outside of the function.\nPass-by-value is elegant and modular in that functions do not have side effects - the effect of the function occurs only through the return value of the function. However, it can be inefficient in terms of the amount of computation and of memory used. In contrast, pass-by-reference is more efficient, but also more dangerous and less modular. It’s more difficult to reason about code that uses pass-by-reference because effects of calling a function can be hidden inside the function. Thus pass-by-value is directly related to functional programming.\nArrays and other non-scalar objects in Python are pass-by-reference (but note that tuples are immutable, so one could not modify a tuple that is passed as an argument).\n\ndef myfun(x):\n    x[1] = 99\n\ny = [0, 1, 2]\nz = myfun(y)\ntype(z)\n\n&lt;class 'NoneType'&gt;\n\ny\n\n[0, 99, 2]\n\n\nLet’s see what operations cause arguments modified in a function to affect state outside of the function:\n\ndef myfun(f_scalar, f_x, f_x_new, f_x_newid, f_x_copy):\n\n    f_scalar = 99                 # global input unaffected\n    f_x[0] = 99                   # global input MODIFIED\n    f_x_new = [99,2,3]            # global input unaffected\n\n    newx = f_x_newid\n    newx[0] = 99                  # global input MODIFIED\n\n    xcopy = f_x_copy.copy() \n    xcopy[0] = 99                 # global input unaffected\n\n\nscalar = 1\nx = [1,2,3]\nx_new = np.array([1,2,3])\nx_newid = np.array([1,2,3])\nx_copy = np.array([1,2,3])\n\n\nmyfun(scalar, x, x_new, x_newid, x_copy)\n\nHere are the cases where state is preserved:\n\nscalar\n\n1\n\nx_new\n\narray([1, 2, 3])\n\nx_copy\n\narray([1, 2, 3])\n\n\nAnd here are the cases where state is modified:\n\nx\n\n[99, 2, 3]\n\nx_newid\n\narray([99,  2,  3])\n\n\nBasically if you replace the reference (object name) then the state outside the function is preserved. That’s because a new local variable in the function scope is created. However in the ` If you modify part of the object, state is not preserved.\nThe same behavior occurs with other mutable objects such as numpy arrays.\n\nPointers\nTo put pass-by-value vs. pass-by-reference in a broader context, I want to briefly discuss the idea of a pointer, common in compiled languages such as C.\n\nint x = 3;\nint* ptr;\nptr = &x;\n*ptr * 7; // returns 21\n\n\nThe int* declares ptr to be a pointer to (the address of) the integer x.\nThe &x gets the address where x is stored.\n*ptr dereferences ptr, returning the value in that address (which is 3 since ptr is the address of x.\n\nArrays in C are really pointers to a block of memory:\n\nint x[10];\n\nIn this case x will be the address of the first element of the vector. We can access the first element as x[0] or *x.\nWhy have we gone into this? In C, you can pass a pointer as an argument to a function. The result is that only the scalar address is copied and not the entire object, and inside the function, one can modify the original object, with the new value persisting on exit from the function. For example in the following example one passes in the address of an object and that object is then modified in place, affecting its value when the function call finishes.\n\nint myCal(int* ptr){\n    *ptr = *ptr + *ptr;\n}\n\nmyCal(&x)  # x itself will be modified\n\nSo Python behaves similarly to the use of pointers in C.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#namespaces-and-scopes",
    "href": "units/unit5-programming.html#namespaces-and-scopes",
    "title": "Programming concepts",
    "section": "Namespaces and scopes",
    "text": "Namespaces and scopes\nAs discussed here in the Python docs, a namespace is a mapping from names to objects that allows Python to find objects by name via clear rules that enforce modularity and avoid name conflicts.\nNamespaces are created and removed through the course of executing Python code. When a function is run, a namespace for the local variables in the function is created, and then deleted when the function finishes executing. Separate function calls (including recursive calls) have separate namespaces.\nScope is closely related concept – a scope determines what namespaces are accessible from a given place in one’s code. Scopes are nested and determine where and in what order Python searches the various namespaces for objects.\nNote that the ideas of namespaces and scopes are relevant in most other languages, though the details of how they work can differ.\nThese ideas are very important for modularity, isolating the names of objects to avoid conflicts.\nThis allows you to use the same name in different modules or submodules, as well as different packages using the same name.\nOf course to make the objects in a module or package available we need to use import.\nConsider what happens if you have two modules that both use x and you import x using from.\n\nfrom mypkg.mymod import x\nfrom mypkg.mysubpkg import x\nx  # which x is used?\n\nWe’ve added x twice to the namespace of the global scope. Are both available? Did one ‘overwrite’ the other? How do I access the other one?\nThis is much better:\n\nimport mypkg\nmypkg.x\n\n7\n\nimport mypkg.mysubpkg\nmypkg.mysubpkg.x\n\n999\n\n\nSide note: notice that import mypkg causes the name mypkg itself to be in the current (global) scope.\nWe can see the objects in a given namespace/scope using dir().\n\nxyz = 7\ndir()\n\n['Bear', 'GrizzlyBear', '__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'add', 'apply_fun', 'content', 'copy', 'f', 'files', 'foo', 'function', 'function_a', 'function_b', 'function_c', 'io', 'it', 'item', 'm', 'math', 'myArray', 'mydict', 'myfun', 'myit', 'mylist', 'mymod', 'mypkg', 'mypkg_updated', 'myts', 'myts2', 'mytsFullCopy', 'mytsRef', 'mytuple', 'new_x', 'np', 'num399', 'os', 'out1', 'out2', 'partial', 'pattern', 'platform', 'plt', 'r', 're', 'result', 'return_group', 'round3', 's', 'scalar', 'smoke', 'st', 'stream', 'subprocess', 'sum_args', 'sys', 'text', 'time', 'tmp', 'traceback', 'tsSimClass', 'x', 'x_copy', 'x_new', 'x_newid', 'xyz', 'y', 'y1', 'y2', 'y3', 'yog', 'z']\n\nimport mypkg\ndir(mypkg)\n\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'auxil', 'helper', 'myfun', 'myfun10', 'mymod', 'mysubpkg', 'x']\n\nimport mypkg.mymod\ndir(mypkg.mymod)\n\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'helper', 'myfun', 'myfun10', 'x']\n\nimport builtins\ndir(builtins)\n\n['ArithmeticError', 'AssertionError', 'AttributeError', 'BaseException', 'BaseExceptionGroup', 'BlockingIOError', 'BrokenPipeError', 'BufferError', 'BytesWarning', 'ChildProcessError', 'ConnectionAbortedError', 'ConnectionError', 'ConnectionRefusedError', 'ConnectionResetError', 'DeprecationWarning', 'EOFError', 'Ellipsis', 'EncodingWarning', 'EnvironmentError', 'Exception', 'ExceptionGroup', 'False', 'FileExistsError', 'FileNotFoundError', 'FloatingPointError', 'FutureWarning', 'GeneratorExit', 'IOError', 'ImportError', 'ImportWarning', 'IndentationError', 'IndexError', 'InterruptedError', 'IsADirectoryError', 'KeyError', 'KeyboardInterrupt', 'LookupError', 'MemoryError', 'ModuleNotFoundError', 'NameError', 'None', 'NotADirectoryError', 'NotImplemented', 'NotImplementedError', 'OSError', 'OverflowError', 'PendingDeprecationWarning', 'PermissionError', 'ProcessLookupError', 'RecursionError', 'ReferenceError', 'ResourceWarning', 'RuntimeError', 'RuntimeWarning', 'StopAsyncIteration', 'StopIteration', 'SyntaxError', 'SyntaxWarning', 'SystemError', 'SystemExit', 'TabError', 'TimeoutError', 'True', 'TypeError', 'UnboundLocalError', 'UnicodeDecodeError', 'UnicodeEncodeError', 'UnicodeError', 'UnicodeTranslateError', 'UnicodeWarning', 'UserWarning', 'ValueError', 'Warning', 'ZeroDivisionError', '_', '__build_class__', '__debug__', '__doc__', '__import__', '__loader__', '__name__', '__package__', '__spec__', 'abs', 'aiter', 'all', 'anext', 'any', 'ascii', 'bin', 'bool', 'breakpoint', 'bytearray', 'bytes', 'callable', 'chr', 'classmethod', 'compile', 'complex', 'copyright', 'credits', 'delattr', 'dict', 'dir', 'divmod', 'enumerate', 'eval', 'exec', 'exit', 'filter', 'float', 'format', 'frozenset', 'getattr', 'globals', 'hasattr', 'hash', 'help', 'hex', 'id', 'input', 'int', 'isinstance', 'issubclass', 'iter', 'len', 'license', 'list', 'locals', 'map', 'max', 'memoryview', 'min', 'next', 'object', 'oct', 'open', 'ord', 'pow', 'print', 'property', 'quit', 'range', 'repr', 'reversed', 'round', 'set', 'setattr', 'slice', 'sorted', 'staticmethod', 'str', 'sum', 'super', 'tuple', 'type', 'vars', 'zip']\n\n\nHere are the key scopes to be aware of, in order (“LEGB”) of how the namespaces are searched:\n\nLocal scope: objects available within function (or class method).\nnon-local (Enclosing) scope: objects available from functions enclosing a given function (we’ll talk about this more later; this relates to lexical scoping).\nGlobal (aka ‘module’) scope: objects available in the module in which the function is defined (which may simply be the default global scope when you start the Python interpreter). This is also the local scope if the code is not executing inside a function.\nBuilt-ins scope: objects provided by Python through the built-ins module but available from anywhere.\n\nNote that import adds the name of the imported module to the namespace of the current (i.e., local) scope.\nWe can see the local and global namespaces using locals() and globals().\n\ncat local.py\n\ngx = 7\n\ndef myfun(z):\n    y = z*3\n    print(\"local: \", locals())\n    print(\"global: \", globals())\n\n\nRun the following code to see what is in the different namespaces:\n\nimport local\n\ngx = 99\nlocal.myfun(3)\n\nStrangely (for me being more used to R, where package namespaces are ‘locked’ such that objects can’t be added), we can add an object to a namespace created from a module or package:\n\nmymod.x = 33\ndir(mymod)\n\n['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'myfun', 'range', 'x']\n\nimport numpy as np\nnp.x = 33\n'x' in dir(np)\n\nTrue\n\n\nAs more motivation, consider this example.\nSuppose we have this code in a module named test_scope.py:\n\ncat test_scope.py\n\nmagic_number = 7\n\ndef myfun(val):\n    return(val * magic_number)\n\n\nNow suppose we also define magic_number in the scope in which myfun is called from.\n\nimport test_scope\nmagic_number = 900\ntest_scope.myfun(3)\n\n21\n\n\nWe see that Python uses magic_number from the module. What would be bad about using magic_number from the global scope of the Python session rather than the global scope of the module? Consider a case where instead of using the test_scope.py module we were using code from a package.\n\nLexical scoping (enclosing scopes)\nIn this section, we seek to understand what happens in the following circumstance. Namely, where does Python get the value for the object x?\n\ndef f(y):\n  return(x + y)\n\nf(3)\n\nVariables in the enclosing scope are available within a function. The enclosing scope is the scope in which a function is defined, not the scope from which a function is called.\nThis approach is called lexical scoping. R and many other languages also use lexical scoping.\nThe behavior of basing lookup on where functions are defined rather than where they are called from extends the local-global scoping discussed in the previous section, with similar motivation.\nLet’s dig deeper to understand where Python looks for non-local variables, illustrating lexical scoping:\n\n## Case 1\nx = 3\ndef f2():\n    print(x)\n\ndef f():\n    x = 7\n    f2()\n\nf() # what will happen?\n\n## Case 2\nx = 3\ndef f2()\n    print(x)\n\ndef f():\n    x = 7\n    f2()\n\nx = 100\nf() # what will happen?\n\n## Case 3\nx = 3\ndef f():\n    def f2():\n        print(x)        \n    x = 7\n    f2()\n\nx = 100\nf() # what will happen?\n\n## Case 4\nx = 3\ndef f():\n    def f2():\n        print(x)\n    f2()\n\nx = 100\nf() # what will happen?\n\nHere’s a tricky example:\n\ny = 100\ndef fun_constructor():\n    y = 10\n    def g(x):\n            return(x + y)     \n    return(g)\n\n## fun_constructor() creates functions\nmyfun = fun_constructor()\nmyfun(3)\n\nLet’s work through this:\n\nWhat is the enclosing scope for the function g()?\nWhich y does g() use?\nWhere is myfun defined (this is tricky – how does myfun relate to g)?\nWhat is the enclosing scope for myfun()?\nWhen fun_constructor() finishes, does its namespace disappear? What would happen if it did?\nWhat does myfun use for y?\n\nWe can use the inspect package to see information about the closure.\n\nimport inspect\ninspect.getclosurevars(myfun)\n\nClosureVars(nonlocals={}, globals={'copy': &lt;module 'copy' from '/system/linux/mambaforge-3.11/lib/python3.11/copy.py'&gt;}, builtins={}, unbound=set())\n\n\n(Note that I haven’t fully investigated the use of inspect, but it looks like it has a lot of useful tools.)\nBe careful when using variables from non-local scopes as the value of that variable may well not be what you expect it to be. In general one wants to think carefully before using variables that are taken from outside the local scope, but in some cases it can be useful.\nNext we’ll see some ways of accessing variables outside of the local scope.\n\n\nGlobal and non-local variables\nWe can create and modify global variables and variables in the enclosing scope using global and nonlocal respectively. Note that global is in the context of the current module so this could be a variable in your current Python session if you’re working with functions defined in that session or a global variable in a module or package.\n\ndel x\n\ndef myfun():\n    global x\n    x = 7\n\nmyfun()\nprint(x)\n\n7\n\nx = 9\nmyfun()\nprint(x)\n\n7\n\n\n\ndef outer_function():\n    x = 10  # Outer variable\n    def inner_function():\n        nonlocal x\n        x = 20  # Modifying the outer variable\n    print(x)  # Output: 10\n    inner_function()\n    print(x)  # Output: 20\n\nouter_function()\n\n10\n20\n\n\nIn R, one can do similar things using the global assignment operator &lt;&lt;-.\n\n\nClosures\nOne way to associate data with functions is to use a closure. This is a functional programming way to achieve something like an OOP class. This Wikipedia entry nicely summarizes the idea, which is a general functional programming idea and not specific to Python.\nUsing a closure involves creating one (or more functions) within a function call and returning the function(s) as the output. When one executes the original function (the constructor), the new function(s) is created and returned and one can then call that function(s). The function then can access objects in the enclosing scope (the scope of the constructor) and can use nonlocal to assign into the enclosing scope, to which the function (or the multiple functions) have access. The nice thing about this compared to using a global variable is that the data in the closure is bound up with the function(s) and is protected from being changed by the user.\n\nx = np.random.normal(size = 5)\ndef scaler_constructor(input):\n    data = input\n    def g(param):\n            return(param * data) \n    return(g)\n\nscaler = scaler_constructor(x)\ndel x # to demonstrate we no longer need x\nscaler(3)\n\narray([ 0.5081473 ,  2.22166935, -2.86110181, -0.79865552,  0.09784364])\n\nscaler(6)\n\narray([ 1.0162946 ,  4.44333871, -5.72220361, -1.59731104,  0.19568728])\n\n\nSo calling scaler(3) multiplies 3 by the value of data stored in the closure (the namespace of the enclosing scope) of the function scaler.\nNote that it can be hard to see the memory use involved in the closure.\nHere’s a more realistic example. There are other ways you could do this, but this is slick:\n\ndef make_container(n):\n    x = np.zeros(n)\n    i = 0\n    def store(value = None):\n        nonlocal x, i\n        if value is None:\n            return x\n        else:\n            x[i] = value\n            i += 1\n    return store\n\n\nnboot = 20\nbootmeans = make_container(nboot)\n\nimport pandas as pd\niris = pd.read_csv('https://raw.githubusercontent.com/pandas-dev/pandas/master/pandas/tests/io/data/csv/iris.csv')\ndata = iris['SepalLength']\n\nfor i in range(nboot): \n    bootmeans(np.mean(np.random.choice(data, size = len(data), replace = True)))\n\n\nbootmeans()\n\narray([5.874     , 5.95533333, 5.86      , 5.754     , 5.77466667,\n       5.81733333, 5.902     , 5.79933333, 5.842     , 5.81933333,\n       5.854     , 5.97      , 5.84      , 5.80133333, 5.99333333,\n       5.88133333, 5.83133333, 5.84533333, 5.91466667, 5.79666667])\n\nbootmeans.__closure__\n\n(&lt;cell at 0x7f9bc9bea2f0: int object at 0x7f9bd791d968&gt;, &lt;cell at 0x7f9bc9beaf80: numpy.ndarray object at 0x7f9bc9d5a130&gt;)",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#decorators",
    "href": "units/unit5-programming.html#decorators",
    "title": "Programming concepts",
    "section": "Decorators",
    "text": "Decorators\nNow that we’ve seen function generators, it’s straightforward to discuss decorators.\nA decorator is a wrapper around a function that extends the functionality of the function without actually modifying the function.\nWe can create a simple decorator “manually” like this:\n\ndef verbosity_wrapper(myfun):\n    def wrapper(*args, **kwargs):\n        print(f\"Starting {myfun.__name__}.\")\n        output = myfun(*args, **kwargs)\n        print(f\"Finishing {myfun.__name__}.\")\n        return output\n    return wrapper\n    \nverbose_rnorm = verbosity_wrapper(np.random.normal)\n\nx = verbose_rnorm(size = 5)\n\nStarting normal.\nFinishing normal.\n\nx\n\narray([ 0.07202449, -0.67672674,  0.98248139, -1.65748762,  0.81795808])\n\n\nPython provides syntax that helps you create decorators with less work (this is an example of the general idea of syntactic sugar).\nWe can easily apply our decorator defined above to a function as follows. Now the function name refers to the wrapped version of the function.\n\n@verbosity_wrapper\ndef myfun(x):\n    return x\n\ny = myfun(7)\n\nStarting myfun.\nFinishing myfun.\n\ny\n\n7\n\n\nOur decorator doesn’t do anything useful, but hopefully you can imagine that the idea of being able to have more control over the operation of functions could be useful. For example we could set up a timing wrapper so that when we run a function, we get a report on how long it took to run the function. Or using the idea of a closure, we could keep a running count of the number of times a function has been called.\nOne real-world example of using decorators is in setting up functions to run in parallel in dask, which we’ll discuss in Unit 7.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#overview-2",
    "href": "units/unit5-programming.html#overview-2",
    "title": "Programming concepts",
    "section": "Overview",
    "text": "Overview\nThe main things to remember when thinking about memory use are: (1) numeric vectors take 8 bytes per element and (2) we need to keep track of when large objects are created, including local variables in the frames of functions.\n\nx = np.random.normal(size = 5)\nx.itemsize # 8 bytes\n\n8\n\nx.nbytes\n\n40\n\n\n\nAllocating and freeing memory\nUnlike compiled languages like C, in Python we do not need to explicitly allocate storage for objects. (However, we will see that there are times that we do want to allocate storage in advance, rather than successively concatenating onto a larger object.)\nPython automatically manages memory, releasing memory back to the operating system when it’s not needed via a process called garbage collection. Very occasionally you may want to remove large objects as soon as they are not needed. del does not actually free up memory, it just disassociates the name from the memory used to store the object. In general Python will quickly clean up such objects without a reference (i.e., a name), so there is generally no need to call gc.collect() to force the garbage collection.\nIn a language like C in which the user allocates and frees up memory, memory leaks are a major cause of bugs. Basically if you are looping and you allocate memory at each iteration and forget to free it, the memory use builds up inexorably and eventually the machine runs out of memory. In Python, with automatic garbage collection, this is generally not an issue, but occasionally memory leaks could occur.\n\n\nThe heap and the stack\nThe heap is the memory that is available for dynamically creating new objects while a program is executing, e.g., if you create a new object in Python or call new in C++. When more memory is needed the program can request more from the operating system. When objects are removed in Python, Python will handle the garbage collection of releasing that memory.\nThe stack is the memory used for local variables when a function is called.\nThere’s a nice discussion of this on this Stack Overflow thread.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#monitoring-memory-use",
    "href": "units/unit5-programming.html#monitoring-memory-use",
    "title": "Programming concepts",
    "section": "Monitoring memory use",
    "text": "Monitoring memory use\n\nMonitoring overall memory use on a UNIX-style computer\nTo understand how much memory is available on your computer, one needs to have a clear understanding of disk caching. The operating system will generally cache files/data in memory when it reads from disk. Then if that information is still in memory the next time it is needed, it will be much faster to access it the second time around than if it had to read the information from disk. While the cached information is using memory, that same memory is immediately available to other processes, so the memory is available even though it is “in use”.\nWe can see this via free -h (the -h is for ‘human-readable’, i.e. show in GB (G)) on Linux machine.\n          total used free shared buff/cache available \n    Mem:   251G 998M 221G   2.6G        29G      247G \n    Swap:  7.6G 210M 7.4G\nYou’ll generally be interested in the Mem row. (See below for some comments on Swap.) The shared column is complicated and probably won’t be of use to you. The buff/cache column shows how much space is used for disk caching and related purposes but is actually available. Hence the available column is the sum of the free and buff/cache columns (more or less). In this case only about 1 GB is in use (indicated in the used column).\ntop (Linux or Mac) and vmstat (on Linux) both show overall memory use, but remember that the amount actually available to you is the amount free plus any buff/cache usage. Here is some example output from vmstat:\n\n    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- \n    r b   swpd      free   buff    cache si so bi bo in cs us sy id wa st \n    1 0 215140 231655120 677944 30660296  0  0  1  2  0  0 18  0 82  0  0\nIt shows 232 GB free and 31 GB used for cache and therefore available, for a total of 263 GB available.\nHere are some example lines from top:\n    KiB Mem : 26413715+total, 23180236+free, 999704 used, 31335072 buff/cache \n    KiB Swap:  7999484 total,  7784336 free, 215148 used. 25953483+avail Mem\nWe see that this machine has 264 GB RAM (the total column in the Mem row), with 259.5 GB available (232 GB free plus 31 GB buff/cache as seen in the Mem row). (I realize the numbers don’t quite add up for reasons I don’t fully understand, but we probably don’t need to worry about that degree of exactness.) Only 1 GB is in use.\nSwap is essentially the reverse of disk caching. It is disk space that is used for memory when the machine runs out of physical memory. You never want your machine to be using swap for memory because your jobs will slow to a crawl. As seen above, the swap line in both free and top shows 8 GB swap space, with very little in use, as desired.\n\n\nMonitoring memory use in Python\nThere are a number of ways to see how much memory is being used. When Python is actively executing statements, you can use top from the UNIX shell.\nIn Python, we can call out to the system to get the info we want:\n\nimport psutil\n\n# Get memory information\nmemory_info = psutil.Process().memory_info()\n\n# Print the memory usage\nprint(\"Memory usage:\", memory_info.rss/10**6, \" Mb.\")\n\nMemory usage: 354.32448  Mb.\n\n\n# Let's turn that into a function for later use:\ndef mem_used():\n    print(\"Memory usage:\", psutil.Process().memory_info().rss/10**6, \" Mb.\")\n\nWe can see the size of an object (in bytes) with sys.getsizeof().\n\nmy_list = [1, 2, 3, 4, 5]\nsys.getsizeof(my_list)\n\n104\n\nx = np.random.normal(size = 10**7) # should use about 80 Mb\nsys.getsizeof(x)\n\n80000112\n\n\nHowever, we need to be careful about objects that refer to other objects:\n\ny = [3, x]\nsys.getsizeof(y)  # Whoops!\n\n72\n\n\nHere’s a trick where we serialize the object, as if to export it, and then see how long the binary representation is.\n\nimport pickle\nser_object = pickle.dumps(y)\nsys.getsizeof(ser_object)\n\n80000201\n\n\nThere are also some flags that one can start python with that allow one to see information about memory use and allocation. See man python. You could also look into the memory_profiler or pympler packages.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#how-memory-is-used-in-python",
    "href": "units/unit5-programming.html#how-memory-is-used-in-python",
    "title": "Programming concepts",
    "section": "How memory is used in Python",
    "text": "How memory is used in Python\n\nTwo key tools: id and is\nWe can use the id function to see where in memory an object is stored and is to see if two object are actually the same objects in memory. It’s particularly useful for understanding storage and memory use for complicated data structures. We’ll also see that they can be handy tools for seeing where copies are made and where they are not.\n\nx = np.random.normal(size = 10**7)\nid(x)\n\n140306907064304\n\nsys.getsizeof(x)\n\n80000112\n\ny = x\nid(y)\n\n140306907064304\n\nx is y\n\nTrue\n\nsys.getsizeof(y)\n\n80000112\n\nz = x.copy()\nid(z)\n\n140306906624528\n\nsys.getsizeof(z)\n\n80000112\n\n\n\n\nMemory use in specific circumstances\n\nHow lists are stored\nHere we can use id to determine how the overall list is stored as well as the elements of the list.\n\nnums = np.random.normal(size = 5)\nobj = [nums, nums, np.random.normal(size = 5), ['adfs']]\n\nid(nums)\n\n140306907063344\n\nid(obj)\n\n140306906964544\n\nid(obj[0])\n\n140306907063344\n\nid(obj[1])\n\n140306907063344\n\nid(obj[2])\n\n140306907064400\n\nid(obj[3])\n\n140306907590208\n\nid(obj[3][0])\n\n140306906961200\n\nobj[0] is obj[1]\n\nTrue\n\nobj[0] is obj[2]\n\nFalse\n\n\nWhat do we notice?\n\nThe list itself appears to be a array of references (pointers) to the component elements.\nEach element has its own address.\nTwo elements of a list can use the same memory (see the first two elements here, whose contents are at the same memory address).\nA list element can use the same memory as another object (or part of another object).\n\n\n\nHow character strings are stored.\nSimilar tricks are used for storing strings (and also integers). We’ll explore this in a problem on PS4.\n\n\nModifying elements in place\nWhat do this simple experiment tell us?\n\nx = np.random.normal(size = 5)\nid(x)\n\n140306907287152\n\nx[2] = 3.5\nid(x)\n\n140306907287152\n\n\nIt makes some sense that modifying elements of an object here doesn’t cause a copy – if it did, working with large objects would be very difficult.\n\n\n\nWhen are copies made?\nLet’s try to understand when Python uses additional memory for objects, and how it knows when it can delete memory. We’ll use large objects so that we can use free or top to see how memory use by the Python process changes.\n\nx = np.random.normal(size = 10**8)\nid(x)\n\n140306907287248\n\ny = x\nid(y)\n\n140306907287248\n\nx = np.random.normal(size = 10**8)\nid(x)\n\n140306907055568\n\n\nOnly if we re-assign x to reference a different object does additional memory get used.\n\nHow does Python know when it can free up memory?\nPython keeps track of how many names refer to an object and only removes memory when there are no remaining references to an object.\n\nimport sys\n\nx = np.random.normal(size = 10**8)\ny = x\nsys.getrefcount(y)\n\n3\n\ndel x\nsys.getrefcount(y)\n\n2\n\ndel y\n\nWe can see the number of references using sys.getrefcount. Confusingly, the number is one higher than we’d expect, because it includes the temporary reference from passing the object as the argument to getrefcount.\n\nx = np.random.normal(size = 5)\nsys.getrefcount(x)  # 2 \n\n2\n\ny = x\nsys.getrefcount(x)  # 3\n\n3\n\nsys.getrefcount(y)  # 3\n\n3\n\ndel y\nsys.getrefcount(x)  # 2\n\n2\n\ny = x\nx = np.random.normal(size = 5)\nsys.getrefcount(y)  # 2\n\n2\n\nsys.getrefcount(x)  # 2\n\n2\n\n\nThis notion of reference counting occurs in other contexts, such as shared pointers in C++ and in how R handles copying and garbage collection.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#strategies-for-saving-memory",
    "href": "units/unit5-programming.html#strategies-for-saving-memory",
    "title": "Programming concepts",
    "section": "Strategies for saving memory",
    "text": "Strategies for saving memory\nA frew basic strategies for saving memory include:\n\nAvoiding unnecessary copies.\nRemoving objects that are not being used, at which point the Python garbage collector should free up the memory.\n\nIf you’re really trying to optimize memory use, you may also consider:\n\nUsing types that take up less memory (e.g., Bool, Int16, Float32) when possible.\n\nx = np.array(np.random.normal(size = 5), dtype = \"float32\")\nx.itemsize\n\n4\n\nx = np.array([3,4,2,-2], dtype = \"int16\")\nx.itemsize\n\n2\n\n\nReading data in from files in chunks rather than reading the entire dataset (more in Unit 7).\nExploring packages such as arrow for efficiently using memory, as discussed in Unit 2.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#example",
    "href": "units/unit5-programming.html#example",
    "title": "Programming concepts",
    "section": "Example",
    "text": "Example\nLet’s work through a real example where we keep a running tally of current memory in use and maximum memory used in a function call. We’ll want to consider hidden uses of memory, when copies are made, and lazy evaluation. This code (translated from the original R code) comes from a PhD student’s research. For our purposes here, let’s assume that xvar and yvar are very long numpy arrays using a lot of memory.\n\ndef fastcount(xvar, yvar):\n    naline = np.isnan(xvar)\n    naline[np.isnan(yvar)] = True\n    localx = xvar.copy()\n    localy = yvar.copy()\n    localx[naline] = 0\n    localy[naline] = 0\n    useline = ~naline\n    ## We'll ignore the rest of the code.\n    ## ....",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#interpreters-and-compilation",
    "href": "units/unit5-programming.html#interpreters-and-compilation",
    "title": "Programming concepts",
    "section": "Interpreters and compilation",
    "text": "Interpreters and compilation\n\nWhy are interpreted languages slow?\nCompiled code runs quickly because the original code has been translated into instructions (machine language) that the processor can understand (i.e., zeros and ones). In the process of doing so, various checking and lookup steps are done once and don’t need to be redone when running the compiled code.\nIn contrast, when one runs code in an interpreted language such as Python or R, the interpreter needs to do all the checking and lookup each time the code is run. This is required because the types and locations in memory of the variables could have changed.\nWe’ll focus on Python in the following discussion, but most of the concepts apply to other interpreted languages.\nFor example, consider this code:\n\nx = 3\nabs(x)\nx*7\nx = 'hi'\nabs(x)\nx*3\n\nBecause of dynamic typing, when the interpreter sees abs(x) it needs to check if x is something to which the absolute value function can be applied, including dealing with the fact that x could be a list or array with many numbers in it. In addition it needs to (using scoping rules) look up the value of x. (Consider that x might not even exist at the point that abs(x) is called.) Only then can the absolute value calculation happen. For the multiplication, Python needs to lookup the version of * that can be used, depending on the type of x.\nLet’s consider writing a loop with some ridiculous code:\n\nx = np.random.normal(10)\nfor i in range(10):\n    if np.random.normal(size = 1) &gt; 0:\n        x = 'hi'\n    if np.random.normal(size = 1) &gt; 0.5:\n        del x\n    x[i]= np.exp(x[i])\n\nThere is no way around the fact that because of how dynamic this is, the interpreter needs to check if x exists, if it is a vector of sufficient length, if it contains numeric values, and it needs to go retrieve the required value, EVERY TIME the np.exp() is executed. Now the code above is unusual, and in most cases, we wouldn’t have the if statements that modify x. So you could imagine a process by which the checking were done on the first iteration and then not needed after that – that gets into the idea of just-in-time compilation, discussed later.\nThe standard Python interpreter (CPython) is a C function so in some sense everything that happens is running as compiled code, but there are lots more things being done to accomplish a given task using interpreted code than if the task had been written directly in code that is compiled. By analogy, consider talking directly to a person in a language you both know compared to talking to a person via an interpreter who has to translate between two languages. Ultimately, the same information gets communicated (hopefully!) but the number of words spoken and time involved is much greater.\nWhen running more complicated functions, there is often a lot of checking that is part of the function itself. For example scipy’s solve_triangular function ultimately calls out to the trtrs Lapack function, but before doing so, there is a lot of checking that can take time. To that point, the documentation suggests you might set check_finite=False to improve performance at the expense of potential problems if the input matrices contain troublesome elements.\nWe can flip the question on its head and ask what operations in an interpreted language will execute quickly. In Python, these include:\n\noperations that call out to compiled C code,\nlinear algebra operations (these call out to compiled C or Fortran code provided by the BLAS and LAPACK software packages), and\nvectorized calls rather than loops:\n\nvectorized calls generally run loops in compiled C code rather than having the loop run in Python, and\nthat means that the interpreter doesn’t have to do all the checking discussed above for every iteration of the loop.\n\n\n\n\nCompilation\n\nOverview\nCompilation is the process of turning code in a given language (such a C++) into machine code. Machine code is the code that the processor actually executes. The machine code is stored in the executable file, which is a binary file. The history of programming has seen ever great levels of abstraction, so that humans can write code using syntax that is easier for us to understand, re-use, and develop building blocks that can be put together to do complicated tasks. For example assembly language is a step above machine code. Languages like C and Fortran provide additional abstraction beyond that. The Statistics 750 class at CMU has a nice overview if you want to see more details.\nNote that interpreters such as Python are themselves programs – the standard Python interpreter (CPython) is a C program that has been compiled. It happens to be a program that processes Python code. The interpreter doesn’t turn Python code into machine code, but the interpreter itself is machine code.\n\n\nJust-in-time (JIT) compilation\nStandard compilation (ahead-of-time or AOT compilation) happens before any code is executed and can involve a lot of optimization to produce the most efficient machine code possible.\nIn contrast, just-in-time (JIT) compilation happens at the time that the code is executing. JIT compilation is heavily used in Julia, which is very fast (in some cases as fast as C). JIT compilation involves translating to machine code as the code is running. One nice aspect is that the results are cached so that if code is rerun, the compilation process doesn’t have to be redone. So if you use a language like Julia, you’ll see that the speed can vary drastically between the first time and later times you run a given function during a given session.\nOne thing that needs to be dealt with is type checking. As discussed above, part of why an interpreter is slow is because the type of the variable(s) involved in execution of a piece of code is not known in advance, so the interpreter needs to check the type. In JIT systems, there are often type inference systems that determine variable types.\nJIT compilation can involve translation from the original code to machine code or translation of bytecode (see next section) to machine code.\nAt the end of this unit, we’ll see the use of JIT compilation with the JAX package in Python.\n\n\nByte compiling (optional)\nFunctions in Python and Python packages may byte compiled. What does that mean? Byte-compiled code is a special representation that can be executed more efficiently because it is in the form of compact codes that encode the results of parsing and semantic analysis of scoping and other complexities of the Python source code. This byte code can be executed faster than the original Python code because it skips the stage of having to be interpreted by the Python interpreter.\nIf you look at the file names in the directory of an installed Python package you may see files with the .pyc extension. These files have been byte-compiled.\nWe can byte compile our own functions using either the py_compile or compileall modules. Here’s an example (silly since as experienced Python programmers, we would use vectorized calculation here rather than this unvectorized code.)\n\nimport time\n\ndef f(vals):\n    x = np.zeros(len(vals))\n    for i in range(len(vals)):\n        x[i] = np.exp(vals[i])\n    return(x)\n\nx = np.random.normal(size = 10**6)\nt0 = time.time()\nout = f(x)\ntime.time() - t0\n\n0.7561802864074707\n\nt0 = time.time()\nout = np.exp(x)\ntime.time() - t0\n\n0.013027191162109375\n\nimport py_compile\npy_compile.compile('vec.py')\n\n'__pycache__/vec.cpython-312.pyc'\n\ncp __pycache__/vec.cpython-312.pyc vec.pyc\nrm vec.py    # Make sure non-compiled module not loaded.\n\n\nimport vec\nvec.__file__\n\n'/accounts/vis/paciorek/teaching/243fall24/fall-2024/units/vec.pyc'\n\nt0 = time.time()\nout = vec.f(x)\ntime.time() - t0\n\n0.7301280498504639\nUnfortunately, as seen above byte compiling may not speed things up much. I’m not sure why.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#benchmarking-and-profiling",
    "href": "units/unit5-programming.html#benchmarking-and-profiling",
    "title": "Programming concepts",
    "section": "Benchmarking and profiling",
    "text": "Benchmarking and profiling\nRecall that it’s a waste of time to optimize code before you determine (1) that the code is too slow for how it will be used and (2) which are the slow steps on which to focus your attempts to speed the code up. A 100x speedup in a step that takes 1% of the time will speed up the overall code by essentially nothing.\n\nTiming your code\nThere are a few ways to time code:\n\nimport time\nt0 = time.time()\nx = 3\nt1 = time.time()\n\nprint(f\"Execution time: {t1-t0} seconds.\")\n\nExecution time: 0.0060672760009765625 seconds.\n\n\nIn general, it’s a good idea to repeat (replicate) your timing, as there is some stochasticity in how fast your computer will run a piece of code at any given moment.\nUsing time is fine for code that takes a little while to run, but for code that is really fast, it may not be very accurate. Measuring fast bits of code is tricky to do well. This next approach is better for benchmarking code (particularly faster bits of code).\n\nimport timeit\n\ntimeit.timeit('x = np.exp(3.)', setup = 'import numpy as np', number = 100)\n\n7.507577538490295e-05\n\ncode = '''\nx = np.exp(3.)\n'''\n\ntimeit.timeit(code, setup = 'import numpy as np', number = 100)\n\n6.598792970180511e-05\n\n\nThat reports the total time for the 100 replications.\nWe can run it from the command line.\n\npython -m timeit -s 'import numpy' -n 1000 'x = numpy.exp(3.)'\n\n1000 loops, best of 5: 570 nsec per loop\n\n\ntimeit ran the code 1000 times for 5 different repetitions, giving the average time for the 1000 samples for the best of the 5 repetitions.\n\n\nProfiling\nThe Cprofile module will show you how much time is spent in different functions, which can help you pinpoint bottlenecks in your code.\nI haven’t run this code when producing this document as the output of the profiling can be lengthy.\n\ndef lr_slow(y, x):\n    xtx = x.T @ x\n    xty = x.T @ y\n    inv = np.linalg.inv(xtx)\n    return inv @ xty\n\n## generate random observations and random matrix of predictors\ny = np.random.normal(size = 5000)\nx = np.random.normal(size = (5000,1000))\n\nt0 = time.time()\nregr = lr_slow(y, x)\nt1 = time.time()\nprint(f\"Execution time: {t1-t0} seconds.\")\n\nimport cProfile\ncProfile.run('lr_slow(y,x)')\n\nThe cumtime column includes the time spent in nested calls to functions while the tottime column excludes it.\nAs we’ll discuss in detail in Unit 10, we almost never want to explicitly invert a matrix. Instead we factorize the matrix and use the factorized result to do the computation of interest. In this case using the Cholesky decomposition is a standard approach, followed by solving triangular systems of equations.\n\nimport scipy as sp\n\ndef lr_fast(y, x):\n    xtx = x.T @ x\n    xty = x.T @ y\n    L = sp.linalg.cholesky(xtx)\n    out = sp.linalg.solve_triangular(L.T, \n          sp.linalg.solve_triangular(L, xty, lower=True),\n          lower=False)\n    return(out)\n\nt0 = time.time()\nregr = lr_fast(y, x)\nt1 = time.time()\nprint(f\"Execution time: {t1-t0} seconds.\")\n\ncProfile.run('lr_fast(y,x)')\n\nThe Cholesky now dominates the computational time (but is much faster than inv), so there’s not much more we can do in this case.\nYou might wonder if it’s better to use x.T or np.transpose(x). Try using timeit to decide.\nThe Python profilers (cProfile and profile (not shown)) use deterministic profiling – calculating the interval between events (i.e., function calls and returns). However, there is some limit to accuracy – the underlying ‘clock’ measures in units of about 0.001 seconds.\n(In contrast, R’s profiler works by sampling (statistical profiling) - every little while during a calculation it finds out what function R is in and saves that information to a file. So if you try to profile code that finishes really quickly, there’s not enough opportunity for the sampling to represent the calculation accurately and you may get spurious results.)",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#writing-efficient-python-code",
    "href": "units/unit5-programming.html#writing-efficient-python-code",
    "title": "Programming concepts",
    "section": "Writing efficient Python code",
    "text": "Writing efficient Python code\nWe’ll discuss a variety of these strategies, including:\n\nPre-allocating memory rather than growing objects iteratively\nVectorization and use of fast matrix algebra\nConsideration of loops vs. map operations\nSpeed of lookup operations, including hashing\n\n\nPre-allocating memory\nLet’s consider whether we should pre-allocate space for the output of an operation or if it’s ok to keep extending the length of an array or list.\n\nn = 100000\nz = np.random.normal(size = n)\n\n## Pre-allocation\n\ndef fun_prealloc(vals):\n   n = len(vals)\n   x = [0] * n\n   for i in range(n):\n       x[i] = np.exp(vals[i])\n   return(x)\n\n## Appending to a list\n\ndef fun_append(vals):\n   x = []\n   for i in range(n):\n       x.append(np.exp(vals[i]))\n   return(x)\n\n## Appending to a numpy array\n\ndef fun_append_np(vals):\n   x = np.array([])\n   for i in range(n):\n       x = np.append(x, np.exp(vals[i]))\n   return(x)\n\n\nt0 = time.time()\nout1 = fun_prealloc(z)\ntime.time() - t0\n\n0.07333183288574219\n\nt0 = time.time()\nout2 = fun_append(z)\ntime.time() - t0\n\n0.07479381561279297\n\nt0 = time.time()\nout3 = fun_append_np(z)\ntime.time() - t0\n\n2.4501047134399414\n\n\nSo what’s going on? First let’s consider what is happening with the use of np.append. Note that it is a function, rather than a method, and we need to reassign to x. What must be happening in terms of memory use and copying when we append an element?\n\nx = np.random.normal(size = 5)\nid(x)\n\n140306907288400\n\nid(np.append(x, 3.34))\n\n140306907288784\n\n\nWe can avoid that large cost of copying and memory allocation by pre-allocating space for the entire output array. (This is equivalent to variable initialization in compiled languages.)\nOk, but how is it that we can append to the list at apparently no cost?\nIt’s not magic, just that Python is clever. Let’s get an idea of what is going on:\n\ndef fun_append2(vals):\n   n = len(vals)\n   x = []\n   print(f\"Initial id: {id(x)}\")\n   sz = sys.getsizeof(x)\n   print(f\"iteration 0: size {sz}\")\n   for i in range(n):\n       x.append(np.exp(vals[i]))\n       if sys.getsizeof(x) != sz:\n           sz = sys.getsizeof(x)\n           print(f\"iteration {i}: size {sz}\")\n   print(f\"Final id: {id(x)}\")\n   return(x)\n\nz = np.random.normal(size = 1000)\nout = fun_append2(z)\n\nInitial id: 140306825483456\niteration 0: size 56\niteration 0: size 88\niteration 4: size 120\niteration 8: size 184\niteration 16: size 248\niteration 24: size 312\niteration 32: size 376\niteration 40: size 472\niteration 52: size 568\niteration 64: size 664\niteration 76: size 792\niteration 92: size 920\niteration 108: size 1080\niteration 128: size 1240\niteration 148: size 1432\niteration 172: size 1656\niteration 200: size 1912\niteration 232: size 2200\niteration 268: size 2520\niteration 308: size 2872\niteration 352: size 3256\niteration 400: size 3704\niteration 456: size 4216\niteration 520: size 4792\niteration 592: size 5432\niteration 672: size 6136\niteration 760: size 6936\niteration 860: size 7832\niteration 972: size 8856\nFinal id: 140306825483456\n\n\nSurprisingly, the id of x doesn’t seem to change, even though we are allocating new memory at many of the iterations. What is happening is that x is an wrapper object that contains within it a reference to an array of references (pointers) to the list elements. The location of the wrapper object doesn’t change, but the underlying array of references/pointers is being reallocated.\nSide note: our assessment of size above does not include the actual size of the list elements.\n\nprint(sys.getsizeof(out))\n\n8856\n\nout[2] = np.random.normal(size = 100000)\nprint(sys.getsizeof(out))\n\n8856\n\n\nOne upshot of this is that if you need to grow an object use a Python list. Then once it is complete, you can always convert it to another type, such as a numpy array.\n\n\nVectorization and use of fast matrix algebra\nOne key way to write efficient Python code is to take advantage of numpy’s vectorized operations.\n\nn = 10**6\nz = np.random.normal(size = n)\nt0 = time.time()\nx = np.exp(z)\nprint(time.time() - t0)\n\n0.014235258102416992\n\nx = np.zeros(n)  # Leave out pre-allocation timing to focus on computation.\nt0 = time.time()\nfor i in range(n):\n    x[i] = np.exp(z[i])\n\n\nprint(time.time() - t0)\n\n0.8167824745178223\n\n\nSo what is different in how Python handles the calculations above that explains the huge disparity in efficiency? The vectorized calculation is being done natively in C in a for loop. The explicit Python for loop involves executing the for loop in Python with repeated calls to C code at each iteration. This involves a lot of overhead because of the repeated processing of the Python code inside the loop. For example, in each iteration of the loop, Python is checking the types of the variables because it’s possible that the types might change, as discussed earlier.\nYou can usually get a sense for how quickly a Python call will pass things along to C or Fortran by looking at the body of the relevant function(s) being called.\nUnfortunately seeing the source code in Python often involves going and finding it in a file on disk, whereas in R, printing a function will show its source code. However you can use ?? in IPython to get the code for non-builtin functions. Consider numpy.linspace??.\nHere I found the source code for the scipy triangular_solve function, which calls out to a Fortran function trtrs, found in the LAPACK library.\n\n## On an SCF machine:\n/usr/local/linux/miniforge-3.12/lib/python3.12/site-packages/scipy/linalg/_basic.py\n\nWith a bit more digging around we could verify that trtrs is a LAPACK funcion by doing some grepping:\n./linalg/_basic.py:    trtrs, = get_lapack_funcs(('trtrs',), (a1, b1))\nMany numpy and scipy functions allow you to pass in arrays, and operate on those arrays in vectorized fashion. So before writing a for loop, look at the help information on the relevant function(s) to see if they operate in a vectorized fashion. Functions might take arrays for one or more of their arguments.\nOutside of the numerical packages, we often have to manually do the looping:\n\nx = [3.5, 2.7, 4.6]\ntry:\n    math.cos(x)\nexcept Exception as error:\n    print(error)\n\nmust be real number, not list\n\n[math.cos(val) for val in x]\n\n[-0.9364566872907963, -0.9040721420170612, -0.11215252693505487]\n\nlist(map(math.cos, x))\n\n[-0.9364566872907963, -0.9040721420170612, -0.11215252693505487]\n\n\nChallenge: Consider the chi-squared statistic involved in a test of independence in a contingency table:\n\\[\n\\chi^{2}=\\sum_{i}\\sum_{j}\\frac{(y_{ij}-e_{ij})^{2}}{e_{ij}},\\,\\,\\,\\, e_{ij}=\\frac{y_{i\\cdot}y_{\\cdot j}}{y_{\\cdot\\cdot}}\n\\]\nwhere \\(y_{i\\cdot}=\\sum_{j}y_{ij}\\) and \\(y_{\\cdot j} = \\sum_{i} y_{ij}\\) and \\(y_{\\cdot\\cdot} = \\sum_{i} \\sum_{j} y_{ij}\\). Write this in a vectorized way without any loops. Note that ‘vectorized’ calculations also work with matrices and arrays.\nSometimes we can exploit vectorized mathematical operations in surprising ways, though sometimes the code is uglier.\n\nx = np.random.normal(size = n)\n\n## List comprehension\ntimeit.timeit('truncx = [max(0,val) for val in x]', number = 10, globals = {'x':x})\n\n1.8440100383013487\n\n\n\n## Vectorized slice replacement\ntimeit.timeit('truncx = x.copy(); truncx[x &lt; 0] = 0', number = 10, globals = {'x':x})\n\n0.06943236477673054\n\n\n\n## Vectorized math trick\ntimeit.timeit('truncx = x * x&gt;0', number = 10, globals = {'x':x})\n\n0.017205309122800827\n\n\nWe’ll discuss what has to happen (in terms of calculations, memory allocation, and copying) in the two vectorized approaches to try to understand which is more efficient.\nAdditional tips:\n\nIf you do need to loop over dimensions of a matrix or array, if possible loop over the smallest dimension and use the vectorized calculation on the larger dimension(s). For example if you have a 10000 by 10 matrix, try to set up your problem so you can loop over the 10 columns rather than the 10000 rows.\nIn general, in Python looping over rows is likely to be faster than looping over columns because of numpy’s row-major ordering (by default, matrices are stored in memory as a long array in which values in a row are adjacent to each other). However how numpy handles this is more complicated (see more in the Section on cache-aware programming), such that it may not matter for numpy calculations.\nYou can use direct arithmetic operations to add/subtract/multiply/divide a vector by each column of a matrix, e.g. A*b does element-wise multiplication of each column of A by a vector b. If you need to operate by row, you can do it by transposing the matrix.\n\nCaution: relying on Python’s broadcasting rule in the context of vectorized operations, such as is done when direct-multiplying a matrix by a vector to scale the columns relative to each other, can be dangerous as the code may not be easy for someone to read and poses greater dangers of bugs. In some cases you may want to first write the code more directly and then compare the more efficient code to make sure the results are the same. It’s also a good idea to comment your code in such cases.\n\n\nVectorization, mapping, and loops\nNext let’s consider when loops and mapping would be particularly slow and how mapping and loops might compare to each other.\nFirst, the potential for inefficiency of looping and map operations in interpreted languages will depend in part on whether a substantial part of the work is in the overhead involved in the looping or in the time required by the function evaluation on each of the elements.\nHere’s an example, where the core computation is very fast, so we might expect the overhead of looping (in its various forms seen here) to be important.\n\nimport time\nn = 10**6\nx = np.random.normal(size = n)\n\nt0 = time.time()\nout = np.exp(x)\ntime.time() - t0\n\n0.012601137161254883\n\nt0 = time.time()\nvals = np.zeros(n)\nfor i in range(n):\n    vals[i] = np.exp(x[i])\n\n\ntime.time() - t0\n\n0.7832422256469727\n\nt0 = time.time()\nvals = [np.exp(v) for v in x]\ntime.time() - t0\n\n0.6422276496887207\n\nt0 = time.time()\nvals = list(map(np.exp, x))\ntime.time() - t0\n\n0.5719358921051025\n\n\nRegardless of how we do the looping (an explicit loop, list comprehension, or map), it looks like we can’t avoid the overhead unless we use the vectorized call, which is of course the recommended approach in this case, both for speed and readability (and conciseness).\nSecond, is it faster to use map than to use a loop? In the example above it is somewhat faster to use map. That might not be surprising. In the loop case, the interpreter needs to do the checking we discussed earlier in this section at each iteration of the loop. What about in the map case? For mapping over a numpy array, perhaps not, but what if mapping over a list? So without digging into how map works, it’s hard to say.\nHere’s an example where the bulk of time is in the actual computation and not in the looping itself. We’ll run a bunch of regressions on a matrix X (i.e., each column of X is a predictor) using each column of the matrix mat to do a separate regression.\n\nimport time\nimport statsmodels.api as sm\n\nn = 500000;\nnr = 10000\nnCalcs = int(n/nr)\n\nmat = np.random.normal(size = (nr, nCalcs))\n\nX = list(range(nr))\nX = sm.add_constant(X)\n\ndef regrFun(i):\n    model = sm.OLS(mat[:,i], X)\n    return(model.fit().params[1])\n\nt0 = time.time()\nout1 = list(map(regrFun, range(nCalcs)))\ntime.time() - t0\n\n0.058492422103881836\n\nt0 = time.time()\nout2 = np.zeros(nCalcs)\nfor i in range(nCalcs):\n    out2[i] = regrFun(i)\n\n\ntime.time() - t0\n\n0.06137442588806152\n\n\nHere the looping is faster. I don’t have any particular explanation for that result.\n\n\nMatrix algebra efficiency\nOften calculations that are not explicitly linear algebra calculations can be done as matrix algebra. If our Python installation has a fast (and possibly parallelized) BLAS, this allows our calculation to take advantage of it.\nFor example, we can sum the rows of a matrix by multiplying by a vector of ones.\n\nmat = np.random.normal(size=(500,500))\n\ntimeit.timeit('mat.dot(np.ones(500))', setup = 'import numpy as np',\n              number = 1000, globals = {'mat': mat})\n\n0.024077018722891808\n\ntimeit.timeit('np.sum(mat, axis = 1)', setup = 'import numpy as np',\n              number = 1000, globals = {'mat': mat})\n\n0.12098740972578526\n\n\nGiven the extra computation involved in actually multiplying each number by one, it’s surprising that this is faster than numpy sum function. One thing we’d want to know is whether the BLAS matrix multiplication call is being done in parallel.\nOn the other hand, big matrix operations can be slow.\nChallenge: Suppose you want a new matrix that computes the differences between successive columns of a matrix of arbitrary size. How would you do this as matrix algebra operations? It’s possible to write it as multiplying the matrix by another matrix that contains 0s, 1s, and -1s in appropriate places. Here it turns out that the for loop is much faster than matrix multiplication. However, there is a way to do it faster as matrix direct subtraction.\n\n\nOrder of operations and efficiency\nWhen doing matrix algebra, the order in which you do operations can be critical for efficiency. How should I order the following calculation?\n\nn = 5000\nA = np.random.normal(size=(n, n))\nB = np.random.normal(size=(n, n))\nx = np.random.normal(size=n)\n\nt0 = time.time()\nres1 = (A @ B) @ x\nprint(time.time() - t0)\n\n2.1078994274139404\n\nt0 = time.time()\nres1 = A @ (B @ x)\nprint(time.time() - t0)\n\n0.0403590202331543\n\n\nWhy is the second order much faster?\n\n\nAvoiding unnecessary operations\nWe can use the matrix direct product (i.e., A*B) to do some manipulations much more quickly than using matrix multiplication. Challenge: How can I use the direct product to find the trace of a matrix, \\(XY\\)?\nFinally, when working with diagonal matrices, you can generally get much faster results by being smart. The following operations: \\(X+D\\), \\(DX\\), \\(XD\\) are mathematically the sum of two matrices and products of two matrices. But we can do the computation without using two full matrices. Challenge: How?\n\nn = 1000\nX = np.random.normal(size=(n, n))\ndiagvals = np.random.normal(size=n)\nD = np.diag(diagvals)\n\n# The following lines are very inefficient\nsummedMat = X + D\nprodMat1 = D @ X\nprodMat2 = X @ D\n\nMore generally, sparse matrices and structured matrices (such as block diagonal matrices) can generally be worked with MUCH more efficiently than treating them as arbitrary matrices. The scipy.sparse package (for both structured and arbitrary sparse matrices) can help, as can specialized code available in other languages, such as C and Fortran packages.\n\n\nSpeed of lookup operations\nThere are lots of situations in which we need to retrieve values for subsequent computations. In some cases we might be retrieving elements of an array or looking up values in a dictionary.\nLet’s compare the speed of some different approaches to lookup.\n\nn = 1000\nx = list(np.random.normal(size = n))\nkeys = [str(v) for v in range(n)]\nxD = dict(zip(keys, x))\n\ntimeit.timeit(\"x[500]\", number = 10**6, globals = {'x':x})\n\n0.01463991403579712\n\ntimeit.timeit(\"xD['500']\", number=10**6, globals = {'xD':xD})\n\n0.02826646901667118\n\n\nHow is it that Python can look up by key in the dictionary at essentially the same speed as jumping to an index position? It uses hashing, which allows O(1) lookup. In contrast, if one has to look through each key in turn, that is O(n), which is much slower:\n\ntimeit.timeit(\"x[keys.index('500')]\", number = 10**6, globals = {'x':x, 'keys':keys})\n\n5.7586834616959095\n\n\nAs a further point of contrast, if we look up elements by name in R in named vectors or lists, that is much slower than looking up by index, because R doesn’t use hashing in that context and has to scan through the objects one by one until it finds the one with the name it is looking for. This stands in contrast to R and Python being able to directly go to the position of interest based on the index of an array, or to the hash-based lookup in a Python dictionary or an R environment.\n\n\nHashing (including name lookup)\nAbove I mentioned that Python uses hashing to store and lookup values by key in a dictionary. I’ll briefly describe what hashing is here, because it is a commonly-used strategy in programming in general.\nA hash function is a function that takes as input some data (some input abitrary length) and maps it to a fixed-length output that can be used as a shortened reference to the data. (The function should be deterministic, always returing the same output for a given input.) We’ve seen this in the context of git commits where each commit was labeled with a long base-16 number. This also comes up when verifying files on the Internet. You can compute the hash value on the file you get and check that it is the same as the hash value associated with the legitimate copy of the file. Even small changes in the file will result in a different hash value.\nWhile there are various uses of hashing, for our purposes here, hashing can allow one to look up values by their name via a hash table. The idea is that you have a set of key-value pairs (sometimes called a dictionary) where the key is the name associated with the value and the value is some arbitrary object. You want to be able to quickly find the value/object quickly.\nHashing allows one to quickly determine an index associated with the key and therefore quickly find the relevant value based on the index. For example, one approach is to compute the hash as a function of the key and then take the remainder when dividing by the number of possible results (here the fact that the result is a fixed-length output is important) to get the index. Here’s the procedure in pseudocode:\n    hash = hashfunc(key) \n    index = hash %% array_size \n    ## %% is modulo operator - it gives the remainder\nIn general, there will be collisions – multiple keys will be assigned to the same index (this is unavoidable because of the fact that the hash function returns a fixed length output). However with a good hash function, usually there will be a small number of keys associated with a given bucket. So each bucket will contain a list of a small number of values and the associated keys. (The buckets might contain the actual values or they might contain the addresses of where the values are actually stored if the values are complicated objects.) Then determining the correct value (or the required address) within a given bucket is fast even with simple linear search through the items one by one. Put another way, the hash function distributes the keys amongst an array of buckets and allows one to look up the appropriate bucket quickly based on the computed index value. When the hash table is properly set up, the cost of looking up a value does not depend on the number of key-value pairs stored.\nPython uses hashing to look up the value based on the key in a given dictionary, and similarly when looking up variables in namespaces. This allows Python to retrieve objects very quickly.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit5-programming.html#additional-general-strategies-for-efficiency",
    "href": "units/unit5-programming.html#additional-general-strategies-for-efficiency",
    "title": "Programming concepts",
    "section": "Additional general strategies for efficiency",
    "text": "Additional general strategies for efficiency\nIt’s also useful to be aware of some other strategies for improving efficiency.\n\nCache-aware programming\nIn addition to main memory (what we usually mean when we talk about RAM), computers also have memory caches, which are small amounts of fast memory that can be accessed very quickly by the processor. For example your computer might have L1, L2, and L3 caches, with L1 the smallest and fastest and L3 the largest and slowest. The idea is to try to have the data that is most used by the processor in the cache.\nIf the next piece of data needed for computation is available in the cache, this is a cache hit and the data can be accessed very quickly. However, if the data is not available in the cache, this is a cache miss and the speed of access will be a lot slower. Cache-aware programming involves writing your code to minimize cache misses. Generally when data is read from memory it will be read in chunks, so values that are contiguous will be read together.\nHow does this inform one’s programming? For example, if you have a matrix of values stored in row-major order, computing on a row will be a lot faster than computing on a column, because the row can be read into the cache from main memory and then accessed in the cache. In contrast, if the matrix is large and therefore won’t fit in the cache, when you access the values of a column, you’ll have to go to main memory repeatedly to get the values for the row because the values are not stored contiguously.\nThere’s a nice example of the importance of the cache at the bottom of this blog post.\nIf you know the size of the cache, you can try to design your code so that in a given part of your code you access data structures that will fit in the cache. This sort of thing is generally more relevant if you’re coding in a language like C. But it can matter sometimes in interpreted languages too.\nLet’s see what happens in Python. By default, matrices in numpy are row-major, also called “C order”. I’ll create a long matrix with a small number of very long columns and a wide matrix with a small number of very long rows.\n\nnr = 800000\nnc = 100\n\nA = np.random.normal(size=(nr, nc))   # long matrix\ntA = np.random.normal(size=(nc, nr))  # wide matrix\n\n## Verify that A is row-major using `.flags` (notice the `C_CONTIGUOUS: True`).\nA.flags\n\n  C_CONTIGUOUS : True\n  F_CONTIGUOUS : False\n  OWNDATA : True\n  WRITEABLE : True\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n\n\nNote that I didn’t use A.T or np.transpose as that doesn’t make a copy in memory and so the transposed matrix doesn’t end up being row-major. You can use A.flags and A.T.flags` to see this.\nNow let’s time calculating the sum by column in the long matrix vs. the sum by row in the wide matrix. Exactly the same number of arithmetic operations needs to be done in an equivalent manner for the two cases. We want to use a large enough matrix so the entire matrix doesn’t fit in the cache, but not so large that the example takes a long time or a huge amount of memory. We’ll use a rectangular matrix, such that the summation for a single column of the long matrix or a single row of the wide matrix involves many numbers, but there are a limited number of such summations. This focuses the example on the efficiency of the column-wise vs. row-wise summation rather than any issues that might be involved in managing large numbers of such summations (e.g., doing many, many summations that involve just a few numbers).\n\n# Define the sum calculations as functions\ndef sum_by_column():\n    return np.sum(A, axis=0)\n\ndef sum_by_row():\n    return np.sum(tA, axis=1)\n\ntimeit.timeit(sum_by_column, number=10)  # potentially slow\n\n0.504505192860961\n\ntimeit.timeit(sum_by_row, number=10)\n\n0.4103789310902357\n\n\nSuppose we instead do the looping manually.\n\ntimeit.timeit('[np.sum(A[:,col]) for col in range(A.shape[1])]',\n    setup = 'import numpy as np', number=10, globals = {'A': A})   \n\n5.139367422088981\n\ntimeit.timeit('[np.sum(tA[row,:]) for row in range(tA.shape[0])]',\n    setup = 'import numpy as np', number=10, globals = {'tA': tA})  \n\n0.408579183742404\n\n\nIndeed, the row-wise calculations are much faster when done manually. However, when done with the axis argument in np.sum there is little difference. So that suggests numpy might be doing something clever in its implementation of sum with the axis argument.\n\nChallenge: suppose you were writing code for this kind of use case. How could you set up your calculations to do either row-wise or column-wise operations in a way that processes each number sequentially based on the order in which the numbers are stored. For example suppose the values are stored row-major but you want the column sums.\n\nWhen we define a numpy array, we can choose to use column-major order (i.e., “Fortran” order) with the order argument.\n\n\nLoop fusion\nLet’s consider this (vectorized) code:\n\nx = np.exp(x) + 3*np.sin(x)\n\nThis code has some downsides.\n\nThink about whether any additional memory has to be allocated.\nThink about how many for loops will have to get executed.\n\nContrast that to running directly as a for loop (e.g., here in Julia or in C/C++):\n\nfor i in 1:length(x)\n    x[i] = exp(x[i]) + 3*sin(x[i])\nend\n\nHow does that affect the downsides mentioned above?\nCombining loops is called ‘fusing’ and is an important optimization that Julia can do, as shown in this demo. It’s also a key optimization done by XLA, a compiler used with JAX and Tensorflow, so one approach to getting loop fusion in Python is to use JAX (or Tensorflow) for such calculations within Python rather than simply using numpy.\n\n\nJIT compilation (with JAX)\nYou can think of JAX as a version of numpy enabled to use the GPU (or automatically parallelize on CPU threads) and provide automatic differentiation.\nWe can also JIT compile JAX code. Behind the scenes, the instructions are compiled to machine code for different backends (e.g., CPU and GPU) using the XLA compiler.\nLet’s first consider running a vectorized calculation using JAX on the CPU, which will use multiple threads (discussed in Unit 6), each thread running on a separate CPU core on our computer. For now all we need to know is that the calculation will run in parallel, so we expect it to be faster than when using numpy, which won’t run the calculation in parallel.\n\nimport time\nimport numpy as np\nimport jax.numpy as jnp\n\ndef myfun_np(x):\n    y = np.exp(x) + 3 * np.sin(x)\n    return(y)\n    \ndef myfun_jnp(x):\n    print('hello')\n    y = jnp.exp(x) + 3 * jnp.sin(x)\n    return(y)\n\nn = 500000000\n\nx = np.random.normal(size = n).astype(np.float32)  # 32-bit for consistency with JAX default\nx_jax = jnp.array(x)  # 32-bit by default\nprint(x_jax.platform())\n\n\nt0 = time.time()\nz = myfun_np(x)\nt1 = time.time() - t0\n\nt0 = time.time()\nz_jax = myfun_jnp(x_jax).block_until_ready()\nt2 = time.time() - t0\n\nprint(f\"numpy time: {round(t1,3)}\\njax time: {round(t2,3)}\")\n\nRunning on the SCF gandalf machine (not shown above), we get these times.\nnumpy time: 17.181\njax time: 5.722\nThere’s a nice speedup compared to numpy.\nSince JAX will often execute computations asynchronously (in particular when using the GPU), the block_until_ready invocation ensures that the computation finishes before we stop timing.\nBy default the JAX floating point type is 32-bit so we forced the use of 32-bit numbers for numpy for comparability. One could have JAX use 64-bit numbers like this:\n\nimport jax.config\njax.config.update(\"jax_enable_x64\", True)  \n\nNext let’s consider JIT compiling it, which should fuse the vectorized operations and avoid temporary objects. The JAX docs have a nice discussion of when JIT compilation will be beneficial.\n\nimport jax\nmyfun_jnp_jit = jax.jit(myfun_jnp)\n\nt0 = time.time()\nz_jax_jit = myfun_jnp_jit(x_jax).block_until_ready()\nt3 = time.time() - t0\nprint(f\"jitted jax time: {round(t3,3)}\")\n\njitted jax time: 3.874\nSo that gives some additional speedup, although in this case, it’s not a huge improvement.\n\n\nLazy evaluation\nWhat’s strange about this R code?\n\nf &lt;- function(x) print(\"hi\")\nsystem.time(mean(rnorm(1000000)))\n\n   user  system elapsed \n  0.059   0.000   0.059 \n\nsystem.time(f(3))\n\n[1] \"hi\"\n\n\n   user  system elapsed \n      0       0       0 \n\nsystem.time(f(mean(rnorm(1000000)))) \n\n[1] \"hi\"\n\n\n   user  system elapsed \n  0.001   0.000   0.001 \n\n\nLazy evaluation is not just an R thing. It also occurs in Tensorflow (particularly version 1), the Python Dask package, and in Spark. The basic idea is to delay executation until it’s really needed, with the goal that if one does so, the system may be able to better optimize a series of multiple steps as a joint operation relative to executing them one by one.\nHowever, Python itself does not have lazy evaluation.",
    "crumbs": [
      "Units",
      "Unit 5 (Programming)"
    ]
  },
  {
    "objectID": "units/unit3-bash.html",
    "href": "units/unit3-bash.html",
    "title": "The bash shell and UNIX commands",
    "section": "",
    "text": "PDF\nReference:",
    "crumbs": [
      "Units",
      "Unit 3 (Bash shell)"
    ]
  },
  {
    "objectID": "units/unit3-bash.html#first-challenge",
    "href": "units/unit3-bash.html#first-challenge",
    "title": "The bash shell and UNIX commands",
    "section": "4.1 First challenge",
    "text": "4.1 First challenge\nConsider the file cpds.csv. How would you write a shell command that returns “There are 8 occurrences of the word ‘Belgium’ in this file.”, where ‘8’ should instead be the correct number of times the word occurs.\nExtra: make your code into a function that can operate on any file indicated by the user and any word of interest.",
    "crumbs": [
      "Units",
      "Unit 3 (Bash shell)"
    ]
  },
  {
    "objectID": "units/unit3-bash.html#second-challenge",
    "href": "units/unit3-bash.html#second-challenge",
    "title": "The bash shell and UNIX commands",
    "section": "4.2 Second challenge",
    "text": "4.2 Second challenge\nConsider the data in the RTADataSub.csv file. This is a subset of data giving freeway travel times for segments of a freeway in an Australian city. The data are from a kaggle.com competition. We want to try to understand the kinds of data in each field of the file. The following would be particularly useful if the data were in many files or the data were many gigabytes in size.\n\nFirst, take the fourth column. Figure out the unique values in that column.\nNext, automate the process of determining if any of the values are non-numeric so that you don’t have to scan through all of the unique values looking for non-numbers. You’ll need to look for the following regular expression pattern [^0-9], which is interpreted as NOT any of the numbers 0 through 9.\n\nExtra: do it for all the fields, except the first one. Have your code print out the result in a human-readable way understandable by someone who didn’t write the code. For simplicity, you can assume you know the number of fields.",
    "crumbs": [
      "Units",
      "Unit 3 (Bash shell)"
    ]
  },
  {
    "objectID": "units/unit3-bash.html#third-challenge",
    "href": "units/unit3-bash.html#third-challenge",
    "title": "The bash shell and UNIX commands",
    "section": "4.3 Third challenge",
    "text": "4.3 Third challenge\n\nFor Belgium, determine the minimum unemployment value (field #6) in cpds.csv in a programmatic way.\nHave what is printed out to the screen look like “Belgium 6.2”.\nNow store the unique values of the countries in a variable, first stripping out the quotation marks.\nFigure out how to automate step 1 to do the calculation for all the countries and print to the screen.\nHow would you instead store the results in a new file?",
    "crumbs": [
      "Units",
      "Unit 3 (Bash shell)"
    ]
  },
  {
    "objectID": "units/unit3-bash.html#fourth-challenge",
    "href": "units/unit3-bash.html#fourth-challenge",
    "title": "The bash shell and UNIX commands",
    "section": "4.4 Fourth challenge",
    "text": "4.4 Fourth challenge\nLet’s return to the RTADataSub.csv file and the issue of missing values.\n\nCreate a new file without any rows that have an ‘x’ (which indicate a missing value).\nTurn the code into a function that also prints out the number of rows that are being removed and that sends its output to stdout so that it can be used with piping.\nNow modify your function so that the user could provide the missing value string and the input filename.",
    "crumbs": [
      "Units",
      "Unit 3 (Bash shell)"
    ]
  },
  {
    "objectID": "units/unit3-bash.html#fifth-challenge",
    "href": "units/unit3-bash.html#fifth-challenge",
    "title": "The bash shell and UNIX commands",
    "section": "4.5 Fifth challenge",
    "text": "4.5 Fifth challenge\nConsider the coop.txt weather station file.\nFigure out how to use grep to tell you the starting position of the state field. Hints: search for a known state-country combination and figure out what flags you can use with grep to print out the “byte offset” for the matched state.\nUse that information to automate the first mission where we extracted the state field using cut. You’ll need to do a bit of arithmetic using shell commands.",
    "crumbs": [
      "Units",
      "Unit 3 (Bash shell)"
    ]
  },
  {
    "objectID": "units/unit3-bash.html#sixth-challenge",
    "href": "units/unit3-bash.html#sixth-challenge",
    "title": "The bash shell and UNIX commands",
    "section": "4.6 Sixth challenge",
    "text": "4.6 Sixth challenge\nHere’s an advanced one - you’ll probably need to use sed, but the brief examples of text substitution in the using bash tutorial (or in the demos above) should be sufficient to solve the problem.\nConsider a CSV file that has rows that look like this:\n1,\"America, United States of\",45,96.1,\"continental, coastal\" \n2,\"France\",33,807.1,\"continental, coastal\"\nWhile Pandas would be able to handle this using read_csv(), using cut in UNIX won’t work because of the commas embedded within the fields. The challenge is to convert this file to one that we can use cut on, as follows.\nFigure out a way to make this into a new delimited file in which the delimiter is not a comma. At least one solution that will work for this particular two-line dataset does not require you to use regular expressions, just simple replacement of fixed patterns.",
    "crumbs": [
      "Units",
      "Unit 3 (Bash shell)"
    ]
  },
  {
    "objectID": "units/unit3-bash.html#versions-of-regular-expressions",
    "href": "units/unit3-bash.html#versions-of-regular-expressions",
    "title": "The bash shell and UNIX commands",
    "section": "Versions of regular expressions",
    "text": "Versions of regular expressions\nOne thing that can cause headaches is differences in version of regular expression syntax used. As discussed in man grep, extended regular expressions are standard, with basic regular expressions providing less functionality and Perl regular expressions additional functionality.\nThe bash shell tutorial provides a full documentation of the extended regular expressions syntax, which we’ll focus on here. This syntax should be sufficient for most usage and should be usable in Python and R, but if you notice something funny going on, it might be due to differences between the regular expressions versions.\n\nIn bash, grep -E (or egrep) enables use of the extended regular expressions, while grep -P enables Perl-style regular expressions.\nIn Python, the re package provides syntax “similar to” Perl.\nIn R, stringr provides ICU regular expressions (see help(regex)), which are based on Perl regular expressions.\n\nMore details about Perl regular expressions can be found in the regex Wikipedia page.",
    "crumbs": [
      "Units",
      "Unit 3 (Bash shell)"
    ]
  },
  {
    "objectID": "units/unit3-bash.html#general-principles-for-working-with-regex",
    "href": "units/unit3-bash.html#general-principles-for-working-with-regex",
    "title": "The bash shell and UNIX commands",
    "section": "General principles for working with regex",
    "text": "General principles for working with regex\nThe syntax is very concise, so it’s helpful to break down individual regular expressions into the component parts to understand them. As Murrell notes, since regex are their own language, it’s a good idea to build up a regex in pieces as a way of avoiding errors just as we would with any computer code. re.findall in Python and str_detect in R’s stringr, as well as regex101.com are particularly useful in seeing what was matched to help in understanding and learning regular expression syntax and debugging your regex. As with many kinds of coding, I find that debugging my regex is usually what takes most of my time.",
    "crumbs": [
      "Units",
      "Unit 3 (Bash shell)"
    ]
  },
  {
    "objectID": "units/unit3-bash.html#challenge-problem",
    "href": "units/unit3-bash.html#challenge-problem",
    "title": "The bash shell and UNIX commands",
    "section": "Challenge problem",
    "text": "Challenge problem\nChallenge: Let’s think about what regex syntax we would need to detect any number, integer- or real-valued. Let’s start from a test-driven development perspective of writing out test cases including:\n\nvarious cases we want to detect,\nvarious tricky cases that are not numbers and we don’t want to detect, and\n“corner cases” – tricky (perhaps unexpected) cases that might trip us up.",
    "crumbs": [
      "Units",
      "Unit 3 (Bash shell)"
    ]
  },
  {
    "objectID": "units/unit4-goodPractices.html",
    "href": "units/unit4-goodPractices.html",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "",
    "text": "PDF\nSources:\nThis unit covers good coding/software development practices, debugging (and practices for avoiding bugs), and doing reproducible research. As in later units of the course, the material is generally not specific to Python, but some details and the examples are in Python.",
    "crumbs": [
      "Units",
      "Unit 4 (Good practices)"
    ]
  },
  {
    "objectID": "units/unit4-goodPractices.html#editors",
    "href": "units/unit4-goodPractices.html#editors",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Editors",
    "text": "Editors\nUse an editor that supports the language you are using (e.g., Atom, Emacs/Aquamacs, Sublime, vim, VSCode, TextMate, WinEdt, or the built-in editor in RStudio [you can use Python from within RStudio]). Some advantages of this can include:\n\nhelpful color coding of different types of syntax,\nautomatic indentation and spacing,\nparenthesis matching,\nline numbering (good for finding bugs), and\ncode can often be run (or compiled) and debugged from within the editor.\n\nSee the problem set submission how-to for more information about editors that interact nicely with Quarto documents.",
    "crumbs": [
      "Units",
      "Unit 4 (Good practices)"
    ]
  },
  {
    "objectID": "units/unit4-goodPractices.html#code-syntax-and-style",
    "href": "units/unit4-goodPractices.html#code-syntax-and-style",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Code syntax and style",
    "text": "Code syntax and style\n\nCoding syntax tips\nThe PEP 8 style guide is your go-to reference for Python style. I’ve highlighted some details here as well as included some general suggestions of my own.\n\nHeader information: put metainfo on the code into the first few lines of the file as comments. Include who, when, what, how the code fits within a larger program (if appropriate), possibly the versions of Python and key packages that you used.\nWrite docstrings for public modules, classes, functions, and methods. For non-public items, a comment after the def line is sufficient to describe the purpose of the item.\nIndentation: Python is strict about indentation of course, which helps to enforce clear indentation more than in other languages. This helps you and others to read and understand the code and can help in detecting errors in your code because it can expose lack of symmetry.\n\nuse 4 spaces per indentation level (avoid tabs if possible).\n\nWhitespace: use it in a variety of places. Some places where it is good to have it are\n\naround operators (assignment and arithmetic);\nbetween function arguments;\nbetween list/tuple elements; and\nbetween matrix/array indices.\n\nUse blank lines to separate blocks of code with comments to say what the block does.\nUse whitespaces or parentheses for clarity even if not needed for order of operations. For example, a/y*x will work but is not easy to read and you can easily induce a bug if you forget the order of ops. Instead, use a/y * x.\nAvoid code lines longer than 79 characters and comment/docstring lines longer than 72 characters.\nComments: add helpful comments (but don’t belabor the obvious, such as x = x + 1  # increment x).\n\nRemember that in a few months, you may not follow your own code any better than a stranger.\nSome key things to document: (1) summarizing a block of code, (2) explaining a very complicated piece of code - recall our complicated regular expressions, and (3) explaining arbitrary constant values.\nComments should generally be complete sentences.\n\nYou can use parentheses to group operations such that they can be split up into lines and easily commented, e.g.,\n\nnewdf = (\n        pd.read_csv('file.csv')\n        .rename(columns = {'STATE': 'us_state'})  # adjust column names\n        .dropna()                                 # remove some rows\n        )\n\nFor software development, break code into separate files (2000-3000 lines per file) with meaningful file names and related functions grouped within a file.\nBeing consistent about the naming style for objects and functions is hard, but try to be consistent. PEP8 suggests:\n\nClass names should be UpperCamelCase.\nFunction, method, and variable names should be snake_case, e.g., number_of_its or n_its.\nNon-public methods and variables should have a leading underscore.\n\nTry to have the names be informative without being overly long.\nDon’t overwrite names of objects/functions that already exist in Python. E.g., don’t use len. That said, the namespace system helps with the unavoidable cases where there are name conflicts.\nUse active names for functions (e.g., calc_loglik, calc_log_lik rather than loglik or loglik_calc). The idea is that a function in a programming language is like a verb in regular language (a function does something), so use a verb to name it.\nLearn from others’ code\n\nThis semester, someone will be reading your code - the GSI and and me when we look at your assignments. So to help us in understanding your code and develop good habits, put these ideas into practice in your assignments.\nWhile not Python examples, the files goodCode.R and badCode.R in the units directory of the class repository provide examples of R code written such that it does and does not conform to the general ideas listed above (leaving aside the different syntax of Python and R).\n\n\nCoding style suggestions\nThis is particularly focused on software development, but some of the ideas are useful for data analysis as well.\n\nBreak down tasks into core units.\nWrite reusable code for core functionality and keep a single copy of the code (using version control) so you only need to make changes to a piece of code in one place.\nSmaller functions are easier to debug, easier to understand, and can be combined in a modular fashion (like the UNIX utilities).\nWrite functions that take data as an argument and not lines of code that operate on specific data objects. Why? Functions allow us to reuse blocks of code easily for later use and for recreating an analysis (reproducible research). It’s more transparent than sourcing a file of code because the inputs and outputs are specified formally, so you don’t have to read through the code to figure out what it does.\nFunctions should:\n\nbe modular (having a single task);\nhave meaningful name; and\nhave a doc string describing their purpose, inputs and outputs.\n\nWrite tests for each function (i.e., unit tests).\nDon’t hard code numbers - use variables (e.g., number of iterations, parameter values in simulations), even if you don’t expect to change the value, as this makes the code more readable. For example, the speed of light is a constant in a scientific sense, but best to make it a variable in code: speed_of_light = 3e8.\nUse lists or tuples to keep disparate parts of related data together.\nPractice defensive programming (see also the discussion below on raising exceptions and assertions):\n\ncheck function inputs and warn users if the code will do something they might not expect or makes particular choices;\ncheck inputs to if:\n\nNote that in Python, an expression used as the condition of an if will be equivalent to True unless it is one of False, 0, None, or an empty list/tuple/string.\n\nprovide reasonable default arguments;\ndocument the range of valid inputs;\ncheck that the output produced is valid; and\nstop execution based on checks and give an informative error message.\n\nTry to avoid system-dependent code that only runs on a specific version of an OS or specific OS.\nLearn from others’ code.\nAsk others to review your code and help by reviewing other people’s code.\nConsider rewriting your code once you know all the settings and conditions; often analyses and projects meander as we do our work and the initial plan for the code no longer makes sense and the code is no longer designed specifically for the job being done.\n\n\n\nLinting\nLinting is the process of applying a tool to your code to enforce style.\nHere we’ll see how to use ruff. You might also consider black.\nWe’ll practice with ruff with a small module we’ll use next also for debugging.\nFirst, we check for and fix syntax errors.\nruff check test.py\nAll checks passed!\nThen we ask ruff to reformat to conform to standard style.\ncp test.py test-save.py   # Not required, just so we can see what `ruff` did.\nruff format test.py\n1 file reformatted\nLet’s see what changed:\ndiff test-save.py test.py\n1c1\n&lt; x=  7\n---\n&gt; x = 7\n3,5d2\n&lt; def myfun(x, y =0):\n&lt; # This is a toy function.\n&lt;       return x*y\n6a4,6\n&gt; def myfun(x, y=0):\n&gt;     # This is a toy function.\n&gt;     return x * y\nSo we see that ruff fixed spacing and properly indented the comment line.",
    "crumbs": [
      "Units",
      "Unit 4 (Good practices)"
    ]
  },
  {
    "objectID": "units/unit4-goodPractices.html#assertions-exceptions-and-testing",
    "href": "units/unit4-goodPractices.html#assertions-exceptions-and-testing",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Assertions, exceptions and testing",
    "text": "Assertions, exceptions and testing\nAssertions, exceptions and testing are critically important for writing robust code that is less likely to contain bugs.\n\nExceptions\nYou’ve probably already seen exceptions in action whenever you’ve done something in Python that causes an error to occur and an error message to be printed. Syntax errors are different from exceptions in that exceptions occur when the syntax is correct, but the execution of the code results in some sort of error (i.e., a run-time error).\nExceptions can be a valuable tool for making your code handle different modes of failure (missing file, URL unreachable, permission denied, invalid inputs, etc.). You use them when you are writing code that is supposed to perform a task (e.g., a function that does something with an input file) to indicate that the task failed and the reason for that failure (e.g., the file was not there in the first place). In such a case, you want your code to raise an exception and make the error message informative as possible, typically by handling the exception thrown by another function you call and augmenting the message. Another possibility is that your code detects a situation where you need to throw an exception (e.g., an invalid input to a function).\nThe other side of the coin happens when you want to write a piece of code that handles failure in a specific way, instead of simply giving up. For example, if you are writing a script that reads and downloads hundreds of URLs, you don’t want your program to stop when any of them fails to respond (or do you?). You might want to continue with the rest of the URLs, and write out the failed URLs in a secondary output file.\n\nUsing try-except to continue execution\nIf you want some code to continue running even when it encounters an error, you can use try-except. This would often be done in code where you were running some workflow, but can also be useful in functions that you write for general purpose use (e.g., code in a package you are writing).\nSuppose we have a loop and we want to run all the iterations even if the code for some iterations fail. We can embed the code that might fail in a try block and then in the except block, run code that will handle the situation when the error occurs.\n\nfor i in range(n):\n    try:\n        &lt;some code that might fail&gt;\n        result[i] = &lt;actual result&gt;\n    except:\n        &lt;what to do if the code fails&gt;\n        result[i] = None   \n\n\n\nStrategies for invoking and handling errors\nHere we’ll address situations that might arise when you are developing code for general purpose use (e.g., writing functions in a package) and need that code to invoke an error under certain circumstances or deal gracefully with an error occurring in some code that you are calling.\nA basic situation is when you want to detect a situation where you need to invoke an error (i.e., “throw an exception”).\nWith raise you can invoke an exception. Suppose we need an input to be a positive number. We’ll use Python’s built-in ValueError, one of the various exception types that Python provides and that you could use. You can also create your own exceptions by subclassing one of Python’s existing exception classes. (We haven’t yet discussed classes and object-oriented programming, so don’t worry if you’re not sure about what that means.)\n\ndef myfun(val):\n    if val &lt;= 0:\n        raise ValueError(\"`val` should be positive\")\n\nmyfun(-3)\n\nValueError: `val` should be positive\nNext let’s consider cases where your function runs some code that might return an error.\nWe’d often want to catch the error using try-except. In some cases we would want to notify the user and then continue (perhaps falling back to a different way to do things or returning None from our function) while in others we might want to provide a more informative error message than if we had just let the error occur, but still have the exception be raised.\nFirst let’s see the case of continuing execution.\n\nimport os\n\ndef myfun(filename):\n    try:\n        with open(filename, \"r\") as file:\n            text = file.read()\n    except Exception as err:\n        print(f\"{err}\\nCheck that the file `{filename}` can be found \"\\\n              f\"in the current path: `{os.getcwd()}`.\")\n        return None\n\n    return(text.lower())\n\n\nmyfun('missing_file.txt')\n\n[Errno 2] No such file or directory: 'missing_file.txt'\nCheck that the file `missing_file.txt` can be found in the current path: `/accounts/vis/paciorek/teaching/243fall24/fall-2024/units`.\nFinally let’s see how we can intercept an error but then “re-raise” the error rather than continuing execution.\n\nimport requests\n\ndef myfun(url):\n    try:\n        requests.get(url)\n    except Exception as err:\n        print(f\"There was a problem accessing {url}. \"\\\n              f\"Perhaps it doesn't exist or the URL has a typo?\")\n        raise\n\nmyfun(\"http://missingurl.com\")\n\nThere was a problem accessing http://missingurl.com. Perhaps it doesn't exist or the URL has a typo?\n\nConnectionError: HTTPConnectionPool(host='missingurl.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f348db44590&gt;: Failed to establish a new connection: [Errno -2] Name or service not known'))\n\n\n\nAssertions\nAssertions are a quick way to raise a specific type of Exception (AssertionError). Assertions are useful for performing quick checks in your code that the state of the program is as you expect. They’re primarily intended for use during the development process to provide “sanity checks” that specific conditions are true, and there are ways to disable them when you are running your code for production purposes (to improve performance). A common use is for verifying preconditions and postconditions (especially preconditions). One would generally only expect such conditions not to be true if there is a bug in the code. Here’s an example of using the assert statement in Python, with a clear assertion message telling the developer what the problem is.\n\nnumber = -42\nassert number &gt; 0, f\"number greater than 0 expected, got: {number}\"\n## Produces this error:\n## Traceback (most recent call last):\n##   File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n## AssertionError: number greater than 0 expected, got: -42\n\nVarious operators/functions are commonly used in assertions, including\n\nassert x in y\nassert x not in y\nassert x is y\nassert x is not y\nassert isinstance(x, &lt;some_type&gt;)\nassert all(x)\nassert any(x)\n\nAssertions can also be quite useful in the context of data analysis pipelines for checking that the state of the analysis is as expected. E.g., at various points, you might check that the dimension of your data is as expected, check for missing or extreme/unexpected values, etc.\nNext we’ll see that assertions are a core part of setting up tests.\n\n\nTesting\nTesting is informally what you do after you write some code and want to check that it actually works. But when you are developing important code (e.g. functions that are going to be used by others) you typically want to encode your tests in code. There are many reasons to do that, including making sure that if anyone changes the code later on and breaks something, the test suite should immediately indicate that.\nSome people even advocate for writing a preliminary test suite before writing the code itself(!) as it can be a good way to organize work and track progress, as well as act as a secondary form of documentation for clarity. This can include tests that your code provides correct and useful errors when something goes wrong (so that means that a test might be to see if problematic input correctly produces an error). Unit tests are intended to test the behavior of small pieces (units) of code, generally individual functions. Unit tests naturally work well with the ideas above of writing small, modular functions. I recommend the pytest package, which is designed to make it easier to write sets of good tests.\nHere is an example of the contents of a test file, called test_dummy.py.\n\nimport pytest\nimport numpy as np\n\nimport dummy\n\ndef test_numeric():\n    assert dummy.add_one(3) == 4\n\n# This test will fail.\ndef test_numpy_array():\n    assert np.all(np.equal(dummy.add_one(np.array([3,4])), np.array([4,5])))\n\ndef test_bad_input():\n    with pytest.raises(TypeError):\n        dummy.add_one('hello')\n\ndef test_warning():\n    with pytest.warns(UserWarning, match='complex'):\n        dummy.add_one(1+3j)\n\nWe can then run the tests via pytest like this:\npytest mytestfile.py\n====================================== test session starts ======================================\nplatform linux -- Python 3.12.2, pytest-8.1.1, pluggy-1.4.0\nrootdir: /accounts/vis/paciorek/teaching/243fall24/fall-2024/units\nplugins: anyio-4.3.0\ncollected 4 items                                                                               \n\ntest_dummy.py .F..                                                                        [100%]\n\n=========================================== FAILURES ============================================\n_______________________________________ test_numpy_array ________________________________________\n\n    def test_numpy_array():\n&gt;       assert np.all(np.equal(dummy.add_one(np.array([3,4])), np.array([4,5])))\n\ntest_dummy.py:11: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nx = array([3, 4])\n\n    def add_one(x):\n        if not isinstance(x, (float, int, complex)):\n&gt;           raise TypeError(f\"`{x}` should be numeric\")\nE           TypeError: `[3 4]` should be numeric\n\ndummy.py:5: TypeError\n==================================== short test summary info ====================================\nFAILED test_dummy.py::test_numpy_array - TypeError: `[3 4]` should be numeric\n================================== 1 failed, 3 passed in 0.84s ==================================\nIn lab, we’ll go over assertions, exceptions, and testing in detail.\n\n\nAutomated testing\nContinuous integration (CI) is the term for carrying out actions automatically as your code changes. The most common kind of CI is automated testing - running your tests on your code when you make changes to the code. This enforces the discipline of running tests regularly and avoids the common problem that testing passes locally on your own machine, but fails for various reasons when done elsewhere.\nA standard way to do this is via GitHub Actions (GHA).\nTo set up a GitHub Actions workflow, one\n\nspecifies when the workflow will run (e.g., when a push or pull request is made, or only manually)\nprovides instructions for how to set up the environment for the workflow\nprovides the operations that the workflow should run.\n\nThe workflow is specified using a YAML file placed in the .github/workflows directory of the repository.\nWith GHA, you specify the operating system and then the steps to run in the YAML file. Some steps will customize the environment as the initial steps and then additional step(s) will run shell or other code to run your workflow. You use pre-specified operations (called actions) to do common things (such as checking out a GitHub repository and installing commonly used software).\nWhen triggered, GitHub will run the steps in a virtual machine, which is called the runner.\nHere’s an example YAML file for testing an example package called mytoy:\non:\n  push:\n    branches:\n    - main\n\njobs:\n  CI:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n\n    # Install package and dependencies\n    - name: Set up Python \n      uses: actions/setup-python@v5\n      with:\n        python-version: 3.12\n\n    - name: Install mytoy and pytest\n      run: |\n        pip install pytest\n        pip install --user .\n\n    - name: Run tests\n      run: |\n        cd mytoy\n        pytest\nYou’ll use GitHub Actions to automate testing during your projects. Unfortunately, GitHub Actions is not available via github.berkeley.edu so we can’t use it with your class repositories for your problem set work.",
    "crumbs": [
      "Units",
      "Unit 4 (Good practices)"
    ]
  },
  {
    "objectID": "units/unit4-goodPractices.html#version-control",
    "href": "units/unit4-goodPractices.html#version-control",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Version control",
    "text": "Version control\n\nUse it! Even for projects that only you are working on. It’s the closest thing you’ll get to having a time machine!\nUse an issues tracker (e.g., the GitHub issues tracker is quite nice), or at least a simple to-do file, noting changes you’d like to make in the future.\nIn addition to good commit messages, it’s a good idea to keep good running notes documenting your projects.\n\nWe’ll be discussing Git a lot separately.",
    "crumbs": [
      "Units",
      "Unit 4 (Good practices)"
    ]
  },
  {
    "objectID": "units/unit4-goodPractices.html#basic-debugging-strategies",
    "href": "units/unit4-goodPractices.html#basic-debugging-strategies",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Basic debugging strategies",
    "text": "Basic debugging strategies\nHere we’ll discuss some basic strategies for finding and fixing bugs. Other useful locations for tips on debugging include:\n\nEfficient Debugging by Goldspink\nDebugging for Beginners by Brody\n\nRead and think about the error message (the traceback), starting from the bottom of the traceback. Sometimes it’s inscrutable, but often it just needs a bit of deciphering. Looking up a given error message by simply doing a web search with the exact message in double quotes can be a good strategy, or you could look specifically on Stack Overflow.\nBelow we’ll see how one can view the stack trace. Usually when an error occurs, it occurs in a function call that is nested in a series of function calls. This series of calls is the call stack and the traceback or stack trace shows that series of calls that led to the error. To debug, you’ll often need to focus on the function being executed at the time the error occurred (which will be at the top of the call stack but the bottom of the traceback) and the arguments passed into that function. However, if the error occurs in a function you didn’t write, the problem will often be with the arguments that your code provided at the last point in the call stack at which code that you wrote was run. Check the arguments that your code passed into that first function that is not a function of yours.\nWhen running code that produces multiple errors, fix errors from the top down - fix the first error that is reported, because later errors are often caused by the initial error. It’s common to have a string of many errors, which looks daunting, caused by a single initial error.\nIs the bug reproducible - does it always happen in the same way at at the same point? It can help to restart Python and see if the bug persists - this can sometimes help in figuring out if there is a scoping issue and we are using a global variable that we did not mean to.\nIf you can’t figure out where the error occurs based on the error messages, a basic strategy is to build up code in pieces (or tear it back in pieces to a simpler version). This allows you to isolate where the error is occurring. You might use a binary search strategy. Figure out which half of the code the error occurs in. Then split the ‘bad’ half in half and figure out which half the error occurs in. Repeat until you’ve isolated the problem.\nIf you’ve written your code modularly with lots of functions, you can test individual functions. Often the error will be in what gets passed into and out of each function.\nAt the beginning of time (the 1970s?), the standard debugging strategy was to insert print statements in one’s code to see the value of a variable and thereby decipher what could be going wrong. We have better tools nowadays. But sometimes we still need to fall back to inserting print statements.\nPython is a scripting language, so you can usually run your code line by line to figure out what is happening. This can be a decent approach, particularly for simple code. However, when you are trying to find errors that occur within a series of many nested function calls or when the errors involve variable scoping (how Python looks for variables that are not local to a function), or in other complicated situations, using formal debugging tools can be much more effective. Finally, if the error occurs inside of functions provided by Python, rather than ones you write, it can be hard to run the code in those functions line by line.",
    "crumbs": [
      "Units",
      "Unit 4 (Good practices)"
    ]
  },
  {
    "objectID": "units/unit4-goodPractices.html#using-pdb",
    "href": "units/unit4-goodPractices.html#using-pdb",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Using pdb",
    "text": "Using pdb\nWe can activate the debugger in various ways:\n\nby inserting breakpoint() (or equivalently import pdb; pdb.set_trace()) inside a function or module at a location of interest (and then running the function or module)\nby using pdb.pm() after an error (i.e., an exception) has occurred to invoke the browser at the point the error occurred\n\nalternatively in IPython/Jupyter Notebook, run %debug (an IPython ‘magic’ command) and then run the code that results in the error\n\nby running a function under debugger control with pdb.run()\nby starting python with python -m pdb file.py and adding breakpoints\n\nIf you’re using IPython or a Jupyter Notebook, ipdb is a wrapper for pdb that has all the same commands, but provides some nice features for interactivity (such as tab completion and syntax highlighting).\n\nUsing breakpoint\nLet’s define a function that will run a stratified analysis, in this case fitting a regression to each of the strata (groups/clusters) in some data. Our function is in run_with_break.py, and it contains breakpoint at the point where we want to invoke the debugger.\nNow I can call the function and will be put into debugging mode just before the next line is called:\n\nimport run_with_break as run\nrun.fit(run.data, run.n_cats)\n\nWhen I run this, I see this:\n&gt;&gt;&gt; run.fit(data, n_cats)\n&gt; /accounts/vis/paciorek/teaching/243/fall-2024/units/run_with_break.py(10)fit()\n-&gt; sub = data[data['cats'] == i]\n(Pdb) \nThis indicates I am debugging at line 10 of run_with_break.py, which is the line that creates sub, but I haven’t yet created sub.\nI can type n to run that line and go to the next one:\n(Pdb) n\n&gt; /accounts/vis/paciorek/teaching/243/fall-2024/units/run_with_break.py(11)fit()\n-&gt; model = statsmodels.api.OLS(sub['y'], statsmodels.api.add_constant(sub['x']))\nat which point the debugger is about to execute line 11, which fits the regression.\nI can type c to continue until the next breakpoint:\n(Pdb) c\n&gt; /accounts/vis/paciorek/teaching/243/fall-2024/units/run_with_break.py(10)fit()\n-&gt; sub = data[data['cats'] == i]\nNow if I print i, I see that it has incremented to 1.\n(Pdb) p i\n1\nWe could keep hitting n or c until hitting the stratum where an error occurs, but that would be tedious.\nLet’s hit q to quit out of the debugger.\n(Pdb) q\n&gt;&gt;&gt;\nNext let’s see how we can enter debugging mode only at point an error occurs.\n\n\nPost-mortem debugging\nWe’ll use a version of the module without the breakpoint() command.\n\nimport pdb\nimport run_no_break as run \n\nrun.fit(run.data, run.n_cats)\npdb.pm()\n\nThat puts us into debugging mode at the point the error occurred:\n&gt; /usr/local/linux/miniforge-3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py(86)_wrapreduction()\n-&gt; return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n(Pdb)\nwhich turns out to be in some internal Python function that calls a reduce function, which is where the error occurs (presumably the debugger doesn’t enter this function because it calls compiled code):\n(Pdb) l\n 81                 if dtype is not None:\n 82                     return reduction(axis=axis, dtype=dtype, out=out, **passkwargs)\n 83                 else:\n 84                     return reduction(axis=axis, out=out, **passkwargs)\n 85     \n 86  -&gt;     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n 87     \n 88     \n 89     def _take_dispatcher(a, indices, axis=None, out=None, mode=None):\n 90         return (a, out)\n 91     \nWe can enter u multiple times (it’s only shown once below) to go up in the stack of function calls until we recognize code that we wrote:\n(Pdb) u\n&gt; /accounts/vis/paciorek/teaching/243/fall-2024/units/run_no_break.py(10)fit()\n-&gt; model = statsmodels.api.OLS(sub['y'], statsmodels.api.add_constant(sub['x']))\nNow let’s use p to print variable values to understand the problem:\n(Pdb)  p i\n29\n(Pdb) p sub\nEmpty DataFrame\nColumns: [y, x, cats]\nIndex: []\nAh, so in the 29th stratum there are no data!\n\n\npdb commands\nHere’s a list of useful pdb commands (some of which we saw above) that you can use once you’ve entered debugging mode.\n\nh or help: shows all the commands\nl or list: show the code around where the debugger is currently operating\nc or continue: continue running the code until the next breakpoint\np or print: print a variable or the result of evaluating an expression\nn or next: run the current line and go to the next line in the current function\ns or step: jump (step) into the function called in the current line (if it’s a Python function)\nr or run: exit out of the current function (e.g., if you accidentally stepped into a function) (but note this stops at breakpoints)\nunt or until: run until the next line (or unt &lt;number&gt; to run until reaching line number ); this is useful for letting a loop run until completion\nb or break: set a breakpoint\ntbreak: one-time breakpoint\nwhere: shows call stack\nu (or up) and d (or down): move up and down the call stack\nq quit out of the debugger\n&lt;return&gt;: runs the previous pdb command again\n\n\n\nInvoking pdb on a function or block of code\nWe can use pdb.run() to run a function under the debugger. We need to make sure to use s as the first pdb command in order to actually step into the function. From there, we can debug as normal as if we had set a breakpoint at the start of the function.\n\nimport run_with_break as run\nimport pdb\npdb.run(\"run.fit(run.data, run.n_cats)\")\n(Pdb) s\n\n\n\nInvoking pdb on a module\nWe can also invoke pdb when we start Python, executing a file (module). Here we’ve added fit(data, n_cats) at the end of run_no_break2.py so that we can have that run under the debugger.\n#| eval: false\npython -m pdb run_no_break2.py\n&gt; /accounts/vis/paciorek/teaching/243/fall-2024/units/run_no_break2.py(1)&lt;module&gt;()\n-&gt; import numpy as np\n(Pdb) \nLet’s set a breakpoint at the same place we did with breakpoint() but using a line number (this avoids having to actually modify our code):\n(Pdb) b 9\nBreakpoint 1 at /accounts/vis/paciorek/teaching/243/fall-2024/units/run_no_break.py:9\n\n(Pdb) c\n&gt; /accounts/vis/paciorek/teaching/243/fall-2024/units/run_no_break2.py(9)fit()\n-&gt; model = statsmodels.api.OLS(sub['y'], statsmodels.api.add_constant(sub['x']))\n\nSo we’ve broken at the same point where we manually added breakpoint() in run_with_break.py.\nOr we could have set a breakpoint at the start of the function:\n(Pdb) disable 1\nDisabled breakpoint 1 at /accounts/vis/paciorek/teaching/243/fall-2024/units/run_no_break2.py:9\n(Pdb) b fit\nBreakpoint 1 at /accounts/vis/paciorek/teaching/243/fall-2024/units/run_no_break.py:6",
    "crumbs": [
      "Units",
      "Unit 4 (Good practices)"
    ]
  },
  {
    "objectID": "units/unit4-goodPractices.html#some-common-causes-of-bugs",
    "href": "units/unit4-goodPractices.html#some-common-causes-of-bugs",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Some common causes of bugs",
    "text": "Some common causes of bugs\nSome of these are Python-specific, while others are common to a variety of languages.\n\nParenthesis mis-matches\n== vs. =\nComparing real numbers exactly using == is dangerous because numbers on a computer are only represented to limited numerical precision. For example,\n\n1/3 == 4*(4/12-3/12)\n\nFalse\n\n\nWe’ll discuss in detail in Unit 8.\nYou expect a single value but execution of the code gives an array\nSilent type conversion when you don’t want it, or lack of coercion where you’re expecting it\nUsing the wrong function or variable name\nGiving unnamed arguments to a function in the wrong order\nForgetting to define a variable in the environment of a function and having Python, via lexical scoping, get that variable as a global variable from one of the enclosing scope. At best the types are not compatible and you get an error; at worst, you use a garbage value and the bug is hard to trace. In some cases your code may work fine when you develop the code (if the variable exists in the enclosing environment), but then may not work when you restart Python if the variable no longer exists or is different.\nPython (usually helpfully) drops matrix and array dimensions that are extraneous. This can sometimes confuse later code that expects an object of a certain dimension. More on this below.",
    "crumbs": [
      "Units",
      "Unit 4 (Good practices)"
    ]
  },
  {
    "objectID": "units/unit4-goodPractices.html#tips-for-avoiding-bugs-and-catching-errors",
    "href": "units/unit4-goodPractices.html#tips-for-avoiding-bugs-and-catching-errors",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Tips for avoiding bugs and catching errors",
    "text": "Tips for avoiding bugs and catching errors\n\nPractice defensive programming\nWhen writing functions, and software more generally, you’ll want to warn the user or stop execution when there is an error and exit gracefully, giving the user some idea of what happened. Here are some things to consider:\n\ncheck function inputs and warn users if the code will do something they might not expect or makes particular choices;\ncheck inputs to if and the ranges in for loops;\nprovide reasonable default arguments;\ndocument the range of valid inputs;\ncheck that the output produced is valid; and\nstop execution based on assertions, try or raise with an informative error message.\n\nHere’s an example of building a robust square root function:\n\nimport warnings\n\ndef mysqrt(x):\n    if isinstance(x, str):\n        raise TypeError(f\"What is the square root of '{x}'?\")\n    if isinstance(x, (float, int)):\n        if x &lt; 0:\n            warnings.warn(\"Input value is negative.\", UserWarning)\n            return float('nan')   # Avoid complex number result.\n        else:\n            return x**0.5\n    else:\n        raise ValueError(f\"Cannot take the square root of {x}\")\n\n\nmysqrt(3.1)\nmysqrt(-3)\n\n/tmp/ipykernel_3985830/18181922.py:8: UserWarning:\n\nInput value is negative.\n\n\n\nnan\n\n\n\nmysqrt('hat')\n\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"&lt;stdin&gt;\", line 3, in mysqrt\nTypeError: What is the square root of 'hat'?\n\n\nCatch run-time errors with try/except statements\nAlso, sometimes a function you call will fail, but you want to continue execution. For example, consider the stratified analysis show previously in which you take subsets of your data based on some categorical variable and fit a statistical model for each value of the categorical variable. If some of the subsets have no or very few observations, the statistical model fitting might fail. To do this, you might be using a for loop or apply. You want your code to continue and fit the model for the rest of the cases even if one (or more) of the cases cannot be fit. You can wrap the function call that may fail within the try statement and then your code won’t stop, even when an error occurs. Here’s a toy example.\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport statsmodels.api\n\nnp.random.seed(2)\nn_cats = 30\nn = 80\ny = np.random.normal(size=n)\nx = np.random.normal(size=n)\ncats = [np.random.randint(0, n_cats-1) for _ in range(n)]\ndata = pd.DataFrame({'y': y, 'x': x, 'cats': cats})\n\nparams = np.full((n_cats, 2), np.nan)\nfor i in range(n_cats):\n    sub = data[data['cats'] == i]\n    try:\n        model = statsmodels.api.OLS(sub['y'], statsmodels.api.add_constant(sub['x']))\n        fit = model.fit()\n        params[i, :] = fit.params.values\n    except Exception as error:\n        print(f\"Regression cannot be fit for stratum {i}.\")\n\nprint(params)\n\nRegression cannot be fit for stratum 7.\nRegression cannot be fit for stratum 20.\nRegression cannot be fit for stratum 24.\nRegression cannot be fit for stratum 29.\n[[ 5.52897442e-01  2.61511154e-01]\n [ 5.72564369e-01  4.37210543e-02]\n [-9.91086764e-01  2.84116572e-01]\n [-6.50606465e-01  4.26310060e-01]\n [-2.59058826e+00 -2.59058826e+00]\n [ 8.59455139e-01 -4.64514288e+00]\n [ 3.82737032e-06  3.82737032e-06]\n [            nan             nan]\n [-5.55478634e-01 -1.17864561e-01]\n [-9.11601460e-02 -5.91519525e-01]\n [-7.30270153e-01 -1.99976841e-01]\n [-1.14495705e-01 -3.06421213e-02]\n [ 4.01648095e-01  9.30890661e-01]\n [ 7.88388728e-01 -1.45835443e+00]\n [ 4.08462508e+01  6.89262864e+01]\n [ 2.95467536e-01  8.80528901e-01]\n [ 1.04592517e+00  4.55379445e+00]\n [ 6.99549010e-01 -5.17503241e-01]\n [-1.75642254e+00 -8.07798224e-01]\n [-4.49033150e-02  3.53455362e-01]\n [            nan             nan]\n [ 2.63097970e-01  2.63097970e-01]\n [ 1.13328314e+00 -1.39985074e-01]\n [ 1.17996663e+00  3.68770563e-01]\n [            nan             nan]\n [-3.85101497e-03 -3.85101497e-03]\n [-8.04536124e-01 -5.19470059e-01]\n [-5.19200779e-01 -1.39952387e-01]\n [-9.16593858e-01 -2.67613324e-01]\n [            nan             nan]]\n\n\nThe stratum with id 7 had no observations, so that call to do the regression failed, but the loop continued because we ‘caught’ the error with try. In this example, we could have checked the sample size for the subset before doing the regression, but in other contexts, we may not have an easy way to check in advance whether the function call will fail.\n\n\nMaintain dimensionality\nPython (usually helpfully) drops array dimensions that are extraneous. This can sometimes confuse later code that expects an object of a certain dimension. Here’s a work-around:\n\nimport numpy as np\nmat = np.array([[1, 2], [3, 4]])\nnp.sum(mat, axis=0)         # This sums columns, as desired\n\nrow_subset = 1\nmat2 = mat[row_subset, :]\nnp.sum(mat2, axis=0)        # This sums the elements, not the columns.\n\nif len(mat2.shape) != 2:    # Fix dimensionality.\n    mat2 = mat2.reshape(1, -1)\n\n\nnp.sum(mat2, axis=0)   \n\narray([3, 4])\n\n\nIn this simple case it’s obvious that a dimension will be dropped, but in more complicated settings, this can easily occur for some inputs without the coder realizing that it may happen. Not dropping dimensions is much easier than putting checks in to see if dimensions have been dropped and having the code behave differently depending on the dimensionality.\n\n\nFind and avoid global variables\nIn general, using global variables (variables that are not created or passed into a function) results in code that is not robust. Results will change if you or a user modifies that global variable, usually without realizing/remembering that a function depends on it.\nOne ad hoc strategy is to remove objects you don’t need from Python’s global scope, to avoid accidentally using values from an old object via Python’s scoping rules. You can also run your function in a fresh session to see if it’s unable to find variables.\n\ndel x   # Mimic having a fresh sesson (knowing in this case `x` is global).\n\ndef f(z):\n    y = 3\n    print(x + y + z)\n\ntry:\n    f(2)\nexcept Exception as error:\n    print(error)\n\nname 'x' is not defined\n\n\n\n\nMiscellaneous tips\n\nUse core Python functionality and algorithms already coded. Figure out if a functionality already exists in (or can be adapted from) an Python package (or potentially in a C/Fortran library/package): code that is part of standard mathematical/numerical packages will probably be more efficient and bug-free than anything you would write.\nCode in a modular fashion, making good use of functions, so that you don’t need to debug the same code multiple times. Smaller functions are easier to debug, easier to understand, and can be combined in a modular fashion (like the UNIX utilities).\nWrite code for clarity and accuracy first; then worry about efficiency. Write an initial version of the code in the simplest way, without trying to be efficient (e.g., you might use for loops even if you’re coding in Python); then make a second version that employs efficiency tricks and check that both produce the same output.\nPlan out your code in advance, including all special cases/possibilities.\nWrite tests for your code early in the process.\nBuild up code in pieces, testing along the way. Make big changes in small steps, sequentially checking to see if the code has broken on test case(s).\nBe careful that the conditions of if statements and while loops and the sequences of for loops are robust.\nDon’t hard code numbers - use variables (e.g., number of iterations, parameter values in simulations), even if you don’t expect to change the value, as this makes the code more readable and reduces bugs when you use the same number multiple times; e.g. speed_of_light = 3e8 or n_its = 1000.\n\nIn a future Lab, we’ll go over debugging in detail.\n\n\nCode review\nIt’s a good idea to have others review your code, ideally during the development process.\nThis article provides guidance on good practices for code review.",
    "crumbs": [
      "Units",
      "Unit 4 (Good practices)"
    ]
  },
  {
    "objectID": "units/unit4-goodPractices.html#some-basic-strategies-for-reproducible-analyses",
    "href": "units/unit4-goodPractices.html#some-basic-strategies-for-reproducible-analyses",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Some basic strategies for reproducible analyses",
    "text": "Some basic strategies for reproducible analyses\n\nHave a directory for each project with subdirectories with meaningful and standardized names: e.g., code, data, paper. The Journal of the American Statistical Association (JASA) has a template GitHub repository with some suggestions.\nHave a file of code for pre-processing, one or more for analysis, and one for figure/table preparation.\n\nThe pre-processing may involve time-consuming steps. Save the output of the pre-processing as data file(s) that can be read in to the analysis script.\nYou may want to name your files something like this, so there is an obvious ordering: “1-prep.py”, “2-analysis.py”, “3-figs.py”.\nHave the code file for the figures produce the exact manuscript/report figures, operating on a file (e.g., a pickle file) that contains all the objects necessary to run the figure-producing code; the code producing the pickle file should be in your analysis code file (or somewhere else sensible).\nAlternatively, use Quarto or Jupyter notebooks for your document preparation.\n\nKeep a document describing your running analysis with dates in a text file (i.e., a lab book).\nNote where data were obtained (and when, which can be helpful when publishing) and pre-processing steps in the lab book. Have data version numbers with a file describing the changes and dates (or in lab book). If possible, have all changes to data represented as code that processes the data relative to a fixed baseline dataset.\nNote what code files do what in the lab book.\nKeep track of the details of the system and software you are running your code under, e.g., operating system version, software (e.g., Python, R) versions, Python or R package versions, etc.\n\npip list and conda list will show you version numbers for installed packages.\npip freeze &gt; requirements.txt and conda env export &gt; env.yml will create a recipe file for your environment.\npip install -r requirements.txt and conda env create -f env.yml will build that environment on your machine.",
    "crumbs": [
      "Units",
      "Unit 4 (Good practices)"
    ]
  },
  {
    "objectID": "units/unit4-goodPractices.html#formal-tools",
    "href": "units/unit4-goodPractices.html#formal-tools",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Formal tools",
    "text": "Formal tools\n\nIn some cases you may be able to carry out your complete workflow in a Quarto document or in a Jupyter notebook.\nYou might consider workflow/pipeline management software such as Drake or other tools discussed in the CRAN Reproducible Research Task View. Alternatively, one can use the make tool, which is generally used for compiling code, as a tool for reproducible research: if interested, see the tutorial on Using make for workflows or this Journal of Statistical Software article for more details.\nYou might organize your workflow as a Python or R package as described (for the R case) in this article.\nPackage management:\n\nPython: You can manage the versions of Python packages (and dependent packages) used in your project using Conda environments (or virtualenvs).\nR: You can manage the versions of R packages (and dependent packages) used in your project using package management packages such as renv and packrat. Unfortunately, the useful checkpoint package relies on snapshots of CRAN that are not available after January 2023.\n\nIf your project uses multiple pieces of software (e.g., not just Python or R), you can set up a reproducible environment using containers, of which Docker containers are the best known. These provide something that is like a lightweight virtual machine in which you can install exactly the software (and versions) you want and then share with others. Docker container images are a key building block of various tools such as GitHub Actions and the Binder project. Alternatively (and often much easier) Conda is a general package manager that can install lots of non-Python packages and can also be used in many circumstances.",
    "crumbs": [
      "Units",
      "Unit 4 (Good practices)"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Statistics 243 is an introduction to statistical computing, taught using Python. The course will cover both programming concepts and statistical computing concepts. Programming concepts will include data and text manipulation, regular expressions, data structures, functions and variable scope, memory use, efficiency, debugging, testing, and parallel processing. Statistical computing topics will include working with large datasets, numerical linear algebra, computer arithmetic/precision, simulation studies and Monte Carlo methods, and numerical optimization. A goal is that coverage of these topics complement the models/methods discussed in the rest of the statistics/biostatistics graduate curriculum. We will also cover the basics of UNIX/Linux, in particular shell scripting and operating on remote servers, as well as a bit of R.\n\n\n\nWhile the course is taught using Python and you will learn a lot about using Python at an advanced level, this is not a course about learning Python. Rather the focus of the course is computing for statistics and data science more generally, using Python to illustrate the concepts.\nThis is not a course that will cover specific statistical/machine learning/data analysis methods.\n\n\n\n\nInformal prerequisites: If you are not a statistics or biostatistics graduate student, please chat with me if you’re not sure if this course makes sense for you. A background in calculus, linear algebra, probability and statistics is expected, as well as a basic ability to operate on a computer (but I do not assume familiarity with the UNIX-style command line/terminal/shell). Furthermore, I’m expecting you will know the basics of Python, at the level of the Python material in our computing skills workshop offered Aug. 20-21, 2024. If you don’t have that background you’ll need to spend time in the initial couple weeks getting up to speed. The workshop materials are a good resource.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "Statistics 243 is an introduction to statistical computing, taught using Python. The course will cover both programming concepts and statistical computing concepts. Programming concepts will include data and text manipulation, regular expressions, data structures, functions and variable scope, memory use, efficiency, debugging, testing, and parallel processing. Statistical computing topics will include working with large datasets, numerical linear algebra, computer arithmetic/precision, simulation studies and Monte Carlo methods, and numerical optimization. A goal is that coverage of these topics complement the models/methods discussed in the rest of the statistics/biostatistics graduate curriculum. We will also cover the basics of UNIX/Linux, in particular shell scripting and operating on remote servers, as well as a bit of R.\n\n\n\nWhile the course is taught using Python and you will learn a lot about using Python at an advanced level, this is not a course about learning Python. Rather the focus of the course is computing for statistics and data science more generally, using Python to illustrate the concepts.\nThis is not a course that will cover specific statistical/machine learning/data analysis methods.\n\n\n\n\nInformal prerequisites: If you are not a statistics or biostatistics graduate student, please chat with me if you’re not sure if this course makes sense for you. A background in calculus, linear algebra, probability and statistics is expected, as well as a basic ability to operate on a computer (but I do not assume familiarity with the UNIX-style command line/terminal/shell). Furthermore, I’m expecting you will know the basics of Python, at the level of the Python material in our computing skills workshop offered Aug. 20-21, 2024. If you don’t have that background you’ll need to spend time in the initial couple weeks getting up to speed. The workshop materials are a good resource.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#objectives-of-the-course",
    "href": "syllabus.html#objectives-of-the-course",
    "title": "Syllabus",
    "section": "Objectives of the course",
    "text": "Objectives of the course\nThe goals of the course are that, by the end of the course, students be able to:\n\noperate effectively in a UNIX environment and on remote servers and compute clusters;\nhave a solid understanding of general programming concepts and principles, and be able to program effectively (including having an advanced knowledge of Python functionality);\nbe familiar with concepts and tools for reproducible research and good scientific computing practices; and\nunderstand in depth and be able to make use of principles of numerical linear algebra, optimization, and simulation for statistics- and data science-related analyses and research.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#topics-in-order-with-rough-timing",
    "href": "syllabus.html#topics-in-order-with-rough-timing",
    "title": "Syllabus",
    "section": "Topics (in order with rough timing)",
    "text": "Topics (in order with rough timing)\nThe ‘days’ here are (roughly) class sessions, as general guidance.\n\nIntroduction to UNIX, operating on a compute server (1 day)\nData formats, data access, webscraping, data structures (2 days)\nDebugging, good programming practices, reproducible research (1 day)\nThe bash shell and shell scripting, version control (3 days)\nProgramming concepts and advanced Python programming: text processing and regular expressions, object-oriented programming, functions and variable scope, memory use, efficient programming (9 days)\nParallel processing (2 days)\nWorking with databases, hashing, and big data (3 days)\nComputer arithmetic/representation of numbers on a computer (3 days)\nSimulation studies and Monte Carlo (2 days)\nNumerical linear algebra (5 days)\nOptimization (5 days)\nGraphics (1 day)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#personnel",
    "href": "syllabus.html#personnel",
    "title": "Syllabus",
    "section": "Personnel",
    "text": "Personnel\n\nInstructor:\n\nChris Paciorek (paciorek@stat.berkeley.edu)\n\nGSI\n\nJoão Vitor Romano (jv.romano@berkeley.edu)\n\nOffice hours can be found here.\nWhen to see us about an assignment: We’re here to help, including providing guidance on assignments. You don’t want to be futilely spinning your wheels for a long time getting nowhere. That said, before coming to see us about a difficulty, you should try something a few different ways and define/summarize for yourself what is going wrong or where you are getting stuck.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-websites-github-piazza-gradescope-and-bcourses",
    "href": "syllabus.html#course-websites-github-piazza-gradescope-and-bcourses",
    "title": "Syllabus",
    "section": "Course websites: GitHub, Ed Discussion, Gradescope, and bCourses",
    "text": "Course websites: GitHub, Ed Discussion, Gradescope, and bCourses\nKey websites for the course are:\n\nThis course website, which is hosted on GitHub pages, and the GitHub repository containing the source materials: https://github.com/berkeley-stat243/fall-2024\nSCF tutorials for additional content: https://statistics.berkeley.edu/computing/training/tutorials\nEd Discussion site for discussions/Q&A: https://edstem.org/us/courses/62469/discussion\nbCourses site for course capture recordings (see Media Gallery) and possibly some other materials: https://bcourses.berkeley.edu/courses/1537551\nGradescope for assignments (also linked from bCourses): https://www.gradescope.com/courses/825461\n\nAll course materials will be posted on here on the website (and on GitHub) except for video content, which will be in bCourses.\n\nCourse discussion\nWe will use the course Ed Discussion site for communication (announcements, questions, and discussion). You should ask questions about class material and problem sets through the site. Please use this site for your questions so that either or I can respond and so that everyone can benefit from the discussion. I suggest you to modify your settings on Ed Discussion so you are informed by email of postings. In particular you are responsible for keeping track of all course announcements, which we’ll make on the Discussion forum. I strongly encourage you to respond to or comment on each other’s questions as well (this will help your class participation grade), although of course you should not provide a solution to a problem set problem. If you have a specific administrative question you need to direct just to me, it’s fine to email me directly or post privately on the Discussion site. But if you simply want to privately ask a question about content, then just come to an office hour or see me after class or João during/after section.\nIf you’re enrolled in the class you should be a member of the group and be able to access it. If you’re auditing or not yet enrolled and would like access, make sure to fill out the course survey and I will add you. In addition, we will use Gradescope for viewing grades.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-material",
    "href": "syllabus.html#course-material",
    "title": "Syllabus",
    "section": "Course material",
    "text": "Course material\n\nPrimary materials: Course notes on course webpage/GitHub, SCF tutorials, and potentially pre-recorded videos on bCourses.\nBack-up textbooks (generally available via UC Library via links below):\n\nFor bash: Newham, Cameron and Rosenblatt, Bill. Learning the bash Shell available electronically through UC Library\nFor Quarto: The Quarto reference guide\nFor statistical computing topics:\n\nGentle, James. Computational Statistics\nGentle, James. Matrix Algebra or Numerical Linear Algebra with Applications in Statistics\n\nOther resources with more detail on particular aspects of statistical computing concepts:\n\nLange, Kenneth; Numerical Analysis for Statisticians, 2nd ed. First edition available through UC library\nMonahan, John; Numerical Methods of Statistics",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#section",
    "href": "syllabus.html#section",
    "title": "Syllabus",
    "section": "Section",
    "text": "Section\nThe GSI will lead a two-hour discussion section each week (there are two sections). By and large, these will only last for about one hour of actual content, but the second hour may be used as an office hour with the GSI or for troubleshooting software during the early weeks. The discussion sections will vary in format and topic, but material will include demonstrations on various topics (version control, debugging, testing, etc.), group work on these topics, discussion of relevant papers, and discussion of problem set solutions. The first section (12-2 pm) generally has more demand, so to avoid having too many people in the room, you should go to your assigned section unless you talk to me first.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#computing-resources",
    "href": "syllabus.html#computing-resources",
    "title": "Syllabus",
    "section": "Computing Resources",
    "text": "Computing Resources\nMost work for the course can be done on your laptop. Later in the course we’ll also use the Statistics Department Linux cluster. You can also use the SCF JupyterHub or the campus DataHub to access a bash shell or run an IPython notebook. (The campus DataHub is limited in terms of number of CPU cores and memory so won’t be suitable for more computationally-intensive work later in the semester.)\nThe software needed for the course is as follows:\n\nAccess to the UNIX command line (bash shell)\nGit\nPython (the Miniforge installation of Conda is recommended but by no means required)\nQuarto\n\nSee the “how tos” in the left sidebar for tips on software installation and access to a UNIX shell, which you’ll need to be able to do by the second week of class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#class-time",
    "href": "syllabus.html#class-time",
    "title": "Syllabus",
    "section": "Class time",
    "text": "Class time\nMy goal is to have classes be an interactive environment. This is both more interesting for all of us and more effective in learning the material. I encourage you to ask questions and will pose questions to the class to think about, respond to via online polling or Google forms, and discuss. To increase time for discussion and assimilation of the material in class, before some classes I may ask that you read material or work through tutorials in advance of class. Occasionally, I will ask you to submit answers to questions in advance of class as well.\nPlease do not use phones during class and limit laptop use to the material being covered.\nStudent backgrounds with computing will vary. For those of you with limited background on a topic, I encourage you to ask questions during class so I know what you find confusing. For those of you with extensive background on a topic (there will invariably be some topics where one of you will know more about it than I do or have more real-world experience), I encourage you to pitch in with your perspective. In general, there are many ways to do things on a computer, particularly in a UNIX environment and in Python, so it will help everyone (including me) if we hear multiple perspectives/ideas.\nFinally, class recordings for review or to make up for absence will be available through the bCourses Media Gallery, available on the Media Gallery tab on the bCourses page for the class.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-requirements-and-grading",
    "href": "syllabus.html#course-requirements-and-grading",
    "title": "Syllabus",
    "section": "Course requirements and grading",
    "text": "Course requirements and grading\n\nScheduling Conflicts\nCampus asks that I include this information about conflicts: Please notify me in writing by the second week of the term about any known or potential extracurricular conflicts (such as religious observances, graduate or medical school interviews, or team activities). I will try my best to help you with making accommodations, but I cannot promise them in all cases. In the event there is no mutually-workable solution, you may be dropped from the class.\nThe main conflict that would be a problem would be the quizzes, whose dates I will determine in late August / early September.\nQuizzes are in-person. There is no remote option, and the only make-up accommodations I will make are for illness or serious personal issues. Do not schedule any travel that may conflict with a quiz.\n\n\nCourse grades\nThe grade for this course is primarily based on assignments due every 1-2 weeks, two quizzes (likely in early-mid October and mid-late November), and a final group project. I will also provide extra credit questions on some problem sets. There is no final exam.\n\n50% of the grade is based on the problem sets,\n25% on the quizzes,\n15% on the project, and\n10% on your class participation:\n\nyour responses to the in-class (and occasionally in-advance-of-class) Google forms questions,\ncompletion of occasional non-problem set assignments (including assignments in lab section), and\nsubstantive contribution to discussions on Ed (i.e., responding to your classmates’ questions and asking thoughtful questions about course material).\n\n\nGrades will generally be As and Bs. An A involves doing all the work, getting full credit on most of the problem sets, doing well on the quizzes, and doing a thorough job on the final project.\n\n\nProblem sets\nWe will be less willing to help you if you come to our office hours or post a question online at the last minute. Working with computers can be unpredictable, so give yourself plenty of time for the assignments.\nThere are several rules for submitting your assignments.\n\nYou should prepare your assignments using Quarto.\nProblem set submission consists of both of the following:\n\nA PDF submitted electronically through Gradescope, by the start of class (10 am) on the due date, and\nAn electronic copy of the PDF, code files, and Quarto document pushed to your class GitHub repository, following the instructions to be provided by the GSI.\n\nOn-time submission will be determined based on the time stamp of when the PDF is submitted to Gradescope.\nAnswers should consist of textual response or mathematical expressions as appropriate, with key chunks of code embedded within the document. Function definitions would generally be placed in a separate .py code file. The function definitions and any extensive additional code should be provided as an appendix. Before diving into the code for a problem, you should say what the goal of the code is and your strategy for solving the problem. Raw code without explanation is not an appropriate solution. Please see our qualitative grading rubric for guidance. In general the rubric is meant to reinforce good coding practices and high-quality scientific communication.\nAny mathematical derivations may be done by hand and scanned with your phone if you prefer that to writing up LaTeX equations.\nYou must include a “Collaboration” section in which you indicate any other students you worked with. If you did not collaborate with anyone else, simply state that.\n\nNote: Quarto Markdown is an extension to the Markdown markup language that allows one to embed Python and R code within an HTML document. Please see the SCF dynamics document tutorial; there will be additional information in the first section and on the first problem set.\n\n\nSubmitting assignments\nIn the first section (September 1), we’ll discuss how to submit your problem sets both on Gradescope and via your class GitHub repository, located at https://github.berkeley.edu/&lt;your_calnet_username&gt;.\n\n\nProblem set grading\nThe grading scheme for problem sets is as follows. Each problem set will receive a numeric score for (1) presentation and explanation of results, (2) technical accuracy of code or mathematical derivation, and (3) code quality (style, structure, reproducibility, and creativity). For each of these three components, the possible scores are:\n\n0 = no credit,\n1 = partial credit (you did some of the problems but not all),\n2 = satisfactory (you tried everything but there were pieces of what you did that didn’t completely/correctly solve or present/explain one or more problems), and\n3 = full credit.\n\nAgain, the qualitative grading rubric provides guidance on what we want to see for full credit.\nFor components #1 and #3, many of you will get a score of 2 for some problem sets as you develop good presentation and coding practices. You can still get an A in the class despite this.\nYour total score for the PS is a weighted sum of the scores for the three components. If you turn in a PS late, I’ll bump you down by two points (out of the available). If you turn it in really late (e.g., after we start grading them), I will bump you down by four points. No credit after solutions are distributed.\n\n\nFinal project\nThe final project will be a joint coding project in groups of 3-4. I’ll assign an overall task, and you’ll be responsible for dividing up the work, coding, debugging, testing, and documentation. You’ll need to use the Git version control system for working in your group.\n\n\nRules for working together and the campus honor code\nI encourage you to work together and help each other out. However, the problem set solutions you submit must be your own. What do I mean by that?\n\nYou should first try to figure out a given problem on your own. After that, if you’re stuck or want to explore alternative approaches or check what you’ve done, feel free to consult with your fellow students and with the GSI and me.\nWhat does “consult with a fellow student mean”? You can discuss a problem with another student, brainstorm approaches, and share code syntax (generally not more than one line) on how to do small individual coding tasks within a problem.\n\nYou should not ask another student for complete code or solutions, or look at their code/solution.\nYou should not share complete code or solutions with another student or on Ed Discussion.\n\nYou may use ChatGPT (or similar chatbots) for help with small sections of a problem (e.g., how to do some specific Python or bash task). You should not use ChatGPT to try to answer an entire question. You should carefully verify that the result is correct.\nYou must provide attribution for ideas obtained elsewhere, including other students and ChatGPT or similar chatbots.\n\nIf you got a specific idea for how to do part of a problem from a fellow student (or some other resource, including ChatGPT), you should note that in your solution in the appropriate place (for specific syntax ideas, note this in a code comment), just as you would cite a book or URL.\nIn addition, you MUST list in a Collaboration section on your problem set solution any fellow students who you worked/consulted with.\nYou do not need to cite any Ed Discussion posts nor any discussions with Chris or João.\n\nUltimately, your solution to a problem set (writeup and code) must be your own, and you’ll hear from me if either look too similar to someone else’s.\n\nPlease see the last section of this document for more information on the Campus Honor Code, which I expect you to follow.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#feedback",
    "href": "syllabus.html#feedback",
    "title": "Syllabus",
    "section": "Feedback",
    "text": "Feedback\nI welcome comments and suggestions and concerns. Particularly good suggestions will count towards your class participation grade.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#accomodations-for-students-with-disabilities",
    "href": "syllabus.html#accomodations-for-students-with-disabilities",
    "title": "Syllabus",
    "section": "Accomodations for Students with Disabilities",
    "text": "Accomodations for Students with Disabilities\nPlease see me as soon as possible if you need particular accommodations, and we will work out the necessary arrangements.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#campus-honor-code",
    "href": "syllabus.html#campus-honor-code",
    "title": "Syllabus",
    "section": "Campus Honor Code",
    "text": "Campus Honor Code\nThe following is the Campus Honor Code. With regard to collaboration and independence, please see my rules regarding problem sets above – Chris.\nThe student community at UC Berkeley has adopted the following Honor Code: “As a member of the UC Berkeley community, I act with honesty, integrity, and respect for others.” The hope and expectation is that you will adhere to this code.\nCollaboration and Independence: Reviewing lecture and reading materials and studying for exams can be enjoyable and enriching things to do with fellow students. This is recommended. However, unless otherwise instructed, homework assignments are to be completed independently and materials submitted as homework should be the result of one’s own independent work.\nCheating: A good lifetime strategy is always to act in such a way that no one would ever imagine that you would even consider cheating. Anyone caught cheating on a quiz or exam in this course will receive a failing grade in the course and will also be reported to the University Center for Student Conduct. In order to guarantee that you are not suspected of cheating, please keep your eyes on your own materials and do not converse with others during the quizzes and exams.\nPlagiarism: To copy text or ideas from another source without appropriate reference is plagiarism and will result in a failing grade for your assignment and usually further disciplinary action. For additional information on plagiarism and how to avoid it, see, for example: http://gsi.berkeley.edu/teachingguide/misconduct/prevent-plag.html\nAcademic Integrity and Ethics: Cheating on exams and plagiarism are two common examples of dishonest, unethical behavior. Honesty and integrity are of great importance in all facets of life. They help to build a sense of self-confidence, and are key to building trust within relationships, whether personal or professional. There is no tolerance for dishonesty in the academic world, for it undermines what we are dedicated to doing – furthering knowledge for the benefit of humanity.\nYour experience as a student at UC Berkeley is hopefully fueled by passion for learning and replete with fulfilling activities. And we also appreciate that being a student may be stressful. There may be times when there is temptation to engage in some kind of cheating in order to improve a grade or otherwise advance your career. This could be as blatant as having someone else sit for you in an exam, or submitting a written assignment that has been copied from another source. And it could be as subtle as glancing at a fellow student’s exam when you are unsure of an answer to a question and are looking for some confirmation. One might do any of these things and potentially not get caught. However, if you cheat, no matter how much you may have learned in this class, you have failed to learn perhaps the most important lesson of all.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "labs/lab2-testing.html",
    "href": "labs/lab2-testing.html",
    "title": "Lab 2: Exceptions and Testing",
    "section": "",
    "text": "Today we will spend some time getting familiar with some of the programming tools that can help make your code more robust and resilient to errors and boundary conditions - tools like unit tests and exceptions (and next week we will spend some time on debugging misbehaving code).\nTesting is what you do when you finish implementing a piece of code and want to try it out to see if it works. Running your code manually and seeing if it works is a workable strategy for simple one-time scripts that do simple tasks, but there are situations (like writing a function that others will repeatedly use, or like running the same piece of code on hundreds of files or URLs) where it is prudent to test your code ahead of its actual use or deployment. In today’s lab, we are going to use pytest to test for correct behavior and verify that exceptions are raised in the right situations.",
    "crumbs": [
      "Labs",
      "Lab 2 (Exceptions/Testing)"
    ]
  },
  {
    "objectID": "labs/lab2-testing.html#lab-exercise",
    "href": "labs/lab2-testing.html#lab-exercise",
    "title": "Lab 2: Exceptions and Testing",
    "section": "Lab Exercise",
    "text": "Lab Exercise\n1- Write a function in Python that finds a number in a string using regular expressions. The number can be positive or negative, integer-valued, or real, and numbers less than one can also be of the form 0.96 or .96. It can be assumed that no numbers appear in scientific notation. Refer to Using regex in Python for a quick overview of Python’s re package.\n2- Write an interface for that function (a function name and arguments), but do not implement the function yet (you can have it return an None, or an empty string for now). We will do this in a good old fashioned .py file (not a notebook or a quarto file).\n3- Build a test suite using the pytest package to test that your function works as intended. Add at least 8 test cases with justification for each. Try to cover the main use cases, and as many potential corner cases or boundary conditions as possible.\n4- Now run the test suite. It should fail for all your tests (unless one of them was passing an empty string).\n5- Implement the function. You can do this at one go, or case by case. As you implement a case, you can rerun the test suite and see some of the tests relevant to that cases stopping to fail. When all the tests pass, you are done. This is called test-driven development.\n6- If some cases are still failing, that’s alright, we can use that failing code next week for demonstrating debugging functionality.\n7- Make sure you raise an exception to trap invalid input types.\n8- Add a test to check that an exception is properly raised when the function is given invalid input types.",
    "crumbs": [
      "Labs",
      "Lab 2 (Exceptions/Testing)"
    ]
  },
  {
    "objectID": "labs/lab2-testing.html#acknowledgements",
    "href": "labs/lab2-testing.html#acknowledgements",
    "title": "Lab 2: Exceptions and Testing",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis lab was originally authored by Ahmed Eldeeb and adapted for the Fall 2024 semester.",
    "crumbs": [
      "Labs",
      "Lab 2 (Exceptions/Testing)"
    ]
  },
  {
    "objectID": "labs/lab0-setup.html",
    "href": "labs/lab0-setup.html",
    "title": "Lab 0: Setup",
    "section": "",
    "text": "You will need to set up (or make sure you have access to) the following:\n\nUnix shell;\nGit;\nQuarto;\nPython;\nA text editor or IDE of your choice.\n\nAfter making sure you have access to all these 5 tools, it may be a good idea to go through the tutorial on unix and (possibly) unix commands. You can do this on your own or in the Lab section on Friday 08/30.\nAttending Lab 0 is optional. If you successfully set up your environment to have all the listed tools, you don’t need to attend. You are welcome to attend to ask for help with the setup or to help your classmates with setting up their environments.\nYou are also welcome to come and ask for help on using any of these 5 tools/systems (especially shell and unix commands this week), but priority will be given to environment setup questions.\nReach out to us on Ed with any unresolved problems or if you discover something that needs to be changed with our howtos and instructions.",
    "crumbs": [
      "Labs",
      "Lab 0 (Optional setup)"
    ]
  },
  {
    "objectID": "labs/lab0-setup.html#setting-up-your-environment-083024",
    "href": "labs/lab0-setup.html#setting-up-your-environment-083024",
    "title": "Lab 0: Setup",
    "section": "",
    "text": "You will need to set up (or make sure you have access to) the following:\n\nUnix shell;\nGit;\nQuarto;\nPython;\nA text editor or IDE of your choice.\n\nAfter making sure you have access to all these 5 tools, it may be a good idea to go through the tutorial on unix and (possibly) unix commands. You can do this on your own or in the Lab section on Friday 08/30.\nAttending Lab 0 is optional. If you successfully set up your environment to have all the listed tools, you don’t need to attend. You are welcome to attend to ask for help with the setup or to help your classmates with setting up their environments.\nYou are also welcome to come and ask for help on using any of these 5 tools/systems (especially shell and unix commands this week), but priority will be given to environment setup questions.\nReach out to us on Ed with any unresolved problems or if you discover something that needs to be changed with our howtos and instructions.",
    "crumbs": [
      "Labs",
      "Lab 0 (Optional setup)"
    ]
  },
  {
    "objectID": "labs/lab0-setup.html#acknowledgements",
    "href": "labs/lab0-setup.html#acknowledgements",
    "title": "Lab 0: Setup",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis lab was originally authored by Ahmed Eldeeb and adapted for the Fall 2024 semester.",
    "crumbs": [
      "Labs",
      "Lab 0 (Optional setup)"
    ]
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar",
    "section": "",
    "text": "We can just embed the iframe html:"
  },
  {
    "objectID": "ps/ps2.html",
    "href": "ps/ps2.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "This covers material in Units 3 and 4.\nIt’s due at 10 am (Pacific) on September 20, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 for formatting and attribution requirements.\nNote that using chunks of bash code in Qmd may be troublesome.\n\nYou will need to add engine: knitr to the YAML preface of your qmd document. The default jupyter engine won’t run both bash and Python chunks in the same document because the Jupyter notebooks are associated with a single ‘kernel’ (i.e., a single language for the code chunks).\nFor the knitr engine, you’ll need to have R installed on your computer, including the knitr package. Quarto will then process the code chunks through knitr (which will use R’s reticulate package to handle Python chunks).\nIf you have trouble on your own computer, you can always render your solution for this problem set on an SCF machine (we won’t generally use bash chunks in future problem sets). (In particular I’m not quite sure what will happen if you render on Windows.)\nWe can help troubleshoot and feel free to post on Ed.\nTest things out well before the due date with a dummy qmd file with both a bash chunk and a Python chunk and make sure things work.\n\nYou will probably need to use sed in a basic way as shown in the bash tutorial. You should not need to use more advanced functionality nor should you need to use awk, but you may if you want to.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "ps/ps2.html#comments",
    "href": "ps/ps2.html#comments",
    "title": "Problem Set 2",
    "section": "",
    "text": "This covers material in Units 3 and 4.\nIt’s due at 10 am (Pacific) on September 20, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 for formatting and attribution requirements.\nNote that using chunks of bash code in Qmd may be troublesome.\n\nYou will need to add engine: knitr to the YAML preface of your qmd document. The default jupyter engine won’t run both bash and Python chunks in the same document because the Jupyter notebooks are associated with a single ‘kernel’ (i.e., a single language for the code chunks).\nFor the knitr engine, you’ll need to have R installed on your computer, including the knitr package. Quarto will then process the code chunks through knitr (which will use R’s reticulate package to handle Python chunks).\nIf you have trouble on your own computer, you can always render your solution for this problem set on an SCF machine (we won’t generally use bash chunks in future problem sets). (In particular I’m not quite sure what will happen if you render on Windows.)\nWe can help troubleshoot and feel free to post on Ed.\nTest things out well before the due date with a dummy qmd file with both a bash chunk and a Python chunk and make sure things work.\n\nYou will probably need to use sed in a basic way as shown in the bash tutorial. You should not need to use more advanced functionality nor should you need to use awk, but you may if you want to.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "ps/ps2.html#problems",
    "href": "ps/ps2.html#problems",
    "title": "Problem Set 2",
    "section": "Problems",
    "text": "Problems\n\nA friend of mine is planning to get married in Death Valley National Park in March (this problem is based on real events…). She wants to hold it as late in March as possible but without having a high chance of a very hot day. This problem will automate the task of generating information about what day of March to hold the wedding using data from the Global Historical Climatology Network. All of your operations should be done using the bash shell except part (c). Also, ALL of your work should be done using shell commands that you save in your solution file. So you can’t say “I downloaded the data from such-and-such website” or “I unzipped the file”; you need to give us the bash code that we could run to repeat what you did. This is partly for practice in writing shell code and partly to enforce the idea that your work should be reproducible and documented.\n\nDownload yearly climate data for a set of years of interest into a temporary directory. Do not download all the years and feel free to focus on a small number of years to reduce the amount of data you need to download. Note that data for Death Valley is only present in the last few decades. As you are processing the files, report the number of observations in each year by printing the information to the screen (i.e., stdout), including if there are no observations for that year.\nSubset to the station corresponding to Death Valley, to the TMAX (maximum daily temperature) variable, and to March, and put all the data into a single file. In subsetting to Death Valley, get the information programmatically from the ghcnd-stations.txt file one level up in the website. Do NOT type in the station ID code when you retrieve the Death Valley data from the yearly files.\nCreate a Python chunk (or R would be fine too) that takes as input your single file from (b) and makes a single plot showing side-by-side boxplots containing the maximum daily temperatures on each calendar day in March. (If you somehow really have trouble mixing Python and bash chunks, it’s ok to insert this figure manually, after running the Python code separately. In this case you could use the jupyter engine provided that a bash kernel is available for Jupyter.)\nNow generalize your code from parts (a) and (b). Write a shell function that takes as arguments a string for identifying the location, the weather variable of interest, and the time period (i.e., the years of interest and the month of interest), and returns the results. Your function should detect if the user provides the wrong number of arguments or a string that doesn’t allow one to identify a single weather station and return a useful error message. It should also give useful help information if the user invokes the function as: get_weather -h. Finally the function should remove the raw downloaded data files (or you should download into your operating system’s temporary file location).\nHint: to check for equality in an if statement, you generally need syntax like:\nif [ \"${var}\" == \"7\" ]\n\nAdd documentation, error-trapping and testing for your code from Problem 4, parts (b) and (c) of PS1. You may use a modified version of your PS1 solution, perhaps because you found errors in what you did or wanted to make changes based on Chris’ solutions (to be distributed in class on Friday Sep. 13) or your discussions with other students. These topics will be covered in Lab 2 (Sep. 13) and are also discussed in Unit 4.\n\nAdd informative doc strings to your functions.\nAdd exceptions for handling run-time errors. You should try to catch the various incorrect inputs a user could provide and anything else that could go wrong (e.g., what happens if the server refuses the request or if one is not online?). In some cases you will want to raise an error, but in others you may want to catch an error with try-except and return None.\nUse the pytest package to set up a thoughtful set of unit tests of your functions.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 2"
    ]
  },
  {
    "objectID": "howtos/RandRStudioInstall.html",
    "href": "howtos/RandRStudioInstall.html",
    "title": "Installing R & RStudio",
    "section": "",
    "text": "On your laptop\nIf your version of R is older than 4.0.0, please install the latest version.\nTo install R, see:\n\nMacOS: install the R-4.2.1.pkg from https://cran.rstudio.com/bin/macosx\nWindows: https://cran.rstudio.com/bin/windows/base/\nLinux: https://cran.rstudio.com/bin/linux/\n\nThen install RStudio. To do so, see https://www.rstudio.com/ide/download/desktop, scrolling down to the “Installers for Supported Platforms” section and selecting the Installer for your operating system.\nVerify that you can install add-on R packages by installing the ‘fields’ package. In RStudio, go to ‘Tools-&gt;Install Packages’. In the resulting dialog box, enter ‘fields’ (without quotes) in the ‘Packages’ field. Depending on the location specified in the ‘Install to Library’ field, you may need to enter your administrator password. To be able to install packages to the directory of an individual user, you may need to do the following:\n\nIn R, enter the command Sys.getenv()['R_LIBS_USER'].\nCreate the directory specified in the result that R returns, e.g., on a Mac, this might be ~/Library/R/4.0/library.\n\nFor more detailed installation instructions for Windows, see Using R, RStudio, and LaTeX on Windows file.\n\n\nVia DataHub\nSee the instructions in Accessing the Unix Command Line for how to login to Datahub. Then in the mid-upper right, click on New and RStudio. Alternatively, to go directly to RStudio, go to https://r.datahub.berkeley.edu."
  },
  {
    "objectID": "howtos/submitPS.html",
    "href": "howtos/submitPS.html",
    "title": "Problem Set Submissions",
    "section": "",
    "text": "Problem set solutions should be written in Quarto Markdown (.qmd) source files, interspersing explanatory text with Python (and in some cases bash) code chunks. Please do not use Jupyter notebook (.ipynb) files as your underlying source file for your solutions. In some cases we will ask that you put function definitions for more complicated functions into one or more Python code (.py) file(s) and show us the code in the appendix of your main solution file by using inspect.getsource().\nWhy?\n\nFor one or two of the initial problem sets you’ll need to include both bash and Python code. This isn’t possible in a single notebook.\nThe underlying format of .ipynb files is JSON. While this is a plain text format, the key-value pair structure (not generally being aligned with the file lines) is much less well-suited for use with Git version control (which relies on diff) than Markdown-based formats.\nOne can run chunks in a Jupyter notebook in arbitrary order. What is printed to PDF depends on the order in which the chunks are run and the results can differ from what one would expect based on reading the notebook sequentially and running the chunks sequentially. For example, consider the following experiment and you’ll see what I mean: (1) Have one code chunk with a = 3 and run it; (2) Add a second chunk with print(a) and run it; and (3) Change the first chunk to a=4 and DO NOT rerun the second chunk. Save the notebook to PDF. You’ll see that your “report” makes no sense. Here’s the result of me doing that experiment.\n\nIf you really want to do your initial explorations of the problems in a Jupyter notebook, with content then copied to qmd, that is fine.",
    "crumbs": [
      "How tos",
      "Problem Set Submissions"
    ]
  },
  {
    "objectID": "howtos/submitPS.html#submission-format",
    "href": "howtos/submitPS.html#submission-format",
    "title": "Problem Set Submissions",
    "section": "",
    "text": "Problem set solutions should be written in Quarto Markdown (.qmd) source files, interspersing explanatory text with Python (and in some cases bash) code chunks. Please do not use Jupyter notebook (.ipynb) files as your underlying source file for your solutions. In some cases we will ask that you put function definitions for more complicated functions into one or more Python code (.py) file(s) and show us the code in the appendix of your main solution file by using inspect.getsource().\nWhy?\n\nFor one or two of the initial problem sets you’ll need to include both bash and Python code. This isn’t possible in a single notebook.\nThe underlying format of .ipynb files is JSON. While this is a plain text format, the key-value pair structure (not generally being aligned with the file lines) is much less well-suited for use with Git version control (which relies on diff) than Markdown-based formats.\nOne can run chunks in a Jupyter notebook in arbitrary order. What is printed to PDF depends on the order in which the chunks are run and the results can differ from what one would expect based on reading the notebook sequentially and running the chunks sequentially. For example, consider the following experiment and you’ll see what I mean: (1) Have one code chunk with a = 3 and run it; (2) Add a second chunk with print(a) and run it; and (3) Change the first chunk to a=4 and DO NOT rerun the second chunk. Save the notebook to PDF. You’ll see that your “report” makes no sense. Here’s the result of me doing that experiment.\n\nIf you really want to do your initial explorations of the problems in a Jupyter notebook, with content then copied to qmd, that is fine.",
    "crumbs": [
      "How tos",
      "Problem Set Submissions"
    ]
  },
  {
    "objectID": "howtos/submitPS.html#problem-set-solution-workflows",
    "href": "howtos/submitPS.html#problem-set-solution-workflows",
    "title": "Problem Set Submissions",
    "section": "Problem set solution workflows",
    "text": "Problem set solution workflows\nHere we outline a few suggested workflows for developing your problem set solutions:\n\nOpen the qmd file in any editor you like (e.g., Emacs, Sublime, ….). From the command line (we think this will work from a Windows command line such as cmd.exe or PowerShell as well), run quarto preview FILE to show your rendered document live as you edit and save changes. You can put the preview window side by side with your editor, and the preview document should automatically render as you save your qmd file.\nUse VS Code with the following extensions: Python, Quarto, and Jupyter Notebooks. This allows you to execute and preview chunks (and whole document) inside VS Code. This is currently our favorite path due to how well it integrated with the Python debugger.\nUse RStudio (yes, RStudio), which can manage Python code and will display chunk output in the same way it does with R chunks. This path seems to work quite well and is recommended if you are already familiar with RStudio.\n\nLater in the semester, you may be allowed to work directly in Jupyter notebooks and use quarto to render from them directly. This has a few quirks and limitations, but may be allowed for some problem sets.\nPlease commit your work regularly to your repository as you develop your solutions.",
    "crumbs": [
      "How tos",
      "Problem Set Submissions"
    ]
  },
  {
    "objectID": "howtos/submitPS.html#github-repository",
    "href": "howtos/submitPS.html#github-repository",
    "title": "Problem Set Submissions",
    "section": "GitHub repository",
    "text": "GitHub repository\n\nSetting up your repository\nWe are creating repositories for everyone at github.berkeley.edu. Additionally, homeworks still need to be submitted as PDFs on Gradescope.\nSteps:\n\nLog into github.berkeley.edu using your Berkeley credentials. Because of how the system works, you will need to log in before your account is created. Nothing else needs to be done, just log in and log out.\nAfter accounts are created (may take a couple days after first login), when you log in again, you should see one private repository listed on the left side (e.g., stat243-fall-2024/ahv36). This is your class repository. Do not change the repository settings! They are set up for this class.\nClone the repo to your home directory (I would clone it into a directory just for repositories (e.g., I use ~/repos). In the top-level of your working directory, you should create a file named (exactly) .gitignore.\n\nThe .gitignore file causes Git to ignore transient or computer-specific files that Quarto generates. (more info at https://github.com/github/gitignore) In it, put (again, don’t put dashed lines):\n# cache directories\n/__pycache__\n\n# pickle files\n*.pkl\n*.pickle\n\n\nRepository Organization\nThe problem sets in your repository should be organized into folders with specific filenames.\nWhen we pull from your repository, our code will be assuming the following structure:\nyour_repo/\n├── ps1/\n│   ├── ps1.pdf\n│   ├── ps1.qmd \n│   ├── &lt;possibly auxiliary code or other files&gt;\n├── ps2/\n│   ├── ...\n├── ...\n├── ps8/\n└── .gitignore\nThe file names are case-sensitive, so please keep everything lowercase.\n\n\nSave or cache credentials in Git\nIf authenticating to github.berkeley.edu every time sounds inconvenient, note that your credentials can be saved or cache to avoid entering them constantly. More details can be found on this SCF page. Besides the methods mentioned on the page, one can also store credentials in a file via git config --global credential.helper store.",
    "crumbs": [
      "How tos",
      "Problem Set Submissions"
    ]
  },
  {
    "objectID": "howtos/windowsInstall.html",
    "href": "howtos/windowsInstall.html",
    "title": "R/Rstudio on Windows",
    "section": "",
    "text": "While R was built/designed for UNIX systems, it has been well adapted for Windows. Here, we’ll start with the basics of installing R on Windows. Then, we’ll cover the recommended editor (Rstudio), and how to build pdf documents using MikTeX.\n\n\n\n\n\n\nNote\n\n\n\nThis tutorial installs Windows-only versions of everything. Modern Windows systems have an Ubuntu subsystem available that we highly recommend. See the Installing the Linux Subsystem on Windows tutorial for setting up that configuration.\n\n\n\nInstalling R\nThe first step in installing the R language. This is available on CRAN (Comprehensive R Archive Network).\n\nGo to the CRAN webpage, www.r-project.org\nIn the first paragraph, click the link download R\nYou’re now on a page titled CRAN Mirrors, choose the mirror located closest to your geographic location\n\nMirrors are different servers that all host copies of the same website. You get best performance from the location closest to you.\n\nYou’re now on a paged titled The Comprehensive R Archive Network. The first box is labeled Downloand and Install R, click Download R for Windows\nClick base or install R for the first time, these take you to the same place\n\nFor more advanced things, you may need the Rtools download later. It isn’t necessary now, but remember that for the future.\n\nAt the top is a large-font link, Download R X.X.X for Windows, click this. It will begin downloading the Windows installer for R.\nFollow the instructions for setup. If you are unsure of anything, leave the default settings\n\n\n\nInstalling RStudio\nRStudio is one of the best text editors for coding in R. It is our recommended option for beginning. After you are comfortable with the language, or if you use other languages as well, you may want to explore Atom or Sublime. More advanced options include Emacs with [ESS package][https://ess.r-project.org/] and vim with the Nvim-R plugin.\nTo install RStudio:\n\nGo to the RStudio Desktop download page, rstudio.com/products/rstudio/download/#download\nChoose the download for your OS, most likely the Windows 10/8/7 one\nFollow the instructions for setup. If you are unsure of anything, leave the default settings\nOpen RStudio (R will run automatically in the background)\n\nYou may have to allow RStudio to run if prompted (depends on security settings and anti-virus software)\n\n\nOnce RStudio is installed, you can install or update packages in one of two ways:\n\nVia the console, using install.packages() or update.packages()\n\nVia the gui:\n\nIn the top bar, click on tools\nSelect Install Packages… to install packages\nSelect Check for Package Updates… to update packages\n\n\n\n\nCompiling PDF Documents\nFor the purposes of this class, you will be submitting homeworks as PDF documents that blend written text, code, and code-generated output. These documents are RMarkdown documents, and are dynamic documents that provide a convenient method for documenting your work (more on this in one of the lab sections). To do this, you need a LaTeX renderer. We recommend MiKTeX for Windows.\n\nGo to Getting MiKTeX to download MiKTeX for Windows, miktex.org/download\nThe first page should be Install on Windows, click Download at the bottom of the page\n\nClick the download to begin\n\n\n\n\n\n\n\nImportant\n\n\n\nFOLLOW THESE INSTALL INSTRUCTIONS.\nThe default options are fine in most places, but there is one that will cause problems.\n\n\n\nAccept the Copying Conditions, click next\nInstall only for you, click next\nUse the default directory, click next\nThis should be the Settings page. Under Install missing packages on-the-fly, change the setting to Yes, click next\n\n\n\nBecause we are using MiKTeX as an external renderer, it can’t ask you to install missing packages, and will then fail, so we have to set that installation as automatic.\n\n\nClick start (Optional, but highly recommended) Open RStudio, select a new .Rmd document, d then choose knit. This may take some time, because MiKTeX is installing new braries, but it ensures that your pipeline is setup correctly"
  },
  {
    "objectID": "howtos/installQuarto.html",
    "href": "howtos/installQuarto.html",
    "title": "Installing and Using Quarto",
    "section": "",
    "text": "Unless you plan to generate your problem set solutions on the SCF, you’ll need to install Quarto.\nOnce installed, you should be able to run commands such as quarto render FILE and quarto preview FILE from the command line.\nQuarto also runs from the Windows Command shell or PowerShell. We’ll add details/troubleshooting tips here as needed.",
    "crumbs": [
      "How tos",
      "Installing and Using Quarto"
    ]
  }
]